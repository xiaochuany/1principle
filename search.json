[
  {
    "objectID": "posts/deep-dive/uml-recap1.html",
    "href": "posts/deep-dive/uml-recap1.html",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "",
    "text": "In a supervised learning, we specify\n\nan example space \\(\\mathcal X\\)\n\na label space \\(\\mathcal Y\\)\na collection of hypotheses \\(h: \\mathcal X \\to \\mathcal Y\\), making up the hypothesis class (inductive bias), from which we want to pick a predictor\na loss function \\(\\ell: \\mathcal Y \\times \\mathcal Y \\to \\mathbb{R}_+\\), quantifying how good or bad a prediction \\(\\hat y\\) is compared to the ground truth label \\(y\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe are given an independent and identically distributed input-output pairs \\(S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y\\) with distribution \\(D\\).\n\n\nOur goal is to pick the “best” predictor in the sense of minimising the true loss\n\\[\nL_D(h) = \\mathbb{E}_{(x,y)\\sim D}[\\ell(h(x),y))]  \n\\] where \\(D\\) is the true distribution of \\((x,y)\\).\nObviously a priori the distribution \\(D\\) of the samples is unkonwn. However by law of large numbers we know that \\[\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n\\] is a consistent estimator of \\(L_D(h)\\) (when \\(m\\to\\infty\\) under mild condition on the distribution of \\(\\ell(h(x),y)\\)). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \\[\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n\\] Hence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when \\(m\\) is large, for a fixed \\(h\\). Whether such approximation is valid uniformly for all the hypothesis in \\(\\mathcal H\\), in other words, whether \\(h_S\\approx h^*\\in \\mathrm{argmin}_{h\\in\\mathcal H} L_D(h)\\), is at the centre of the so-called learning theory.\nWe often decompose the generalisation error \\(L_D(h_S)\\) in two parts: \\[\nL_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)]\n\\] the first is called the approximation error, and second estimation error."
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#setting-up-the-scene",
    "href": "posts/deep-dive/uml-recap1.html#setting-up-the-scene",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "",
    "text": "In a supervised learning, we specify\n\nan example space \\(\\mathcal X\\)\n\na label space \\(\\mathcal Y\\)\na collection of hypotheses \\(h: \\mathcal X \\to \\mathcal Y\\), making up the hypothesis class (inductive bias), from which we want to pick a predictor\na loss function \\(\\ell: \\mathcal Y \\times \\mathcal Y \\to \\mathbb{R}_+\\), quantifying how good or bad a prediction \\(\\hat y\\) is compared to the ground truth label \\(y\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe are given an independent and identically distributed input-output pairs \\(S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y\\) with distribution \\(D\\).\n\n\nOur goal is to pick the “best” predictor in the sense of minimising the true loss\n\\[\nL_D(h) = \\mathbb{E}_{(x,y)\\sim D}[\\ell(h(x),y))]  \n\\] where \\(D\\) is the true distribution of \\((x,y)\\).\nObviously a priori the distribution \\(D\\) of the samples is unkonwn. However by law of large numbers we know that \\[\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n\\] is a consistent estimator of \\(L_D(h)\\) (when \\(m\\to\\infty\\) under mild condition on the distribution of \\(\\ell(h(x),y)\\)). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \\[\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n\\] Hence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when \\(m\\) is large, for a fixed \\(h\\). Whether such approximation is valid uniformly for all the hypothesis in \\(\\mathcal H\\), in other words, whether \\(h_S\\approx h^*\\in \\mathrm{argmin}_{h\\in\\mathcal H} L_D(h)\\), is at the centre of the so-called learning theory.\nWe often decompose the generalisation error \\(L_D(h_S)\\) in two parts: \\[\nL_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)]\n\\] the first is called the approximation error, and second estimation error."
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#lets-be-concrete",
    "href": "posts/deep-dive/uml-recap1.html#lets-be-concrete",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "Let’s be concrete",
    "text": "Let’s be concrete\nFrom a practical point of view, we may not want to get into the learning theory bounds despite its elegance, what we do is to split the sample \\(S^0\\) into two parts \\(S, V\\), where \\(S\\) is used to find an ERM which we denote by \\(h_S\\), another for testing whether the found ERM achieves small \\(L_D(h_S)\\). The rationale is simple, since we make iid assumption, \\(V\\) is independent of \\(S\\), hence \\(L_{V}(h_S) \\approx L_D(h_S)\\) when \\(|V|\\) is not too small. Therefore, the smallness of \\(L_{V}(h_S)\\) indicates good quality (small true risk) of our predictor \\(h_1\\).\nThe split method for arrays is implemented in sklearn.model_selection\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\ng = np.random.default_rng(12)\nx = g.normal(0,1,(20,))\nx_tr, x_te = train_test_split(x,test_size=0.2,random_state=12)\n\nTypically, examples are vectors in \\(\\mathbb{R}^d, d\\ge 1\\). In \\(k\\)-way (\\(k\\ge 2\\)) classification problems, labels are one-hot encodings \\(e_1,..., e_k\\) where \\(e_i\\) is the unit vector in \\(\\mathbb{R}^k\\) with one on the \\(i\\) th coordinate and zero elsewhere. If \\(k=2\\), we can drop the second coordinate and simply denote the two classes by \\(\\{1, -1\\}\\) or \\(\\{0,1\\}\\). In regression problems, the labels are in the continuum \\(\\mathbb{R}^k, k\\ge 1\\).\nNow consider \\(\\mathcal H\\). For classification problems, instead of predicting discrete class labels directly, it is sometimes beneficial to predict a probability mass function over the \\(k\\) classes. From the pmf, a label can be obtained by taking argmax. In other words, the range of \\(h\\in\\mathcal H\\) is assumed to be \\(\\{y\\in\\mathbb{R}^k: y_i\\ge 0, y_1+...+y_k=1\\}\\). For regression problems there are no such constraints and the range can be the whole \\(\\mathbb{R}^k\\).\nThe choie of the loss function may vary, depending on what goal we are trying to achiecve. Researchers can design new losses suitable for their use case. Here we mention a few popular ones. For classification problems, if we use hypothesis predicting pmf, the cross entropy loss is often a good choice \\[\nXE(p,\\hat p) =  - \\sum_{i=1}^k p_i \\log(\\hat p_i)\n\\] For regression problems, the squared loss is often a good choice \\[SE(y,\\hat y)= \\|y-\\hat y\\|^2_2\\] where \\(\\|.\\|\\) is Euclidean norm.\nOne of the advantages of these loss functions is that they are convex in the \\(\\hat p\\) or \\(\\hat y\\) variables, making it possible to leverage the machinery of convex optimistion when it comes to the actual training process.\nMany loss functions are already implemented in sklearn.metrics. The XE is named log_loss and the squared loss is named mean_sqquared_error. Let’s list all of them.\n\nimport sklearn.metrics as metrics\nfor m in dir(metrics):\n    if not m.startswith('_'): print(m)\n\nConfusionMatrixDisplay\nDetCurveDisplay\nDistanceMetric\nPrecisionRecallDisplay\nPredictionErrorDisplay\nRocCurveDisplay\naccuracy_score\nadjusted_mutual_info_score\nadjusted_rand_score\nauc\naverage_precision_score\nbalanced_accuracy_score\nbrier_score_loss\ncalinski_harabasz_score\ncheck_scoring\nclass_likelihood_ratios\nclassification_report\ncluster\ncohen_kappa_score\ncompleteness_score\nconfusion_matrix\nconsensus_score\ncoverage_error\nd2_absolute_error_score\nd2_pinball_score\nd2_tweedie_score\ndavies_bouldin_score\ndcg_score\ndet_curve\neuclidean_distances\nexplained_variance_score\nf1_score\nfbeta_score\nfowlkes_mallows_score\nget_scorer\nget_scorer_names\nhamming_loss\nhinge_loss\nhomogeneity_completeness_v_measure\nhomogeneity_score\njaccard_score\nlabel_ranking_average_precision_score\nlabel_ranking_loss\nlog_loss\nmake_scorer\nmatthews_corrcoef\nmax_error\nmean_absolute_error\nmean_absolute_percentage_error\nmean_gamma_deviance\nmean_pinball_loss\nmean_poisson_deviance\nmean_squared_error\nmean_squared_log_error\nmean_tweedie_deviance\nmedian_absolute_error\nmultilabel_confusion_matrix\nmutual_info_score\nnan_euclidean_distances\nndcg_score\nnormalized_mutual_info_score\npair_confusion_matrix\npairwise\npairwise_distances\npairwise_distances_argmin\npairwise_distances_argmin_min\npairwise_distances_chunked\npairwise_kernels\nprecision_recall_curve\nprecision_recall_fscore_support\nprecision_score\nr2_score\nrand_score\nrecall_score\nroc_auc_score\nroc_curve\nsilhouette_samples\nsilhouette_score\ntop_k_accuracy_score\nv_measure_score\nzero_one_loss\n\n\nIt comes in handy that sklearn has them already defined. Implementing each one of them is often not hard, e.g.\n\ndef log_loss(yt,yp):\n    yt[yt==0]= 1-yp[yt==0]\n    return -np.log(y).mean()"
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#analysis-of-errors",
    "href": "posts/deep-dive/uml-recap1.html#analysis-of-errors",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "Analysis of errors",
    "text": "Analysis of errors\nIn practice, \\(D\\) is unknown and we only observe the training error \\(L_S(h_S)\\) and the validation error \\(L_V(h_S)\\). Hence we decompose the generalisation error differently \\[\nL_D(h_S) = [L_D(h_S) - L_V(h_S)] + [L_V(h_S) - L_S(h_S)] + L_S(h_S)\n\\] The first term is small when \\(|V|\\) is moderately large by independence of \\(V\\) and \\(S\\). The second and third term are observalbe and several cases may arise.\n\nthe gap is small and the training error is small. This is a happy scenario\nthe training error is large. To address this, we may consider\n\nengarge the hypothesis class,\ncompletely changing it,\nfind a better feature representation,\nfind a better optimiser\n\n\ntraining error is small but the gap is large. To address this, we may consider\n\nadd regularisation\nget more training data\nreduce the hypothesis class\n\n\nIt is benefiical to plot the learning curve during training. This amounts to visualise the training error and validation error on the same plot as time evolves (every X batches, every X epoch etc)."
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html",
    "href": "posts/deep-dive/crm2-ml.html",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter tuning",
    "section": "",
    "text": "In a previous post, we’ve modelled the loss of a portfolio of \\(d\\) instruments as follows \\[L = \\sum_{i=1}^d \\mu_i S_i I_i,\n\\] where \\(L\\) represents the total loss, \\(\\mu_i\\) is the exposure at default, \\(S_i\\) is the loss given default, and \\(I_i\\) is the event of default for the \\(i\\)-th instrument. To compute expected loss, VaR, and other relevant metrics in risk management, it is crucial to estimate these underlying parameters accurately. In this post, our focus is on estimating \\(p_i=\\mathbb{E}[I_i]\\), which is formulated as a machine learning problem"
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html#estimating-probaiblity-of-default-pd",
    "href": "posts/deep-dive/crm2-ml.html#estimating-probaiblity-of-default-pd",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter tuning",
    "section": "Estimating probaiblity of default (PD)",
    "text": "Estimating probaiblity of default (PD)\nConsider the real-world example of a bank approving loans for applicants based on their profiles. In this scenario, every applicant must fill out a comprehensive application form, including details such as their profession, age, amount of debts, monthly salary, and so on. The bank maintains records and, in retrospect, knows who has defaulted on their loans.\nTo formalize this process, each applicant corresponds to a vector in \\(\\mathbb{R}^k\\), known as the feature vector, which incorporates all the information from the form (possibly encoded for categorical values, e.g., converting ‘profession’ into dummy variables). The output we aim to predict is whether the applicant is in default (1) or not (0).\nThis constitutes a binary classification problem. With a substantial number of input-output pairs (features and default status) available, supervised learning algorithms can be employed to learn a relationship that can subsequently be used for predictions.\nMany supervised learning algorithms for binary prediction actually output probabilities (specifically, the probability of the label being 1). This is suitable for our goal, as we precisely seek to estimate probabilities.\nFor illustration, let’s consider a credit default risk dataset from this Kaggle competition. The dataset we are working with contains 308k rows and 121 input features.\nWhile there are numerous aspects to discuss regarding this dataset, for the sake of brevity, I will address two significant points here:\n\nMissing values\nThere are many missing values in this dataset. To be more precise, 41 columns actually have half of their values missing.\nThis is a critical issue that needs to be addressed because many off-the-shelf machine learning models in scikit-learn, such as logistic regression, random forest, and support vector machines, cannot handle missing values represented as np.nan. There are two possible options to handle this:\n\nImpute the missing values before feeding the data into these models. Imputation can be done using “rule-based” methods such as scikit-learn’s SimpleImputer or “learning-based” methods such as scikit-learn’s IterativeImputer.\nUse a different model that supports missing values natively. For instance, many gradient boosting implementations like HistGradientBoosting, XGBoost, LightGBM, and CatBoost handle missing values natively.\n\nFor the sake of providing a quick benchmark, we have opted for the second option and are using scikit-learn’s gradient boosting implementation, HistGradientBoostingClassifier. Explaining the detailed workings of gradient boosting is a vast topic that we might delve into in a future post.\n\n\nUnbalanced data\nRoughly 8% of the obligors goes into default in this dataset, making it unbalanced. From the perspective of risk management, it is important to predict defaults (label 1) accurately. Therefore, when it comes to evaluate model performance, accuracy is not an appropriate metric. Simply predicting non-default for all obligors would result in a correct prediction 92% of the time, but it’s never correct for the defaults.\nApplying a machine learning model directly to an unbalanced dataset can lead to sub-optimal results. We’ll demonstrate this point in the next section. A simple mitigation strategy is to sub-sample the majority class to match the size of the minority class, creating a balanced dataset. In this example, the minority class comprises roughly 25k rows, so we can train a model using this strategy as the balanced data is not too small to be useful.\nObviously, the downside of this strategy is that we have thrown away lots of valuable data, but let’s not worry about it at this stage of obtaining a quick benchmark."
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html#implementation",
    "href": "posts/deep-dive/crm2-ml.html#implementation",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter tuning",
    "section": "Implementation",
    "text": "Implementation\nFirst we feed the whole unbalanced dataset into HistGradientBoostingClassifier.\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\nPATH = '../../../LearnFromTabularData/data/raw'\ndf = pd.read_csv(os.path.join(PATH,'application_train.csv'))\n\ny = df.pop('TARGET')\nX = df\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=1123)\n\nohe = OneHotEncoder(drop='if_binary')\nscaler = StandardScaler()\nmodel = HistGradientBoostingClassifier()\n\nct = make_column_transformer(\n    (ohe, make_column_selector(dtype_exclude=np.number)),\n    (scaler, make_column_selector(dtype_include=np.number)),\n    remainder='drop',\n)\n\npipe = make_pipeline(\n    ct,model\n)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.92      0.96     61343\n           1       0.02      0.54      0.03       160\n\n    accuracy                           0.92     61503\n   macro avg       0.51      0.73      0.50     61503\nweighted avg       1.00      0.92      0.96     61503\n\n\n\nAs we can see, the precision for class 1 is only 0.02, indicating that only 2% of the predicted defaults are true defaults. Does this mean that the model we chose is rubbish? Not necessarily. Let’s use the exact same model but now with a balanced dataset created using the sub-sampling strategy\n\ndf = pd.read_csv(os.path.join(PATH,'application_train.csv'))\nmask = (df['TARGET']==1)\nm = mask.sum()\ndf_balance = pd.concat([df[~mask].sample(m,random_state=119),df[mask]], axis=0)\n\ny = df_balance.pop(\"TARGET\")\nX = df_balance\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=112)\n\nohe = OneHotEncoder(drop='if_binary')\nscaler = StandardScaler()\nmodel = HistGradientBoostingClassifier()\n\nct = make_column_transformer(\n    (ohe, make_column_selector(dtype_exclude=np.number)),\n    (scaler, make_column_selector(dtype_include=np.number)),\n    remainder='drop',\n)\n\npipe = make_pipeline(\n    ct,model\n)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n\n              precision    recall  f1-score   support\n\n           0       0.69      0.68      0.69      5059\n           1       0.67      0.69      0.68      4871\n\n    accuracy                           0.68      9930\n   macro avg       0.68      0.68      0.68      9930\nweighted avg       0.68      0.68      0.68      9930\n\n\n\nMuch better! All the metrics look roughly the same, hovering around 69%. While this is far from being deployable in the real world, as a baseline, it’s far more reasonable than our previous attempt.\nWe’ll conclude the post here. To further enhance the overall performance, it’s necessary to meticulously explore the features, engage in more feature engineering, employ clever methods of data imputation, and conduct thorough hyperparameter tuning. Give it a go!"
  },
  {
    "objectID": "posts/gist/walkthrough.html",
    "href": "posts/gist/walkthrough.html",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "",
    "text": "Jeremy Howard, the founder of fastai, organized a series of walkthroughs covering the basics of git, bash, vim, tmux, and more, as a companion to the free fastai course on deep learning. Here is the playlist."
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-1",
    "href": "posts/gist/walkthrough.html#live-coding-1",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 1",
    "text": "Live coding 1\n\nintall WSL if on Windows (all commands below are typed in linux terminal in WSL)\nalt + enter for full screen\npwd print working directory\nwhich shows where a file is\nmkdir makes a directory\nls list stuffs -lah long format all files human readable\ndf -h disk free\ndu -sh * disk usage -s summary of all subdirectories in .\ndu -sh . disk usage of .\n\nconda-forge distribution as of september 2023, mambaforge/miniforge3 are the same. -c conda-forge is the default\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh to download the distribution\nbash Miniforge3-Linux-x86_64.sh -b to install, where -b for less intervention\ninstall pytorch here\nmamba install jupyter ipywidgets\nalias jl=\"jupyter lab\" add this command to the end of .bashrc to save alias for good\nmamba install -c fastai fastai\n\n\n\n\n\n\n\nTip\n\n\n\nJeremy automated setup of conda in this file. To run it, one needs to add executable permission to user chmod u+x setup-conda.sh"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-2",
    "href": "posts/gist/walkthrough.html#live-coding-2",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 2",
    "text": "Live coding 2\n\nbash\n\ncd - back to most recent directory\npushd ~ push current directory to a stack and change directory to ~\npopd pop what’s in the stack\nctrl+d exit for most program\nctrl+u ctrl+w ctrl+a ctrl+e move cursor faster\n\n\n\ntmux\n\ntmux\nctrl+b start of a tmux command, then\n\" split top-bottom\n% split left-right\nz zoom in/out\nd detach, back to bash\ntmux a attach stuffs running in tmux from bash (everything remains until restart computer)\n\n\n\ngit (with ssh)\n\nssh-keygen generates public/private rsa key pair (prompt where to save the file, by default it’s in ~/.ssh)\nlogin to github.com and upload public key cat ~/.ssh/id_rsa.pub\ngit clone git@github.com:fastai/fastbook.git\ngit status\ngit commit -am 'MESSAGE'\ngit push"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-3",
    "href": "posts/gist/walkthrough.html#live-coding-3",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 3",
    "text": "Live coding 3\n\nbash\n\nln -s ONE simlink ONE to here\n$PATH paths that bash knows to run program\n\n\n\npaperspace\n\npip install --user PACKAGE will install PACKAGE to ~/.local/lib/python3.*/site-packages which gets wiped after shutdown\nmv ~/.local /storage/.local then\nln -s /storage/.local ~/ to make it persistent (/storage is persistent across notebook instances)\n\n\n\njupyter lab\n\nctrl + shift + [ change tab\nctrl + b hide side column\n%%debug exit with q\nshift + tab or METHOD? shows signature\nMETHOD?? shows source code"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-4",
    "href": "posts/gist/walkthrough.html#live-coding-4",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 4",
    "text": "Live coding 4\n\n\n\n\n\n\nTip\n\n\n\nJeremy teaches how to write your first bash script. The job done via these scripts is to set up paperspace for persistent storage and configs across instances. The repo is here.\n\n\n\nfirst script pre-run.sh\n#!/usr/bin/env bash\n\npushd ~\n\nmkdir -p /storage/cfg\n\nif [ ! -e /storage/cfg/.conda ]; then\n        mamba create -yp /storage/cfg/.conda\nfi\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nchmod 700 /storage/cfg/.ssh\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\npopd\n\n\nsecond script setup.sh\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-5",
    "href": "posts/gist/walkthrough.html#live-coding-5",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 5",
    "text": "Live coding 5\n\nbash\n\ncat FILE display file\ncat f1 f2 &gt; combined concat\ncat f1 &gt;&gt; f2 append\n\n\n\nvim\n\ni insert mode\nesc back to command mode\nin command mode try :q to quit :wq to write and quit\ntutorial https://vim-adventures.com/\n\nalternatively, type code . then edit/create file with VS code"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-6",
    "href": "posts/gist/walkthrough.html#live-coding-6",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 6",
    "text": "Live coding 6\n\ndu -sh * | grep 'G' search ouput of du -sh * that contains G to identify directories larger than GB\nconda install universal-ctags\ncopy config files to Paperspace (they’ll be persistent if we’ve run the bash script before in live coding 4.)\n\ncopy ssh keys to ~/.ssh and change permissions chmod 644 ~/.ssh/id_rsa.pub chmod 600 ~/.ssh/id_rsa\nfirst time git commit needs ~/.gitconfig to have name and email of the user, just follow the prompt."
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-7",
    "href": "posts/gist/walkthrough.html#live-coding-7",
    "title": "Walkthroughs by Jeremy Howard",
    "section": "Live coding 7",
    "text": "Live coding 7\n\npip isntall --user kaggle\n\npre-append ~/.bashrc with export PATH=~/.local/bin:$PATH\nsource .bashrc\ncreate kaggle.json file from kaggle website and copy it into ~/.kaggle\nnavigate into .kaggle and chmod 600 kaggle.json\nkaggle competitions donwnload -c NAME\n\nTry time unzip -q BLA to see how long it takes to unzip.\n\nnvidia-smi dmon if sm is low, this means i/o slow. Try\n\nresize image\nmove files to local (see get_data.sh below)\nreduce augmentation\nchange to CPU instance?\n\n\nOn paperspace, create get_data.sh in /notebooks (persistent)\n#!/user/bin/env bash\ncd\nmkdir BLA\ncd BLA\nkaggle competitions donwnload -c NAME\nunzip -q NAME"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "first princples first",
    "section": "",
    "text": "bits of fastai live-coding sessions\n\n\n\n\n\n\n\nhacks\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning Recap 1 Concepts\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nCredit Risk Models Ep 2 machine learning methods for parameter tuning\n\n\n\n\n\n\n\ncredit risk modelling\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nCredit Risk Models Ep 1 Fundamentals\n\n\n\n\n\n\n\ncredit risk modelling\n\n\napplied probability\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Machine Learning: a series of 20 videos\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nSecret ‘SAWS’ of deep learning, feature backprop\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I like exploring the instricate intersection of mathematics and real-world applications, in particular probabilitistic machine learning and quantitative finance.\nCurious about my research? Here’s my research webpage."
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html",
    "href": "posts/deep-dive/crm1-fun.html",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "",
    "text": "This is the first post in a series dedicated to credit risk models. The model discussed in this post is intentionally naive, as it relies on several unrealistic assumptions for the sake of simplicity and tractability. Despite its simplicity, this model serves as an excellent starting point for understanding the fundamental concepts of credit risk. It lays the foundation for more complex models that will be explored later in this series. The mathematical concepts introduced in this post are commonly found in any first-year probability textbook"
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#pd-lgd-ead-approach",
    "href": "posts/deep-dive/crm1-fun.html#pd-lgd-ead-approach",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "PD-LGD-EAD approach",
    "text": "PD-LGD-EAD approach\nCredit risk models are mathematical representations of the potential losses within a portfolio of financial instruments. Each instrument carries a probability of default, leading to either a complete loss or a partial loss. Modeling credit risk is crucial as it offers valuable insights to stakeholders, enabling them to take proactive measures to mitigate these losses. Consequently, it plays a pivotal role in the financial sector.\nLet’s introduce some notation. Consider a portfolio containing \\(d\\) instruments. The event of default for the \\(i\\)-th instrument is denoted as \\(D_i\\). Here, \\(\\mu_i\\) represents the exposure at default (EAD) of the \\(i\\)-th instrument, and \\(S_i \\in [0,1]\\) the loss given default (LGD) where \\(S\\) signifies severity, indicating the actual loss relative to the exposure. We define \\(I_i := I_{D_i}\\) as the indicator of the event \\(D_i\\), which follows a Bernoulli distribution with parameter \\(p_i := \\mathbb{P}[D_i]\\). The total loss of the portfolio is expressed as the sum: \\[\nL = \\sum_{i=1}^d \\mu_i S_i I_i\n\\tag{1}\\] From the perspective of risk managers, the values of \\(\\mu_i\\) are known. However, the uncertainly lies in whether an instrument will default and the corresponding severity, making the total loss \\(L\\) a random variable. Our interest lies in understanding the probability distribution of \\(L\\).\nIn this post we make the following naive assumptions.\n\n\n\n\n\n\nImportant\n\n\n\nAssume that\n\n\\(\\{(I_i,S_i)\\}_{i=1}^d\\) is a family of independent random vectors.\n\\(I_i\\) is independent of \\(S_i\\) for all \\(i\\in [d]\\).\n\n\n\nUnder this assumption, it is clear that \\[\n\\mathbb{E}[L] = \\sum_{i=1}^d \\mu_i \\mathbb{E}[S_i] p_i\n\\] \\[\n\\mathrm{Var}[L] = \\sum_{i=1}^d \\mu_i^2 (\\mathbb{E}[S_i^2] p_i  - \\mathbb{E}[S]^2 p_i^2)\n\\] The expected loss (EL) is an important quantity in the Basel III guidelines."
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#var-and-expected-shortfalls",
    "href": "posts/deep-dive/crm1-fun.html#var-and-expected-shortfalls",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "VaR and expected shortfalls",
    "text": "VaR and expected shortfalls\nIn a stable economy, default events are infrequent, making credit risk modeling primarily focused on rare occurrences. Obtaining a precise measure of the credit risk of a portfolio is not achieved by merely calculating the average; instead, it’s crucial to comprehend the quantiles. In the realm of credit risk, these quantiles are referred to as the Value at Risk (VaR).\n\\[\n\\mathrm{VaR}_\\alpha(L) = \\inf\\{t\\in\\mathbb{R}: \\mathbb{P}[L\\ge t]\\le \\alpha \\}.\n\\] Another commonly utilized metric is the Expected Shortfall, which represents the conditional expectation of the loss \\(L\\) given that \\(L\\) exceeds the VaR \\[\nE_\\alpha(L) = \\mathbb{E}[L|L\\ge\\mathrm{VaR}_\\alpha].\n\\] Clearly \\(E_\\alpha(L)\\ge \\mathrm{VaR}_\\alpha(L)\\).\nIt’s crucial to acknowledge that the joint distribution of \\((I_i, S_i)\\) is unknown. To compute VaR and expected shortfalls, risk analysts must undertake at least three tasks:\n\nModel Calibration: This involves determining the model’s parameters using available data.\nDistribution Computation: Compute the distribution of \\(L\\) either analytically or through Monte Carlo simulation, based on the calibrated parameters.\nMetrics Computation: Utilize the obtained distribution to compute VaR, shortfall, or other relevant metrics either analytically or numerically.\n\nThe calibration process demands substantial effort and is specific to the chosen model. We will delve into this topic in a future post. For the remainder of this discussion, let’s assume that we have already established a set of parameters and focus on the last two steps.\nIt’s important to emphasize that deriving the exact distribution of \\(L\\) can be tedious, if not infeasible. In practice, risk analysts resort to simulations and numerical computations to determine VaR and shortfalls. This pragmatic approach is the one we adopt here."
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#monte-carlo-simulation-for-estimating-var-and-shortfalls",
    "href": "posts/deep-dive/crm1-fun.html#monte-carlo-simulation-for-estimating-var-and-shortfalls",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "Monte Carlo simulation for estimating VaR and shortfalls",
    "text": "Monte Carlo simulation for estimating VaR and shortfalls\nStep 1\nWe first specify the parameters \\(p_i, \\mu_i\\) and parameters for the distribution of \\(S_i\\). These parameters are generated at random with arbitrary choice of distributions, following\nBolder (2018)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# set random seed for reproducibility\ng = np.random.default_rng(1123)\n\nd = 100 # number of instruments\n\n# set the probability of default for d instruments, low 0.1 high 7 per cent\nx = g.chisquare(1,(d,))\nx = np.where(x&lt;0.1, 0.1, x)\nx = np.where(x&gt;7, 7, x)\np = x/100 \n\n# set exposures parameters normalised to have total exposure 1000\ny = g.weibull(3,(d,))\nmu = y*1000/y.sum()\n\n# set serverity parameters\na,b = 1.5, 2.5\n\nStep 2\nNext we simulate \\(L\\) for a large number of times accoding to Equation 1. We sort the samples in the end, useful for computing the empirical distributions later on.\n\nm = 50_000 # number of repetitions\nS = g.beta(a, b, (m,d))\nI = g.uniform(size=(m,d)) &lt; p\nL = (S*I)@ mu\nL = np.sort(L)\n\nStep 3\nWe plot the histogram and empirical distribution, then estimate VaR and expected shortfalls.\n\n# Estimate VaR \npers = np.array([95,99,99.9])\nalphas = 1 - pers/100\nvars = np.percentile(L, pers)\n\nfig, axes = plt.subplots(2,1)\naxes[0].hist(L, bins=30, label='Histogram of Loss')\nfor i, var in enumerate(vars): \n  axes[0].axvline(var,linestyle='dashed', color=g.uniform(size=(3,)),label=f'VaR{alphas[i]:.3f}')\naxes[0].legend()\n\naxes[1].plot(L,np.linspace(0,1,len(L),endpoint=False), color= 'red', label='empirical distribution')\naxes[1].legend()\nfor var in vars: \n  axes[1].axvline(var,linestyle='dashed')\nplt.show()\n\n# Estimate shortfalls\nshortfalls = np.zeros(len(vars))\nfor i,var in enumerate(vars):\n  shortfalls[i] = L[L&gt;var].mean()\n\npd.set_option('display.precision', 4)\npd.DataFrame({'alphas': alphas, 'VaR': vars, 'Shortfalls': shortfalls}).set_index('alphas')\n\n\n\n\n\n\n\n\n\n\n\nVaR\nShortfalls\n\n\nalphas\n\n\n\n\n\n\n0.050\n12.6668\n16.2741\n\n\n0.010\n18.6163\n21.4599\n\n\n0.001\n25.4548\n28.1533\n\n\n\n\n\n\n\nWe’ve completed tasks 2 and 3! Next time, we’ll delve into methods of parameter estimation, still within the naive setting. This will bring us full circle and complete the lifecycle of a single model."
  },
  {
    "objectID": "posts/deep-dive/saws.html",
    "href": "posts/deep-dive/saws.html",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "",
    "text": "My colleague Simon Shaw came up with the memorable notation SAWS for the key recursion step in backprop in a Year 2 module on deep learning. I found this a perfect example of “good notation helps memorisation” which hopefully reinforces understanding. In this post, I will explain how we get to this recursion and end the post with a python implementation."
  },
  {
    "objectID": "posts/deep-dive/saws.html#multi-layer-perceptron",
    "href": "posts/deep-dive/saws.html#multi-layer-perceptron",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Multi-Layer Perceptron",
    "text": "Multi-Layer Perceptron\nEveryone knows that a neural network is a universal function approximator that can be used to learn complex relationships between inputs and outputs. It is a learning machine that mimics biological neural networks in the human brain. Mathematically, given an input \\(x\\in\\mathbb{R}^{n_x}\\), a neural nerwork computes an ouput \\(\\hat y\\in\\mathbb{R}^{n_y}\\) as follows, \\[\\begin{align*}\na^{[1]} &= \\sigma(W^{[1]\\top} x + b^{[1]}) \\\\\na^{[2]} &= \\sigma(W^{[2]\\top} a^{[1]} + b^{[2]}) \\\\\n&\\vdots \\\\\na^{[L]} &= \\sigma(W^{[L]\\top} a^{[L-1]} + b^{[L]}) \\\\\n\\hat y &= \\sigma(W^{[L+1]\\top} a^{[L]} + b^{[L+1]})\n\\end{align*}\\]\nHere \\(\\sigma:\\mathbb{R}\\to\\mathbb{R}\\) is any nonlinear differentiable function (or sufficiently close to be such), \\(L\\) is the number of hidden layers, \\(W\\) and \\(b\\) are weight matrices and bias vectors. We use \\(n_l\\) to denote the number of hidden nodes in the \\(l\\)-th layer. The pre-activation vector at layer \\(l\\) is denoted by \\(z^{[l]}\\).\nFrom the definition we see that \\(\\hat y\\) is not only a function of \\(x\\), but also a function of \\(z^{[1]}, z^{[2]}\\) and so forth using sub-networks. This may seem like stating the obvious, but it’s a useful fact to keep in mind when we derive the algorithm for training these networks."
  },
  {
    "objectID": "posts/deep-dive/saws.html#training",
    "href": "posts/deep-dive/saws.html#training",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Training",
    "text": "Training\nHow to train a neural network to make good predictions? Well, we first need to specify a computable objective. To achieve this we introduce a loss function that measures how good or bad a predictor is compared to the ground truth. This is called supervised learning. A commonly used loss is the squared loss \\[\\begin{align*}\n    \\ell(u,v) =  \\frac{1}{2} \\|u-v\\|^2\n\\end{align*}\\] where \\(\\|\\cdot\\|\\) is the Euclidean norm. Then our goal is to minimize \\(J=\\ell(y,\\hat y)\\).\nA general-purpose optimization method is gradient descent. In every iteration step, this method updates the parameters (i.e. weights and biases) by moving along the opposite of the gradient of the loss with respect to the parameters. Due to the characterization of the gradient of a function as being the direction along which the function increases the most (infinitesimally speaking), this method is heuristically justified for minimizing an objective function when the step size (aka learning rate) is not too large.\nA crucial point is that we need to compute all the partial derivatives for every gradient descent step! Mathematically this is tedious but not hard at all. Everyone knows that to differentiate a composition of functions, the chain rule is our best friend. Sure, we have multiple compositions, but it does not produce any conceptual complications because we can just apply the chain rule multiple times.\nTake weight matrix \\(W^{[1]}\\) as an example. Any small nudge on its value would result in changes in \\(z^{[1]}\\), and once we have the value of \\(z^{[1]}\\), we feed it into the sub-network made of layers \\(1\\) to \\(L+1\\) and get the output. Hence \\(J=g(z^{[1]})\\) for some \\(g\\). By the chain rule,\n\\[\\begin{align*}\n    \\frac{\\partial J}{\\partial W^{[1]}} =  \n    \\sum_i \\frac{\\partial J}{\\partial z^{[1]}_i} \\frac{\\partial z^{[1]}_i}{\\partial W^{[1]}}.\n\\end{align*}\\]\nThe second gradient in the summand is the rather simple because \\(z^{[1]}\\) is a linear function of \\(W^{[1]}\\). However, the first gradient is not explicit because the function \\(g\\) is cumbersome as a composition of compositions of compositions … What we can do is to apply chain rule again, then \\[\\begin{align*}\n    \\frac{\\partial J}{\\partial z^{[1]}}\n    = \\sum_i \\frac{\\partial J}{\\partial z^{[2]}_i} \\frac{\\partial z^{[2]}_i}{\\partial z^{[1]}}\n\\end{align*}\\]\nRecalling \\(z^{[2]} = W^{[2]\\top} \\sigma(z^{[1]}) + b^{[2]}\\), the second gradient is easy to calculate. Hence, the gradient of \\(J\\) with respect to the first layer pre-activation is a linear combination of the gradient with respect to the second layer pre-activation. Applying the chain rule recursively in the forward direction all the way to the output layer, we can express \\(\\frac{\\partial J}{\\partial z^{[1]}}\\) as a multiple sum over \\(\\frac{\\partial J}{\\partial z^{[L+1]}}\\) times multiple products of gradients of consecutive pre-activations.\nFollowing the same recipe, we can compute the gradients with respect to weights and biases of all the layers. In summary, we would need for all \\(l=1,\\ldots,L+1\\):\nand chain them together using multiple sums and products. This seems like a lot of work, even for a computer!\nHere comes an important observation. There are lots of redundant computations if we use the recipe just described to compute an explicit form for all the gradients at every layer.\nThe big idea is to take advantage of the recursive relations between gradients with respect to pre-activations of consecutive layers. To be more precise, let’s rewrite both equations at a general layer (we also include an equation for the biases).\n\\[\n\\frac{\\partial J}{\\partial W^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial W^{[l]}}.\n\\tag{1}\\]\n\\[\\frac{\\partial J}{\\partial b^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial b^{[l]}}\n\\tag{2}\\]\n\\[\\frac{\\partial J}{\\partial z^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l+1]}_i} \\frac{\\partial z^{[l+1]}_i}{\\partial z^{[l]}}\n\\tag{3}\\]\nLet \\(S^{[l]} = \\frac{\\partial J}{\\partial z^{[l]}}\\). We use equations Equation 1 and Equation 2, along with \\(S^{[L+1]}\\) (easy to compute), to find the required gradients for updating \\(W^{[L+1]}\\) and \\(b^{[L+1]}\\). Then we use equation Equation 3 to find \\(S^{[L]}\\), which can be plugged back into equations Equation 1 and Equation 2 to get the required gradient for updating \\(W^{[L]}\\) and \\(b^{[L]}\\), and so on.\nWhat we just described is the famous backpropagation. The advantage of this approach is that we compute each basic computation (itemized above) only once for each gradient descent iteration.\nFrom layer to layer, the computation is done sequentially, because output of \\(l+1\\)-th layer is requiredd as the input for computing gradients of \\(l\\)-th layer. For each fixed \\(l\\), however, it is better to parallelise the computation and write Equation 1 -Equation 3 in matrix forms. Here is how we do it:\n\nSquared loss\nSet \\(A^{[l]}=\\mathrm{diag}(\\sigma'(z^{[l]}))\\) and \\(e = y - \\hat y\\). It is easy to see that \\[\\begin{align*}\n    S^{[L+1]} = - A^{[L+1]} e\n\\end{align*}\\] Equation Equation 3 can be written in matrix form as well \\[\nS^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}\n\\tag{4}\\] Now the gradients \\[\\begin{align*}\n    dW^{[l]} &= a^{[l-1]} S^{[l]\\top} \\\\\n    db^{[l]} &= S^{[l]}\n\\end{align*}\\]\nEquation 4 is what I meant by secret “SAWS” of deep learning!\n\n\nCross entropy loss\nThe XE loss is defined for two probability mass functions as follows \\[\\begin{align*}\n    \\ell(u,v) = -\\sum_k u_k\\log v_k.\n\\end{align*}\\] It is particularly well suited for multiclass classification problem. To ensure that \\(\\hat y\\) is a probability mass function. We use the softmax activation function at the output layer \\[\\begin{align*}\n    \\mathrm{softmax}(x) =  \\frac{e^{x}}{\\sum_i e^{x_i}}\n\\end{align*}\\] All we have to do is to change the computation of \\(S^{[L+1]}\\), namely the gradient of the XE loss with respect to the output layer pre-activation, then back propagate using the last three equations in the squared loss case. A routine application of chain rule yields that \\[\\begin{align*}\n    S^{[L+1]} = - e\n\\end{align*}\\] where \\(e\\) was defined earlier in the squared loss case."
  },
  {
    "objectID": "posts/deep-dive/saws.html#python-implementation",
    "href": "posts/deep-dive/saws.html#python-implementation",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Python Implementation",
    "text": "Python Implementation\nHere is a python implementation of the training with min-batch SGD, 1 hidden layer, sigmoid activation in the hidden and output layers, and squared loss.\n\nimport numpy as np\n\ndef sig(x):\n    return (1+np.exp(-x))**-1\n\ng = np.random.default_rng(1123)\n\nd,m = 20, 1000\nd_out = 10\nd_hid = 20\nbatch_size = 8\nn_epoch = 2\nlr = 0.01\n\nX_train = g.normal(0,1,(d,m))\nY_train = g.uniform(size=(d_out,m))\n\nW1 = g.normal(0,1,(d,d_hid))\nW2 = g.normal(0,1,(d_hid,d_out))\nb1 = g.normal(0,1,(d_hid,1))*0.01\nb2 = g.normal(0,1,(d_out,1))*0.01\n\nerrors=[]\n\nfor epoch in range(n_epoch):\n    error = 0\n    # permute the data\n    perm_idx = g.permutation(m)\n    X_train = X_train[:,perm_idx]\n    Y_train = Y_train[:,perm_idx]\n    for bat in range(m//batch_size):\n        x_batch = X_train[:,bat*batch_size:(1+bat)*batch_size]\n        y_batch = Y_train[:,bat*batch_size:(1+bat)*batch_size]\n        # forward pass\n        z1 = np.matmul(W1.T,x_batch) + b1\n        a1 = sig(z1)\n        z2 = np.matmul(W2.T,a1) + b2\n        a2 = sig(z2)\n        # backward pass\n        e = y_batch - a2\n        A2 = sig(z2)*(1-sig(z2))\n        S2 = - A2*e\n        A1 = sig(z1)*(1-sig(z1))\n        S1 = A1*np.matmul(W2,S2)\n        # gradient descent\n        dW2 = np.matmul(a1,S2.T)\n        db2 = np.sum(S2, axis = 1, keepdims=True)\n        dW1 = np.matmul(x_batch,S1.T)\n        db1 = np.sum(S1, axis = 1, keepdims=True)\n        W2 -= lr * dW2\n        b2 -= lr * db2\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        # compute the error \n        error += 0.5*np.sum(e*e)\n    # report error of the current epoch\n    print(\"Epoch:\", epoch, \"SE:\", error)\n    errors.append(error)\n\nEpoch: 0 SE: 1084.095701311093\nEpoch: 1 SE: 931.4561396806077\n\n\nExercise: implement the training with XE loss and more hidden layers."
  },
  {
    "objectID": "posts/deep-dive/uml.html",
    "href": "posts/deep-dive/uml.html",
    "title": "Understanding Machine Learning: a series of 20 videos",
    "section": "",
    "text": "Since October 2022, I have created a series of 20 videos (in Chinese) on machine learning theory and algorithms on bilibili.com, closely following the excellent textbook written by Shalev-Shwartz and Ben-David, which is freely available on the authors’ webpage.\n\nPlaylist in my YouTube channel\nTopics covered include\n\nProbably Approximately Correct (PAC) learning model\nVC dimension\nno free lunch theorem\nlinear predictors: logistic regression, linear regression, perceptron\nconvex learning problems\nregularisation\nstochastic gradient descent\nsupport vector machine\nkernel methods\ndecision trees\nboosting: AdaBoost\nclustering: k-means, spectral clustering, linkage-based clustering\ndimenionality reduction: PCA, compressed sensing\ngenerative models: naive Bayes, LDA, EM algorithms"
  }
]