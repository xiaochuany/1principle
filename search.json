[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/deep-dive/index.html",
    "href": "posts/deep-dive/index.html",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "",
    "text": "Everyone knows that a neural network is a universal function approximator that can be used to learn complex relationships between inputs and outputs. It is a learning machine that mimics biological neural networks in the human brain. Mathematically, given an input \\(x\\in\\mathbb{R}^{n_x}\\), a neural nerwork computes an ouput \\(\\hat y\\in\\mathbb{R}^{n_y}\\) as follows, \\[\\begin{align*}\na^{[1]} &= \\sigma(W^{[1]\\top} x + b^{[1]}) \\\\\na^{[2]} &= \\sigma(W^{[2]\\top} a^{[1]} + b^{[2]}) \\\\\n&\\vdots \\\\\na^{[L]} &= \\sigma(W^{[L]\\top} a^{[L-1]} + b^{[L]}) \\\\\n\\hat y &= \\sigma(W^{[L+1]\\top} a^{[L]} + b^{[L+1]})\n\\end{align*}\\]\nHere \\(\\sigma:\\mathbb{R}\\to\\mathbb{R}\\) is any nonlinear differentiable function (or sufficiently close to be such), \\(L\\) is the number of hidden layers, \\(W\\) and \\(b\\) are weight matrices and bias vectors. We use \\(n_l\\) to denote the number of hidden nodes in the \\(l\\)-th layer. The pre-activation vector at layer \\(l\\) is denoted by \\(z^{[l]}\\).\nFrom the definition we see that \\(\\hat y\\) is not only a function of \\(x\\), but also a function of \\(z^{[1]}, z^{[2]}\\) and so forth using sub-networks. This may seem like stating the obvious, but it’s a useful fact to keep in mind when we derive the algorithm for training these networks."
  },
  {
    "objectID": "posts/deep-dive/index.html#sec-training",
    "href": "posts/deep-dive/index.html#sec-training",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Training",
    "text": "Training\nHow to train a neural network to make good predictions? Well, we first need to specify a computable objective. To achieve this we introduce a loss function that measures how good or bad a predictor is compared to the ground truth. This is called supervised learning. A commonly used loss is the squared loss \\[\\begin{align*}\n    \\ell(u,v) =  \\frac{1}{2} \\|u-v\\|^2\n\\end{align*}\\] where \\(\\|\\cdot\\|\\) is the Euclidean norm. Then our goal is to minimize \\(J=\\ell(y,\\hat y)\\).\nA general-purpose optimization method is gradient descent. In every iteration step, this method updates the parameters (i.e. weights and biases) by moving along the opposite of the gradient of the loss with respect to the parameters. Due to the characterization of the gradient of a function as being the direction along which the function increases the most (infinitesimally speaking), this method is heuristically justified for minimizing an objective function when the step size (aka learning rate) is not too large.\nA crucial point is that we need to compute all the partial derivatives for every gradient descent step! Mathematically this is tedious but not hard at all. Everyone knows that to differentiate a composition of functions, the chain rule is our best friend. Sure, we have multiple compositions, but it does not produce any conceptual complications because we can just apply the chain rule multiple times.\nTake weight matrix \\(W^{[1]}\\) as an example. Any small nudge on its value would result in changes in \\(z^{[1]}\\), and once we have the value of \\(z^{[1]}\\), we feed it into the sub-network made of layers \\(1\\) to \\(L+1\\) and get the output. Hence \\(J=g(z^{[1]})\\) for some \\(g\\). By the chain rule,\n\\[\\begin{align*}\n    \\frac{\\partial J}{\\partial W^{[1]}} =  \n    \\sum_i \\frac{\\partial J}{\\partial z^{[1]}_i} \\frac{\\partial z^{[1]}_i}{\\partial W^{[1]}}.\n\\end{align*}\\]\nThe second gradient in the summand is the rather simple because \\(z^{[1]}\\) is a linear function of \\(W^{[1]}\\). However, the first gradient is not explicit because the function \\(g\\) is cumbersome as a composition of compositions of compositions … What we can do is to apply chain rule again, then \\[\\begin{align*}\n    \\frac{\\partial J}{\\partial z^{[1]}}\n    = \\sum_i \\frac{\\partial J}{\\partial z^{[2]}_i} \\frac{\\partial z^{[2]}_i}{\\partial z^{[1]}}\n\\end{align*}\\]\nRecalling \\(z^{[2]} = W^{[2]\\top} \\sigma(z^{[1]}) + b^{[2]}\\), the second gradient is easy to calculate. Hence, the gradient of \\(J\\) with respect to the first layer pre-activation is a linear combination of the gradient with respect to the second layer pre-activation. Applying the chain rule recursively in the forward direction all the way to the output layer, we can express \\(\\frac{\\partial J}{\\partial z^{[1]}}\\) as a multiple sum over \\(\\frac{\\partial J}{\\partial z^{[L+1]}}\\) times multiple products of gradients of consecutive pre-activations.\nFollowing the same recipe, we can compute the gradients with respect to weights and biases of all the layers. In summary, we would need for all \\(l=1,\\ldots,L+1\\):\nand chain them together using multiple sums and products. This seems like a lot of work, even for a computer!\nHere comes an important observation. There are lots of redundant computations if we use the recipe just described to compute an explicit form for all the gradients at every layer.\nThe big idea is to take advantage of the recursive relations between gradients with respect to pre-activations of consecutive layers. To be more precise, let’s rewrite both equations at a general layer (we also include an equation for the biases).\n\\[\n\\frac{\\partial J}{\\partial W^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial W^{[l]}}.\n\\tag{1}\\]\n\\[\\frac{\\partial J}{\\partial b^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial b^{[l]}}\n\\tag{2}\\]\n\\[\\frac{\\partial J}{\\partial z^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l+1]}_i} \\frac{\\partial z^{[l+1]}_i}{\\partial z^{[l]}}\n\\tag{3}\\]\nLet \\(S^{[l]} = \\frac{\\partial J}{\\partial z^{[l]}}\\). We use equations Equation 1 and Equation 2, along with \\(S^{[L+1]}\\) (easy to compute), to find the required gradients for updating \\(W^{[L+1]}\\) and \\(b^{[L+1]}\\). Then we use equation Equation 3 to find \\(S^{[L]}\\), which can be plugged back into equations Equation 1 and Equation 2 to get the required gradient for updating \\(W^{[L]}\\) and \\(b^{[L]}\\), and so on.\nWhat we just described is the famous backpropagation. The advantage of this approach is that we compute each basic computation (itemized above) only once for each gradient descent iteration.\nFrom layer to layer, the computation is done sequentially, because output of \\(l+1\\)-th layer is requiredd as the input for computing gradients of \\(l\\)-th layer. For each fixed \\(l\\), however, it is better to parallelise the computation and write Equation 1 -Equation 3 in matrix forms. Here is how we do it:\n\nSquared loss\nSet \\(A^{[l]}=\\mathrm{diag}(\\sigma'(z^{[l]}))\\) and \\(e = y - \\hat y\\). It is easy to see that \\[\\begin{align*}\n    S^{[L+1]} = - A^{[L+1]} e\n\\end{align*}\\] Equation Equation 3 can be written in matrix form as well \\[\nS^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}\n\\tag{4}\\] Now the gradients \\[\\begin{align*}\n    dW^{[l]} &= a^{[l-1]} S^{[l]\\top} \\\\\n    db^{[l]} &= S^{[l]}\n\\end{align*}\\]\nEquation 4 is what I meant by secret “SAWS” of deep learning!\n\n\nCross entropy loss\nThe XE loss is defined for two probability mass functions as follows \\[\\begin{align*}\n    \\ell(u,v) = -\\sum_k u_k\\log v_k.\n\\end{align*}\\] It is particularly well suited for multiclass classification problem. To ensure that \\(\\hat y\\) is a probability mass function. We use the softmax activation function at the output layer \\[\\begin{align*}\n    \\mathrm{softmax}(x) =  \\frac{e^{x}}{\\sum_i e^{x_i}}\n\\end{align*}\\] All we have to do is to change the computation of \\(S^{[L+1]}\\), namely the gradient of the XE loss with respect to the output layer pre-activation, then back propagate using the last three equations in the squared loss case. A routine application of chain rule yields that \\[\\begin{align*}\n    S^{[L+1]} = - e\n\\end{align*}\\] where \\(e\\) was defined earlier in the squared loss case."
  },
  {
    "objectID": "posts/deep-dive/index.html#implementation",
    "href": "posts/deep-dive/index.html#implementation",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Implementation",
    "text": "Implementation\nHere is a python implementation of the training with min-batch SGD, 1 hidden layer, sigmoid activation in the hidden and output layers, and squared loss.\n\nimport numpy as np\ng = np.random.default_rng(11)\ng.chisquare(3,(5,))\n\narray([2.40798032, 6.10497521, 1.74702188, 3.79315713, 4.34725571])\n\n\nExercise: implement the training with XE loss, more hidden layers, and the tricks we will mention in the next section."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am mathematician & data sciensist based in London and Luxembourg."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "a blog about data science, machine learning and mathematics",
    "section": "",
    "text": "Secret ‘SAWS’ of deep learning, feature backprop\n\n\n\n\n\n\n\ndeep learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\ndemo\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nXY\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gist/demo.html",
    "href": "posts/gist/demo.html",
    "title": "demo",
    "section": "",
    "text": "import scipy as sp\nimport numpy as np\nnp.ones((4,5))\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/gist/demo.html#new-sect",
    "href": "posts/gist/demo.html#new-sect",
    "title": "demo",
    "section": "new sect",
    "text": "new sect\nthis is some tinterseting stuff\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution."
  }
]