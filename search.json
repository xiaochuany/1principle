[
  {
    "objectID": "posts/deep-dive/uml-recap1.html",
    "href": "posts/deep-dive/uml-recap1.html",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "",
    "text": "In a supervised learning, we specify\n\nan example space \\(\\mathcal X\\)\n\na label space \\(\\mathcal Y\\)\na collection of hypotheses \\(h: \\mathcal X \\to \\mathcal Y\\), making up the hypothesis class (inductive bias), from which we want to pick a predictor\na loss function \\(\\ell: \\mathcal Y \\times \\mathcal Y \\to \\mathbb{R}_+\\), quantifying how good or bad a prediction \\(\\hat y\\) is compared to the ground truth label \\(y\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe are given an independent and identically distributed input-output pairs \\(S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y\\) with distribution \\(D\\).\n\n\nOur goal is to pick the “best” predictor in the sense of minimising the true loss\n\\[\nL_D(h) = \\mathbb{E}_{(x,y)\\sim D}[\\ell(h(x),y))]  \n\\] where \\(D\\) is the true distribution of \\((x,y)\\).\nObviously a priori the distribution \\(D\\) of the samples is unkonwn. However by law of large numbers we know that \\[\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n\\] is a consistent estimator of \\(L_D(h)\\) (when \\(m\\to\\infty\\) under mild condition on the distribution of \\(\\ell(h(x),y)\\)). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \\[\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n\\] Hence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when \\(m\\) is large, for a fixed \\(h\\). Whether such approximation is valid uniformly for all the hypothesis in \\(\\mathcal H\\), in other words, whether \\(h_S\\approx h^*\\in \\mathrm{argmin}_{h\\in\\mathcal H} L_D(h)\\), is at the centre of the so-called learning theory.\nWe often decompose the generalisation error \\(L_D(h_S)\\) in two parts: \\[\nL_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)]\n\\] the first is called the approximation error, and second estimation error."
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#setting-up-the-scene",
    "href": "posts/deep-dive/uml-recap1.html#setting-up-the-scene",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "",
    "text": "In a supervised learning, we specify\n\nan example space \\(\\mathcal X\\)\n\na label space \\(\\mathcal Y\\)\na collection of hypotheses \\(h: \\mathcal X \\to \\mathcal Y\\), making up the hypothesis class (inductive bias), from which we want to pick a predictor\na loss function \\(\\ell: \\mathcal Y \\times \\mathcal Y \\to \\mathbb{R}_+\\), quantifying how good or bad a prediction \\(\\hat y\\) is compared to the ground truth label \\(y\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe are given an independent and identically distributed input-output pairs \\(S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y\\) with distribution \\(D\\).\n\n\nOur goal is to pick the “best” predictor in the sense of minimising the true loss\n\\[\nL_D(h) = \\mathbb{E}_{(x,y)\\sim D}[\\ell(h(x),y))]  \n\\] where \\(D\\) is the true distribution of \\((x,y)\\).\nObviously a priori the distribution \\(D\\) of the samples is unkonwn. However by law of large numbers we know that \\[\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n\\] is a consistent estimator of \\(L_D(h)\\) (when \\(m\\to\\infty\\) under mild condition on the distribution of \\(\\ell(h(x),y)\\)). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \\[\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n\\] Hence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when \\(m\\) is large, for a fixed \\(h\\). Whether such approximation is valid uniformly for all the hypothesis in \\(\\mathcal H\\), in other words, whether \\(h_S\\approx h^*\\in \\mathrm{argmin}_{h\\in\\mathcal H} L_D(h)\\), is at the centre of the so-called learning theory.\nWe often decompose the generalisation error \\(L_D(h_S)\\) in two parts: \\[\nL_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)]\n\\] the first is called the approximation error, and second estimation error."
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#lets-be-concrete",
    "href": "posts/deep-dive/uml-recap1.html#lets-be-concrete",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "Let’s be concrete",
    "text": "Let’s be concrete\nFrom a practical point of view, we may not want to get into the learning theory bounds despite its elegance, what we do is to split the sample \\(S^0\\) into two parts \\(S, V\\), where \\(S\\) is used to find an ERM which we denote by \\(h_S\\), another for testing whether the found ERM achieves small \\(L_D(h_S)\\). The rationale is simple, since we make iid assumption, \\(V\\) is independent of \\(S\\), hence \\(L_{V}(h_S) \\approx L_D(h_S)\\) when \\(|V|\\) is not too small. Therefore, the smallness of \\(L_{V}(h_S)\\) indicates good quality (small true risk) of our predictor \\(h_1\\).\nThe split method for arrays is implemented in sklearn.model_selection\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\ng = np.random.default_rng(12)\nx = g.normal(0,1,(20,))\nx_tr, x_te = train_test_split(x,test_size=0.2,random_state=12)\n\nTypically, examples are vectors in \\(\\mathbb{R}^d, d\\ge 1\\). In \\(k\\)-way (\\(k\\ge 2\\)) classification problems, labels are one-hot encodings \\(e_1,..., e_k\\) where \\(e_i\\) is the unit vector in \\(\\mathbb{R}^k\\) with one on the \\(i\\) th coordinate and zero elsewhere. If \\(k=2\\), we can drop the second coordinate and simply denote the two classes by \\(\\{1, -1\\}\\) or \\(\\{0,1\\}\\). In regression problems, the labels are in the continuum \\(\\mathbb{R}^k, k\\ge 1\\).\nNow consider \\(\\mathcal H\\). For classification problems, instead of predicting discrete class labels directly, it is sometimes beneficial to predict a probability mass function over the \\(k\\) classes. From the pmf, a label can be obtained by taking argmax. In other words, the range of \\(h\\in\\mathcal H\\) is assumed to be \\(\\{y\\in\\mathbb{R}^k: y_i\\ge 0, y_1+...+y_k=1\\}\\). For regression problems there are no such constraints and the range can be the whole \\(\\mathbb{R}^k\\).\nThe choie of the loss function may vary, depending on what goal we are trying to achiecve. Researchers can design new losses suitable for their use case. Here we mention a few popular ones. For classification problems, if we use hypothesis predicting pmf, the cross entropy loss is often a good choice \\[\nXE(p,\\hat p) =  - \\sum_{i=1}^k p_i \\log(\\hat p_i)\n\\] For regression problems, the squared loss is often a good choice \\[SE(y,\\hat y)= \\|y-\\hat y\\|^2_2\\] where \\(\\|.\\|\\) is Euclidean norm.\nOne of the advantages of these loss functions is that they are convex in the \\(\\hat p\\) or \\(\\hat y\\) variables, making it possible to leverage the machinery of convex optimistion when it comes to the actual training process.\nMany loss functions are already implemented in sklearn.metrics. The XE is named log_loss and the squared loss is named mean_sqquared_error. Let’s list all of them.\n\nimport sklearn.metrics as metrics\nfor m in dir(metrics):\n    if not m.startswith('_'): print(m)\n\nConfusionMatrixDisplay\nDetCurveDisplay\nDistanceMetric\nPrecisionRecallDisplay\nPredictionErrorDisplay\nRocCurveDisplay\naccuracy_score\nadjusted_mutual_info_score\nadjusted_rand_score\nauc\naverage_precision_score\nbalanced_accuracy_score\nbrier_score_loss\ncalinski_harabasz_score\ncheck_scoring\nclass_likelihood_ratios\nclassification_report\ncluster\ncohen_kappa_score\ncompleteness_score\nconfusion_matrix\nconsensus_score\ncoverage_error\nd2_absolute_error_score\nd2_pinball_score\nd2_tweedie_score\ndavies_bouldin_score\ndcg_score\ndet_curve\neuclidean_distances\nexplained_variance_score\nf1_score\nfbeta_score\nfowlkes_mallows_score\nget_scorer\nget_scorer_names\nhamming_loss\nhinge_loss\nhomogeneity_completeness_v_measure\nhomogeneity_score\njaccard_score\nlabel_ranking_average_precision_score\nlabel_ranking_loss\nlog_loss\nmake_scorer\nmatthews_corrcoef\nmax_error\nmean_absolute_error\nmean_absolute_percentage_error\nmean_gamma_deviance\nmean_pinball_loss\nmean_poisson_deviance\nmean_squared_error\nmean_squared_log_error\nmean_tweedie_deviance\nmedian_absolute_error\nmultilabel_confusion_matrix\nmutual_info_score\nnan_euclidean_distances\nndcg_score\nnormalized_mutual_info_score\npair_confusion_matrix\npairwise\npairwise_distances\npairwise_distances_argmin\npairwise_distances_argmin_min\npairwise_distances_chunked\npairwise_kernels\nprecision_recall_curve\nprecision_recall_fscore_support\nprecision_score\nr2_score\nrand_score\nrecall_score\nroc_auc_score\nroc_curve\nsilhouette_samples\nsilhouette_score\ntop_k_accuracy_score\nv_measure_score\nzero_one_loss\n\n\nIt comes in handy that sklearn has them already defined. Implementing each one of them is often not hard, e.g.\n\ndef log_loss(yt,yp):\n    yt[yt==0]= 1-yp[yt==0]\n    return -np.log(y).mean()"
  },
  {
    "objectID": "posts/deep-dive/uml-recap1.html#analysis-of-errors",
    "href": "posts/deep-dive/uml-recap1.html#analysis-of-errors",
    "title": "Machine Learning Recap 1 Concepts",
    "section": "Analysis of errors",
    "text": "Analysis of errors\nIn practice, \\(D\\) is unknown and we only observe the training error \\(L_S(h_S)\\) and the validation error \\(L_V(h_S)\\). Hence we decompose the generalisation error differently \\[\nL_D(h_S) = [L_D(h_S) - L_V(h_S)] + [L_V(h_S) - L_S(h_S)] + L_S(h_S)\n\\] The first term is small when \\(|V|\\) is moderately large by independence of \\(V\\) and \\(S\\). The second and third term are observalbe and several cases may arise.\n\nthe gap is small and the training error is small. This is a happy scenario\nthe training error is large. To address this, we may consider\n\nengarge the hypothesis class,\ncompletely changing it,\nfind a better feature representation,\nfind a better optimiser\n\n\ntraining error is small but the gap is large. To address this, we may consider\n\nadd regularisation\nget more training data\nreduce the hypothesis class\n\n\nIt is benefiical to plot the learning curve during training. This amounts to visualise the training error and validation error on the same plot as time evolves (every X batches, every X epoch etc)."
  },
  {
    "objectID": "posts/deep-dive/saws.html",
    "href": "posts/deep-dive/saws.html",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "",
    "text": "My colleague Simon Shaw came up with the memorable notation SAWS for the key recursion step in backprop in a Year 2 module on deep learning. I found this a perfect example of “good notation helps memorisation” which hopefully reinforces understanding. In this post, I will explain how we get to this recursion and end the post with a python implementation."
  },
  {
    "objectID": "posts/deep-dive/saws.html#multi-layer-perceptron",
    "href": "posts/deep-dive/saws.html#multi-layer-perceptron",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Multi-Layer Perceptron",
    "text": "Multi-Layer Perceptron\nEveryone knows that a neural network is a universal function approximator that can be used to learn complex relationships between inputs and outputs. It is a learning machine that mimics biological neural networks in the human brain. Mathematically, given an input \\(x\\in\\mathbb{R}^{n_x}\\), a neural nerwork computes an ouput \\(\\hat y\\in\\mathbb{R}^{n_y}\\) as follows, \\[\\begin{align*}\na^{[1]} &= \\sigma(W^{[1]\\top} x + b^{[1]}) \\\\\na^{[2]} &= \\sigma(W^{[2]\\top} a^{[1]} + b^{[2]}) \\\\\n&\\vdots \\\\\na^{[L]} &= \\sigma(W^{[L]\\top} a^{[L-1]} + b^{[L]}) \\\\\n\\hat y &= \\sigma(W^{[L+1]\\top} a^{[L]} + b^{[L+1]})\n\\end{align*}\\]\nHere \\(\\sigma:\\mathbb{R}\\to\\mathbb{R}\\) is any nonlinear differentiable function (or sufficiently close to be such), \\(L\\) is the number of hidden layers, \\(W\\) and \\(b\\) are weight matrices and bias vectors. We use \\(n_l\\) to denote the number of hidden nodes in the \\(l\\)-th layer. The pre-activation vector at layer \\(l\\) is denoted by \\(z^{[l]}\\).\nFrom the definition we see that \\(\\hat y\\) is not only a function of \\(x\\), but also a function of \\(z^{[1]}, z^{[2]}\\) and so forth using sub-networks. This may seem like stating the obvious, but it’s a useful fact to keep in mind when we derive the algorithm for training these networks."
  },
  {
    "objectID": "posts/deep-dive/saws.html#training",
    "href": "posts/deep-dive/saws.html#training",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Training",
    "text": "Training\nHow to train a neural network to make good predictions? Well, we first need to specify a computable objective. To achieve this we introduce a loss function that measures how good or bad a predictor is compared to the ground truth. This is called supervised learning. A commonly used loss is the squared loss \\[\\begin{align*}\n    \\ell(u,v) =  \\frac{1}{2} \\|u-v\\|^2\n\\end{align*}\\] where \\(\\|\\cdot\\|\\) is the Euclidean norm. Then our goal is to minimize \\(J=\\ell(y,\\hat y)\\).\nA general-purpose optimization method is gradient descent. In every iteration step, this method updates the parameters (i.e. weights and biases) by moving along the opposite of the gradient of the loss with respect to the parameters. Due to the characterization of the gradient of a function as being the direction along which the function increases the most (infinitesimally speaking), this method is heuristically justified for minimizing an objective function when the step size (aka learning rate) is not too large.\nA crucial point is that we need to compute all the partial derivatives for every gradient descent step! Mathematically this is tedious but not hard at all. Everyone knows that to differentiate a composition of functions, the chain rule is our best friend. Sure, we have multiple compositions, but it does not produce any conceptual complications because we can just apply the chain rule multiple times.\nTake weight matrix \\(W^{[1]}\\) as an example. Any small nudge on its value would result in changes in \\(z^{[1]}\\), and once we have the value of \\(z^{[1]}\\), we feed it into the sub-network made of layers \\(1\\) to \\(L+1\\) and get the output. Hence \\(J=g(z^{[1]})\\) for some \\(g\\). By the chain rule,\n\\[\\begin{align*}\n    \\frac{\\partial J}{\\partial W^{[1]}} =  \n    \\sum_i \\frac{\\partial J}{\\partial z^{[1]}_i} \\frac{\\partial z^{[1]}_i}{\\partial W^{[1]}}.\n\\end{align*}\\]\nThe second gradient in the summand is the rather simple because \\(z^{[1]}\\) is a linear function of \\(W^{[1]}\\). However, the first gradient is not explicit because the function \\(g\\) is cumbersome as a composition of compositions of compositions … What we can do is to apply chain rule again, then \\[\\begin{align*}\n    \\frac{\\partial J}{\\partial z^{[1]}}\n    = \\sum_i \\frac{\\partial J}{\\partial z^{[2]}_i} \\frac{\\partial z^{[2]}_i}{\\partial z^{[1]}}\n\\end{align*}\\]\nRecalling \\(z^{[2]} = W^{[2]\\top} \\sigma(z^{[1]}) + b^{[2]}\\), the second gradient is easy to calculate. Hence, the gradient of \\(J\\) with respect to the first layer pre-activation is a linear combination of the gradient with respect to the second layer pre-activation. Applying the chain rule recursively in the forward direction all the way to the output layer, we can express \\(\\frac{\\partial J}{\\partial z^{[1]}}\\) as a multiple sum over \\(\\frac{\\partial J}{\\partial z^{[L+1]}}\\) times multiple products of gradients of consecutive pre-activations.\nFollowing the same recipe, we can compute the gradients with respect to weights and biases of all the layers. In summary, we would need for all \\(l=1,\\ldots,L+1\\):\nand chain them together using multiple sums and products. This seems like a lot of work, even for a computer!\nHere comes an important observation. There are lots of redundant computations if we use the recipe just described to compute an explicit form for all the gradients at every layer.\nThe big idea is to take advantage of the recursive relations between gradients with respect to pre-activations of consecutive layers. To be more precise, let’s rewrite both equations at a general layer (we also include an equation for the biases).\n\\[\n\\frac{\\partial J}{\\partial W^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial W^{[l]}}.\n\\tag{1}\\]\n\\[\\frac{\\partial J}{\\partial b^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial b^{[l]}}\n\\tag{2}\\]\n\\[\\frac{\\partial J}{\\partial z^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l+1]}_i} \\frac{\\partial z^{[l+1]}_i}{\\partial z^{[l]}}\n\\tag{3}\\]\nLet \\(S^{[l]} = \\frac{\\partial J}{\\partial z^{[l]}}\\). We use equations Equation 1 and Equation 2, along with \\(S^{[L+1]}\\) (easy to compute), to find the required gradients for updating \\(W^{[L+1]}\\) and \\(b^{[L+1]}\\). Then we use equation Equation 3 to find \\(S^{[L]}\\), which can be plugged back into equations Equation 1 and Equation 2 to get the required gradient for updating \\(W^{[L]}\\) and \\(b^{[L]}\\), and so on.\nWhat we just described is the famous backpropagation. The advantage of this approach is that we compute each basic computation (itemized above) only once for each gradient descent iteration.\nFrom layer to layer, the computation is done sequentially, because output of \\(l+1\\)-th layer is requiredd as the input for computing gradients of \\(l\\)-th layer. For each fixed \\(l\\), however, it is better to parallelise the computation and write Equation 1 -Equation 3 in matrix forms. Here is how we do it:\n\nSquared loss\nSet \\(A^{[l]}=\\mathrm{diag}(\\sigma'(z^{[l]}))\\) and \\(e = y - \\hat y\\). It is easy to see that \\[\\begin{align*}\n    S^{[L+1]} = - A^{[L+1]} e\n\\end{align*}\\] Equation Equation 3 can be written in matrix form as well \\[\nS^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}\n\\tag{4}\\] Now the gradients \\[\\begin{align*}\n    dW^{[l]} &= a^{[l-1]} S^{[l]\\top} \\\\\n    db^{[l]} &= S^{[l]}\n\\end{align*}\\]\nEquation 4 is what I meant by secret “SAWS” of deep learning!\n\n\nCross entropy loss\nThe XE loss is defined for two probability mass functions as follows \\[\\begin{align*}\n    \\ell(u,v) = -\\sum_k u_k\\log v_k.\n\\end{align*}\\] It is particularly well suited for multiclass classification problem. To ensure that \\(\\hat y\\) is a probability mass function. We use the softmax activation function at the output layer \\[\\begin{align*}\n    \\mathrm{softmax}(x) =  \\frac{e^{x}}{\\sum_i e^{x_i}}\n\\end{align*}\\] All we have to do is to change the computation of \\(S^{[L+1]}\\), namely the gradient of the XE loss with respect to the output layer pre-activation, then back propagate using the last three equations in the squared loss case. A routine application of chain rule yields that \\[\\begin{align*}\n    S^{[L+1]} = - e\n\\end{align*}\\] where \\(e\\) was defined earlier in the squared loss case."
  },
  {
    "objectID": "posts/deep-dive/saws.html#python-implementation",
    "href": "posts/deep-dive/saws.html#python-implementation",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Python Implementation",
    "text": "Python Implementation\nHere is a python implementation of the training with min-batch SGD, 1 hidden layer, sigmoid activation in the hidden and output layers, and squared loss.\n\nimport numpy as np\n\ndef sig(x):\n    return (1+np.exp(-x))**-1\n\ng = np.random.default_rng(1123)\n\nd,m = 20, 1000\nd_out = 10\nd_hid = 20\nbatch_size = 8\nn_epoch = 2\nlr = 0.01\n\nX_train = g.normal(0,1,(d,m))\nY_train = g.uniform(size=(d_out,m))\n\nW1 = g.normal(0,1,(d,d_hid))\nW2 = g.normal(0,1,(d_hid,d_out))\nb1 = g.normal(0,1,(d_hid,1))*0.01\nb2 = g.normal(0,1,(d_out,1))*0.01\n\nerrors=[]\n\nfor epoch in range(n_epoch):\n    error = 0\n    # permute the data\n    perm_idx = g.permutation(m)\n    X_train = X_train[:,perm_idx]\n    Y_train = Y_train[:,perm_idx]\n    for bat in range(m//batch_size):\n        x_batch = X_train[:,bat*batch_size:(1+bat)*batch_size]\n        y_batch = Y_train[:,bat*batch_size:(1+bat)*batch_size]\n        # forward pass\n        z1 = np.matmul(W1.T,x_batch) + b1\n        a1 = sig(z1)\n        z2 = np.matmul(W2.T,a1) + b2\n        a2 = sig(z2)\n        # backward pass\n        e = y_batch - a2\n        A2 = sig(z2)*(1-sig(z2))\n        S2 = - A2*e\n        A1 = sig(z1)*(1-sig(z1))\n        S1 = A1*np.matmul(W2,S2)\n        # gradient descent\n        dW2 = np.matmul(a1,S2.T)\n        db2 = np.sum(S2, axis = 1, keepdims=True)\n        dW1 = np.matmul(x_batch,S1.T)\n        db1 = np.sum(S1, axis = 1, keepdims=True)\n        W2 -= lr * dW2\n        b2 -= lr * db2\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        # compute the error \n        error += 0.5*np.sum(e*e)\n    # report error of the current epoch\n    print(\"Epoch:\", epoch, \"SE:\", error)\n    errors.append(error)\n\nEpoch: 0 SE: 1084.095701311093\nEpoch: 1 SE: 931.4561396806077\n\n\nExercise: implement the training with XE loss and more hidden layers."
  },
  {
    "objectID": "posts/202410/tscm.html",
    "href": "posts/202410/tscm.html",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "",
    "text": "In this post we discuss a fundamental model in financial time series modelling - the ARCH model.\nThe model was introduced by Engle in the 80’s and brought him a Nobel prize in Economy. Nowadays ARCH is still the go-to model, or at least a starting point for time series with varying volatility, a characteristic shared by many financial return series."
  },
  {
    "objectID": "posts/202410/tscm.html#arch-model",
    "href": "posts/202410/tscm.html#arch-model",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "ARCH model",
    "text": "ARCH model\nThe ARCH model is widely used to model financial return time series. In this post, we always denote a time series by \\((Y_t)\\). ARCH model is a specification of the conditional variance of \\(Y_t\\) given its history \\(\\mathcal F_{t-1}\\) by the following equation: \\[\\mathrm{Var}[Y_t| \\mathcal F_{t-1}] = \\omega + \\alpha Y_{t-1}^2.\\]\nIn code, it is straightforward to define the ARCH model with the arch library."
  },
  {
    "objectID": "posts/202410/tscm.html#arch-library",
    "href": "posts/202410/tscm.html#arch-library",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "arch library",
    "text": "arch library\n\nimport numpy as np\nfrom arch import arch_model\n\n\nrng = np.random.default_rng(41)\ny = rng.standard_normal(size=(500,)) * np.arange(500) * 0.2\nmod = arch_model(y, mean='zero', q=0)\nres = mod.fit(disp=\"off\")\nfcst = res.forecast(horizon=50, method=\"bootstrap\", simulations=3)\n\nAbove we exposed the main classes of the library.\n- arch_model is a convenience model constructor function with which one can specify the the mean process, the volatility (aka conditional standard deviation), the noise distribution (adhere to scipy.stats).\n- mod.fit() returns a result container: one that contains residuals, standardised residuals, conditional volatility of the fit model.\n- res.forecast() returns a forecast container: one that contains conditonal variance given the present value, simulations given the current value etc.\n\ntype(mod), type(res), type(fcst)\n\n(arch.univariate.mean.ZeroMean,\n arch.univariate.base.ARCHModelResult,\n arch.univariate.base.ARCHModelForecast)\n\n\n\nres.summary()\n\n\nZero Mean - ARCH Model Results\n\n\nDep. Variable:\ny\nR-squared:\n0.000\n\n\nMean Model:\nZero Mean\nAdj. R-squared:\n0.002\n\n\nVol Model:\nARCH\nLog-Likelihood:\n-2719.79\n\n\nDistribution:\nNormal\nAIC:\n5443.58\n\n\nMethod:\nMaximum Likelihood\nBIC:\n5452.01\n\n\n\n\nNo. Observations:\n500\n\n\nDate:\nTue, Oct 15 2024\nDf Residuals:\n500\n\n\nTime:\n20:37:19\nDf Model:\n0\n\n\n\n\n\n\nVolatility Model\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n95.0% Conf. Int.\n\n\nomega\n2082.8286\n396.923\n5.247\n1.542e-07\n[1.305e+03,2.861e+03]\n\n\nalpha[1]\n0.4962\n0.174\n2.850\n4.377e-03\n[ 0.155, 0.838]\n\n\n\nCovariance estimator: robust\n\n\n\nimport polars as pl\nimport hvplot.polars\nhvplot.extension('matplotlib')\n\npl.DataFrame(fcst.simulations.values.squeeze().T).hvplot() # 3 forecasted paths with horizon 50"
  },
  {
    "objectID": "posts/202410/tscm.html#mean-model-and-noise-distribution",
    "href": "posts/202410/tscm.html#mean-model-and-noise-distribution",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "Mean model and noise distribution",
    "text": "Mean model and noise distribution\nBy default the noise distribution is standard normal. Another popular choice is t distribution where the degree of freedom \\(\\nu\\) is a model parameter.\nOne can also change the mean to be non-zero. The model spec is as follows \\[\n\\begin{align*}\nY_t &= \\mu + \\sigma_t W_t \\\\  \n\\sigma_t^2 &= \\omega+ \\alpha (Y_{t-1}-\\mu)^2\n\\end{align*}\n\\] where \\(W_t\\) is iid with t distribution.\nIn arch, it suffices to set mean=\"constant\" and dist=\"t\" in the constructor.\n\narch_model(y=y+30, mean=\"constant\", q=0, dist='t').fit(disp=\"off\")\n\n                         Constant Mean - ARCH Model Results                         \n====================================================================================\nDep. Variable:                            y   R-squared:                       0.000\nMean Model:                   Constant Mean   Adj. R-squared:                  0.000\nVol Model:                             ARCH   Log-Likelihood:               -2679.58\nDistribution:      Standardized Student's t   AIC:                           5367.17\nMethod:                  Maximum Likelihood   BIC:                           5384.03\n                                              No. Observations:                  500\nDate:                      Tue, Oct 15 2024   Df Residuals:                      499\nTime:                              20:37:22   Df Model:                            1\n                               Mean Model                               \n========================================================================\n                 coef    std err          t      P&gt;|t|  95.0% Conf. Int.\n------------------------------------------------------------------------\nmu            27.9482      1.487     18.797  7.917e-79 [ 25.034, 30.862]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega       1557.7431    401.680      3.878  1.053e-04 [7.705e+02,2.345e+03]\nalpha[1]       1.0000      0.263      3.798  1.460e-04     [  0.484,  1.516]\n                              Distribution                              \n========================================================================\n                 coef    std err          t      P&gt;|t|  95.0% Conf. Int.\n------------------------------------------------------------------------\nnu             3.3891      0.357      9.480  2.536e-21 [  2.688,  4.090]\n========================================================================\n\nCovariance estimator: robust\nARCHModelResult, id: 0x758470203050\n\n\nWhy stop here? We can also allow the conditonal mean of \\(Y_t\\) given history to vary with time as well, one simplest choice would be to assume AR dynamic for the conditional mean \\[\n\\begin{align*}\nY_t &= \\mu + \\phi (Y_{t-1} - \\mu) + \\sigma_t W_t \\\\  \n\\sigma_t^2 &= \\omega+ \\alpha (Y_{t-1}-\\mu)^2\n\\end{align*}\n\\] where as before \\(W_t\\) is iid normal by default.\nIn code, just set mean=\"AR\" and lags=[1] where 1 is as in AR(1) for the conditional mean.\n\narch_model(y=y+30, mean=\"AR\", q=0, lags=[1]).fit(disp=\"off\")\n\n                           AR - ARCH Model Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                      -0.009\nMean Model:                        AR   Adj. R-squared:                 -0.011\nVol Model:                       ARCH   Log-Likelihood:               -2713.60\nDistribution:                  Normal   AIC:                           5435.21\nMethod:            Maximum Likelihood   BIC:                           5452.06\n                                        No. Observations:                  499\nDate:                Tue, Oct 15 2024   Df Residuals:                      497\nTime:                        20:37:22   Df Model:                            2\n                               Mean Model                               \n========================================================================\n                 coef    std err          t      P&gt;|t|  95.0% Conf. Int.\n------------------------------------------------------------------------\nConst         27.0714      2.670     10.138  3.746e-24 [ 21.838, 32.305]\ny[1]          -0.0280  8.163e-02     -0.343      0.731 [ -0.188,  0.132]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega       1947.6921    445.220      4.375  1.216e-05 [1.075e+03,2.820e+03]\nalpha[1]       0.5807      0.247      2.351  1.873e-02   [9.656e-02,  1.065]\n============================================================================\n\nCovariance estimator: robust\nARCHModelResult, id: 0x758470216a20\n\n\nas before, we can inspect the result container and forecast container as follows\n\nres = arch_model(y+30, mean=\"AR\", q=0, lags=[1]).fit(disp=\"off\")\nfcst = res.forecast(horizon=5)\n\nnotice the difference of resid and std_resid, as well as the diff between variance and residual_variance\n\nres.resid[1:].std(), res.std_resid[1:].std()\n\n(np.float64(58.66084093396231), np.float64(0.9985090629605182))\n\n\n\nfcst.variance\n\n\n\n\n\n\n\n\nh.1\nh.2\nh.3\nh.4\nh.5\n\n\n\n\n499\n23353.942452\n15527.507157\n10965.954024\n8317.077882\n6778.893468\n\n\n\n\n\n\n\n\nfcst.residual_variance\n\n\n\n\n\n\n\n\nh.1\nh.2\nh.3\nh.4\nh.5\n\n\n\n\n499\n23353.942452\n15509.168033\n10953.760756\n8308.466659\n6772.362326"
  },
  {
    "objectID": "posts/202410/tscm.html#generalized-arch-garch",
    "href": "posts/202410/tscm.html#generalized-arch-garch",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "Generalized ARCH (GARCH)",
    "text": "Generalized ARCH (GARCH)\nAnother generalization of the ARCH model focuses on the conditonal variance specification itself, rather than the conditional mean and the noise distribution. The most prominent example along this line of thinking is the GARCH family, which in its simplest form (zero mean model and lag 1) is specified as follows \\[\n\\begin{align*}\nY_t &=  \\sigma_t W_t \\\\\n\\sigma_{t}^2 &= \\omega + \\alpha Y_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\end{align*}\n\\] The difference lies in the additional beta parameter which specifies a dependence of the conditional variance on the conditional variance of previous timestamp.\nGARCH generalizing ARCH is similar to the way ARMA generalizes AR."
  },
  {
    "objectID": "posts/202410/tscm.html#introducing-archette",
    "href": "posts/202410/tscm.html#introducing-archette",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "Introducing archette",
    "text": "Introducing archette\narchette is a simplified implementation of the zero mean GARCH model with iid Gaussian noise.\nThe existing arch library is amazing and covers many popular GARCH-type models. archette is not meant to replace arch in any way, after all, it supports only a very particular model (undoubtedly a very important one).\nThe goal of archette is to reduce the complexity of arch (e.g. all the class hierarchy) to highlight only the essence of GARCH model. It is meant to be super hackable so that one can build variants of GARCH models by modifying parts of the source code of archette.\nThere is only one class GARCHETTE in archette which implements the methods:\n\nfit\nnll\nforecast_vs\nsimulate\n\nand the properties\n\nvs\nstd_resids\nparams\n\nThe API is simple. States are maintained in the instances of the main class rahter than delegated to separate container classes.\nHere is how one can fit a model and inspect the fitted parameters and the corresponding negative log likelihood.\n\nfrom archette import GARCHETTE\n\n\nrng = np.random.default_rng()\ny = rng.standard_normal(500) * np.arange(500)\nmod = GARCHETTE()\nmod.fit(y)\n\n&lt;archette.core.GARCHETTE at 0x75846af288f0&gt;\n\n\n\nmod.params\n\narray([1.81808314e+02, 1.78658879e-01, 8.21341121e-01])\n\n\n\nmod.nll(mod.params)\n\nnp.float64(5844.282166604636)\n\n\n\nModel fitting\nThe fit method calls under the hood the minimize function in scipy.optimize module to minimize the negative log likelihood (nll).\nLet \\(f\\) denote the joint distribution of time series \\(y\\). By successive conditioning, one can write the nll as \\[\n-\\log f(y) = - \\log f_\\theta(y_0) - \\sum_{t=1}^{T} \\log f_{\\theta}(y_t | y_{[0,t-1]})\n\\] where \\(y_{[0,t-1]}:= (y_0, ..., y_{t-1})\\) and \\(\\theta\\) is the parameter.\nSince the marginal distribution at time 0 is unknown, one typically remove the first term and minimize \\(-\\log f(y|y_0)\\) instead \\[\n-\\log f(y|y_0) = - \\sum_{t=1}^{T} \\log f_{\\theta}(y_t | y_{[0,t-1]})\n\\] By Gaussian noise assumption and lag 1, we have, up to some unimportant additive constant, \\[\n- \\log f_{\\theta}(y_t | y_{[0,t-1]}) = - \\log f_{\\theta}(y_t | y_{t-1}) = \\frac{1}{2}\\Big(\\log v_t +  \\frac{y_t^2}{v_t}\\Big)\n\\] where \\(v_t\\) is the conditional variance of \\(y_t\\) given \\(y_{t-1}\\) (which is denoted by \\(\\sigma_t^2\\) before).\nNotice that \\(v_0\\) is not observable, hence one needs to set a sensible initial value, e.g. sample variance of \\(y\\).\nWe define two helper functions, one computes the sequence \\(v\\), the other the nll.\n\ndef _get_vs(y, params, sig2_init):\n    \"\"\"\n    compute conditional variance by recursion of the GARCH specification\n    \"\"\"\n    om,al,be = params\n    vs = np.zeros(y.size)\n    vs[0] = sig2_init\n    for i in range(1,y.size):\n        vs[i] = om + al * y[i-1]**2 + be * vs[i-1]\n    return vs\n\n\ndef _nllgauss(y,params,sig2_init):\n    \"\"\"\n    compute nll of GARCH(1,1) with zero conditional mean and gaussian noise\n    \"\"\"\n    vs = _get_vs(y,params, sig2_init)\n    nll =  np.log(vs) + y**2 / vs\n    return nll.sum()\n\nNow we are in a place to start writing the main class.\n\nfrom scipy.optimize import minimize\n\n\nclass GARCHETTE:\n    \"\"\"simple garch model\"\"\"\n    def __init__(self):\n        self._y = None\n        self.params = None\n        self._is_fit = False\n        self._v_init = None\n\n    def fit(self,y:np.ndarray):\n        \"\"\"fit y to garch\"\"\"\n        self._y = y\n        self._v_init = y.var()  \n        self._is_fit = True\n        func = self.nll\n        self.params = minimize(\n            func, \n            x0=(self._v_init * 0.4,0.3,0.3), \n            bounds=[(0,None), (0,None), (0,None)],\n            constraints= {\n                \"type\": \"ineq\",\n                \"fun\": lambda x: 1-x[1]-x[2]\n            }\n         ).x\n        return self\n    \n    def nll(self, params)-&gt; float:\n        \"\"\"negative log likelihood of the series at the given params\"\"\"\n        assert self._is_fit\n        return _nllgauss(self._y, params, self._v_init)\n    \n    @property\n    def vs(self):\n        return _get_vs(self._y, self.params, self._v_init)\n\nWe see that the method nll and property vs are just wrappers around the helper functions that we just defined. The fit method, though, needs some attention. For the minimization of nll, apart from the objective function nll and initial value x0, it is also necessary to set bounds and constraints for the variables of nll. We impose the parameters to be non-negative because the sequence v must be non-negative (as squares). Also we need\n\\[\n\\alpha + \\beta &lt;1\n\\] because it is the necessary and sufficnet condition for the stationary of GARCH(1,1) model. Having these constraints are essential for the optimization to work. Finally, we made an educated guess for the initial value: we set the initial values \\(\\alpha=\\beta=0.3\\) to leave some space to the boundary of the constrained parameter space. The initial value for \\(\\omega\\) is set based on an important fact about the long term conditional variance: for any \\(t\\), as \\(T\\to\\infty\\): \\[\n\\mathbb E[Y_{t+T}^2| \\mathcal F_t] \\to \\frac{\\omega}{1-\\alpha-\\beta}\n\\] If we believe that the sample variance of y approaches the long term conditonal variance, then initialize om=(1-0.3-0.3)* y.var() is indeed sensible.\nNow it remains to forecast variances (or equivalently the conditional volatility) and simualte paths. For variance forecast, the recursion for \\(h\\ge 2\\) is simpler than that of the GARCH specification: \\[\n\\mathbb E[Y^2_{t+h}| \\mathcal F_t] = \\omega  + \\alpha \\mathbb E[Y^2_{t+h-1}|\\mathcal F_{t}] + \\beta \\mathbb E[\\sigma^2_{t+h-1}|\\mathcal F_{t}]\n= \\omega  + (\\alpha +\\beta) \\mathbb E[Y^2_{t+h-1}|\\mathcal F_{t}]\n\\] This leads to\n\ndef _make_fcst_vs(params,last_v, last_y, horizon):\n    om,al,be = params\n    fcst_vs = np.empty(horizon)\n    fcst_vs[0] = om + al* last_y**2 + be* last_v\n    for i in range(1,horizon):\n        fcst_vs[i] = om + (al+be) * fcst_vs[i-1]\n    return fcst_vs\n\nThe simulation helper function is rather straightforward. We leave the possibility of specifying noises ws to the user.\n\ndef _simulate(params, last_y, last_v, horizon, n_rep, seed, ws=None):\n    om,al,be = params\n    if ws is None:\n        np.random.seed(seed)\n        ws = np.random.randn(n_rep,horizon)\n    y = np.empty((n_rep, horizon+1))\n    v = np.empty((n_rep, horizon+1))\n    y[:,0] = last_y\n    v[:,0] = last_v\n    for i in range(1,horizon+1):\n        v[:,i] = om + al* y[:,i-1]**2 + be * v[:,i-1]\n        y[:,i] = np.sqrt(v[:,i]) * ws[:,i-1]\n    return y[:,1:]\n\nCombining everything, here is our main class\n\nfrom typing import Literal\n\n\nclass GARCHETTE:\n    \"\"\"simple garch model\"\"\"\n    def __init__(self):\n        self._y = None\n        self.params = None\n        self._is_fit = False\n        self._v_init = None\n\n    def fit(self,y:np.ndarray):\n        \"\"\"fit y to garch\"\"\"\n        self._y = y\n        self._v_init = y.var()  \n        self._is_fit = True\n        func = self.nll\n        self.params = minimize(\n            func, \n            x0=(self._v_init * 0.4,0.3,0.3), \n            bounds=[(0,None), (0,None), (0,None)],\n            constraints= {\n                \"type\": \"ineq\",\n                \"fun\": lambda x: 1-x[1]-x[2]\n            }\n         ).x\n        return self\n    \n    def nll(self, params)-&gt; float:\n        \"\"\"negative log likelihood of the series at the given params\"\"\"\n        assert self._is_fit\n        return _nllgauss(self._y, params, self._v_init)\n    \n    @property\n    def vs(self):\n        assert self._is_fit\n        return _get_vs(self._y, self.params, self._v_init)\n    \n    @property\n    def std_resids(self):\n        assert self._is_fit\n        return y/ np.sqrt(self.vs)\n\n    def forecast_vs(self, \n                    horizon:int\n                    ) -&gt; np.ndarray:\n        \"\"\"forecast conditional variance in the horizon\"\"\"\n        assert self._is_fit\n        return _make_fcst_vs(self.params, self.vs[-1], self._y[-1], horizon)\n\n    def simulate(self, \n                    horizon:int, # path length\n                    method:Literal[\"bootstrap\",\"simulate\"]=\"simulate\",# \"bootstrap\" resamples from past std_resids; \"simulate\" simulates gaussian nosie\n                    n_rep=1_000, # number of repetitions\n                    seed=42) -&gt; np.ndarray: \n        assert self._is_fit\n        if method == \"bootstrap\":\n            np.random.seed(seed)\n            ws = np.random.choice(self.std_resids, size=(n_rep, horizon),replace=True)\n        else: ws=None\n        return _simulate(self.params,self._y[-1], self.vs[-1], horizon, n_rep, seed, ws)"
  },
  {
    "objectID": "posts/202410/tscm.html#testing",
    "href": "posts/202410/tscm.html#testing",
    "title": "Financial time series: a journey from code to maths, and back",
    "section": "Testing",
    "text": "Testing\nWe define GARCH model with both arch and archette, we see that when the model is mis-specified, the MLE found different parameters.\n\nmod_arch = arch_model(y, mean='zero', dist='normal', rescale=False).fit(disp='off')\nmod_archette = GARCHETTE().fit(y)\n\n\nmod_arch.params\n\nomega       36.446855\nalpha[1]     0.053328\nbeta[1]      0.946672\nName: params, dtype: float64\n\n\n\nmod_archette.params\n\narray([1.81808314e+02, 1.78658879e-01, 8.21341121e-01])\n\n\nWhat if model is well specified ?\n\ny_sim = mod_archette.simulate(horizon=500, n_rep =1, seed=89).squeeze()\n\n\narch_model(y_sim, mean='zero', dist='normal', rescale=False).fit(disp='off').params\n\nomega       864.198887\nalpha[1]      0.186017\nbeta[1]       0.776968\nName: params, dtype: float64\n\n\n\nGARCHETTE().fit(y_sim).params\n\narray([8.54615038e+02, 1.81551901e-01, 7.80574996e-01])\n\n\nThe estimated parameters using arch and archette are quite close!"
  },
  {
    "objectID": "posts/gist/sql101.html",
    "href": "posts/gist/sql101.html",
    "title": "SQL 101",
    "section": "",
    "text": "Every professional interacting with data will come across SQL sooner or later. In this post, we cover a core set of vocabulary, grammer, expression for efficient use of this language."
  },
  {
    "objectID": "posts/gist/sql101.html#vocabulary",
    "href": "posts/gist/sql101.html#vocabulary",
    "title": "SQL 101",
    "section": "vocabulary",
    "text": "vocabulary\nfrom where group by having select distinct union order by limit/fetch first rows only\nthese vocabularies are fundamental in the day to day usage of SQL. They are listed in the execution order as well, meaning that the from clause first gets executed (which table do we want), then where (which row) and so on. We don’t have to include all the clauses in one query, but if we do, then knowing the order in which they are executed matters, especially for debugging errors.\nIn this post, we use a jupyter kernel called xsqlite with sqlite3 backend (the most commonly used in-memory database) to demonstrate the SQL language. To install the kernel, plesae visit here.\nHere are simple queries with some of the vocabs.\n\n%load ../assets/tutorial.db\n\n\nselect * from customers limit 5\n\n\n\n\ncustomer_id\nname\nvisited_on\namount\n\n\n1\nJhon\n2019-01-01\n100\n\n\n2\nDaniel\n2019-01-02\n110\n\n\n3\nJade\n2019-01-03\n120\n\n\n4\nKhaled\n2019-01-04\n130\n\n\n5\nWinston\n2019-01-05\n110\n\n\n\n\n\n\nselect * from customers where customer_id&lt;3\n\n\n\n\ncustomer_id\nname\nvisited_on\namount\n\n\n1\nJhon\n2019-01-01\n100\n\n\n2\nDaniel\n2019-01-02\n110\n\n\n1\nJhon\n2019-01-10\n130\n\n\n\n\n\n\nselect * from customers where customer_id=2\nunion\nselect * from customers where customer_id=2\n\n\n\n\ncustomer_id\nname\nvisited_on\namount\n\n\n2\nDaniel\n2019-01-02\n110\n\n\n\n\n\n\nselect * from customers where customer_id=2\nunion all\nselect * from customers where customer_id=2\n\n\n\n\ncustomer_id\nname\nvisited_on\namount\n\n\n2\nDaniel\n2019-01-02\n110\n\n\n2\nDaniel\n2019-01-02\n110\n\n\n\n\n\n\nselect distinct customer_id, visited_on \nfrom customers \norder by visited_on, customer_id\n\n\n\n\ncustomer_id\nvisited_on\n\n\n1\n2019-01-01\n\n\n2\n2019-01-02\n\n\n3\n2019-01-03\n\n\n4\n2019-01-04\n\n\n5\n2019-01-05\n\n\n6\n2019-01-06\n\n\n7\n2019-01-07\n\n\n8\n2019-01-08\n\n\n9\n2019-01-09\n\n\n1\n2019-01-10\n\n\n3\n2019-01-10"
  },
  {
    "objectID": "posts/gist/sql101.html#aggregate-function",
    "href": "posts/gist/sql101.html#aggregate-function",
    "title": "SQL 101",
    "section": "aggregate function",
    "text": "aggregate function\nIt is very common that one wants summary statistics of different group of people/items/products. In SQL, this is done with the group by clause together with aggregate function(s).\n\nselect customer_id, min(visited_on), count(*) \nfrom customers \nwhere customer_id&lt;3 \ngroup by customer_id\n\n\n\n\ncustomer_id\nmin(visited_on)\ncount(*)\n\n\n1\n2019-01-01\n2\n\n\n2\n2019-01-02\n1\n\n\n\n\n\n\nselect customer_id, min(visited_on), count(*) as cnt \nfrom customers \nwhere customer_id&lt;3\ngroup by customer_id \nhaving cnt=1\n\n\n\n\ncustomer_id\nmin(visited_on)\ncnt\n\n\n2\n2019-01-02\n1\n\n\n\n\n\nnotice that having and where play a similar role which is to select relevant rows, but one gets executed before group by, the other after."
  },
  {
    "objectID": "posts/gist/sql101.html#windows-function",
    "href": "posts/gist/sql101.html#windows-function",
    "title": "SQL 101",
    "section": "windows function",
    "text": "windows function\nwindows function and aggregate function are similar-ish in that they both act on groups (with different synatx though). The difference is that an aggregate function collapses rows within the same group into one, whereas a windows function keeps all the rows within the same group/window, and add a new value to each row.\n\nselect row_number() over(partition by visited_on), visited_on from customers\n\n\n\n\nrow_number() over(partition by visited_on)\nvisited_on\n\n\n1\n2019-01-01\n\n\n1\n2019-01-02\n\n\n1\n2019-01-03\n\n\n1\n2019-01-04\n\n\n1\n2019-01-05\n\n\n1\n2019-01-06\n\n\n1\n2019-01-07\n\n\n1\n2019-01-08\n\n\n1\n2019-01-09\n\n\n1\n2019-01-10\n\n\n2\n2019-01-10\n\n\n\n\n\nhere row_number() is the windows function acting on the groups obtained from partition by visited_on.\nWe can use order by instead of partition by, in which case there is a single group, and the function is executed in the requested order.\n\nselect row_number() over(order by visited_on), visited_on from customers\n\n\n\n\nrow_number() over(order by visited_on)\nvisited_on\n\n\n1\n2019-01-01\n\n\n2\n2019-01-02\n\n\n3\n2019-01-03\n\n\n4\n2019-01-04\n\n\n5\n2019-01-05\n\n\n6\n2019-01-06\n\n\n7\n2019-01-07\n\n\n8\n2019-01-08\n\n\n9\n2019-01-09\n\n\n10\n2019-01-10\n\n\n11\n2019-01-10\n\n\n\n\n\nit is possible to use both partition by and order by in over()."
  },
  {
    "objectID": "posts/gist/sql101.html#subquery-and-common-table-expression",
    "href": "posts/gist/sql101.html#subquery-and-common-table-expression",
    "title": "SQL 101",
    "section": "subquery and common table expression",
    "text": "subquery and common table expression\nOne can nest one query in another, called subquery. They are very useful in practice. Indeed, one complicated query needs to be decomposed into a few tasks. After figuring out the intermediate steps, one can put things together by either chaining them sequentially, or union, or a combination of both.\nnested subqueries can be hard to read as the level of nested queries increases. this is where CTE comes into rescue. CTE is like defining intermediate variables in a general-purpose language such as python.\nTo illuastrate this, we consider three tables which are relational through ids.\n\nselect * from movies\n\n\n\n\nmovie_id\ntitle\n\n\n1\nAvengers\n\n\n2\nFrozen 2\n\n\n3\nJoker\n\n\n\n\n\n\nselect * from userss\n\n\n\n\nuser_id\nname\n\n\n1\nDaniel\n\n\n2\nMonica\n\n\n3\nMaria\n\n\n4\nJames\n\n\n\n\n\n\nselect * from movierating\n\n\n\n\nmovie_id\nuser_id\nrating\ncreated_at\n\n\n1\n1\n3\n2020-01-12\n\n\n1\n2\n4\n2020-02-11\n\n\n1\n3\n2\n2020-02-12\n\n\n1\n4\n1\n2020-01-01\n\n\n2\n1\n5\n2020-02-17\n\n\n2\n2\n2\n2020-02-01\n\n\n2\n3\n2\n2020-03-01\n\n\n3\n1\n3\n2020-02-22\n\n\n3\n2\n4\n2020-02-25\n\n\n\n\n\nfirst we join three tables (search online for all kinds of join methods!)\n\nselect * from movierating left join userss using (user_id) left join movies using (movie_id)\n\n\n\n\nmovie_id\nuser_id\nrating\ncreated_at\nname\ntitle\n\n\n1\n1\n3\n2020-01-12\nDaniel\nAvengers\n\n\n1\n2\n4\n2020-02-11\nMonica\nAvengers\n\n\n1\n3\n2\n2020-02-12\nMaria\nAvengers\n\n\n1\n4\n1\n2020-01-01\nJames\nAvengers\n\n\n2\n1\n5\n2020-02-17\nDaniel\nFrozen 2\n\n\n2\n2\n2\n2020-02-01\nMonica\nFrozen 2\n\n\n2\n3\n2\n2020-03-01\nMaria\nFrozen 2\n\n\n3\n1\n3\n2020-02-22\nDaniel\nJoker\n\n\n3\n2\n4\n2020-02-25\nMonica\nJoker\n\n\n\n\n\nnow we define t as the result of joining with CTE, then find the person’s name who watched the largest number of movies, in case of a tie, choose the name that is lexicographical smaller (i.e. appears first in English dictionary). We would need the counts in order to find the name, this is where a subquery is required.\n\n-- CTE\nwith t as (\n    select * from movierating left join userss using (user_id) left join movies using (movie_id)\n)\n\nselect name as results from (\n    -- subquery\n    select name, count(*) cnt from t group by user_id, name order by cnt desc, name \n) \nlimit 1\n\n\n\n\nresults\n\n\nDaniel"
  },
  {
    "objectID": "posts/gist/sql101.html#next-steps",
    "href": "posts/gist/sql101.html#next-steps",
    "title": "SQL 101",
    "section": "next steps",
    "text": "next steps\nwriting efficient SQL is a matter of practice. In the sql directory of this repo, you can find the solution to 50 exercises collected from leetcode. They cover a wide range of problems, including what we’ve covered in this post and date operations, regular expressions… have fun learning SQL!"
  },
  {
    "objectID": "posts/gist/git101.html",
    "href": "posts/gist/git101.html",
    "title": "Git 101",
    "section": "",
    "text": "Using git to manage text files (e.g. code files) is inevitable for keeping track of all changes while developing and maintaining a project."
  },
  {
    "objectID": "posts/gist/git101.html#work-solo",
    "href": "posts/gist/git101.html#work-solo",
    "title": "Git 101",
    "section": "Work solo",
    "text": "Work solo\nAssume that we have cloned a GitHub repo on the disk. For solo project, one might get away with three commands\ngit add .\ngit commit -m \"MESSAGE\"\ngit push origin main\nTo break down these commands,\n\ngit add . moves all modified and untracked files in the current directory to the staged area, a temporary place for files before incorporating changes.\n\ngit commit incorporates changes (locally).\ngit push origin main pushes the commits to remote (GitHub) main branch\n\nThis is simple enough for beginners to get started. However, when it comes to collaboration within an organization or maintaining/participating non-solo open source project, one needs to learn a few more commands."
  },
  {
    "objectID": "posts/gist/git101.html#branching-out",
    "href": "posts/gist/git101.html#branching-out",
    "title": "Git 101",
    "section": "Branching out",
    "text": "Branching out\nthe first thing to do is to checkout a new branch when working on a new feature/task\ngit checkout -b \"new-feature\"\nedit as usual then\ngit add . \ngit commit -m 'MESSAGE'\ngit push origin new-feature\nnotice that we push back to the new-feature branch, which will be automatically created in the remote repo.\nAt this point, we have finished the task at hand and want the main branch to incorporate our edits. We do so by create a pull request, aka PR. The maintainer (could be ourselves) can squash and merge our branch like so\ngit merge --squash new-feature"
  },
  {
    "objectID": "posts/gist/git101.html#handling-conflicts",
    "href": "posts/gist/git101.html#handling-conflicts",
    "title": "Git 101",
    "section": "Handling Conflicts",
    "text": "Handling Conflicts\nthe assumption we make in the last section is that there are no conflicts between the main branch and our feature branch. Such conflicts can happen if main branch has been updated while we work on the feature branch, and the same code has been touched both our feature branch and the main.\nOne can rely on the maintainer to handle merge conflict but it may not be easy because the maintainer couldn’t know all the details/changes in the feature branch. A common practice is to delegate the handling of conflicts to the owner of the feature branch.\nMore precisely, the feature branch owner should do\ngit checkout main\ngit pull\ngit checkout new-feature\ngit rebase main\nlet me break this down. We first get the update of main branch. Then we git rebase main to rebase the feature branch on top of the main branch. This means that our changes are treated as if we are branching out from the updated main branch. Conflicts may arise at this stage, and the terminal will let us know. We now manually resolve the conflicts by opening the file in question, remove/keep code chunks as appropriate, then\ngit add FILE\ngit rebase --continue\nRepeat this until the conflicts are resolved\nNow we can push the feature branch back to remote and ask for a PR as before."
  },
  {
    "objectID": "posts/gist/git101.html#github-cli",
    "href": "posts/gist/git101.html#github-cli",
    "title": "Git 101",
    "section": "GitHub CLI",
    "text": "GitHub CLI\nGitHub has a delightful CLI which extends git in the context of remote workflow such as creating issues, reviewing PR etc.\nFor instance, to see the README.md of the repo\ngh repo view \nTo create an issue,\ngh issue create\nRemembering a few commonly used commands can streamline the workflow, thus lead to some productivity boost."
  },
  {
    "objectID": "posts/gist/git101.html#last-word",
    "href": "posts/gist/git101.html#last-word",
    "title": "Git 101",
    "section": "Last word",
    "text": "Last word\nThis post discuss a small subset of the commands git and gh have to offer. If something is not clear or you need more than what are discussed here, check out the official documentations:\n\ngit: https://git-scm.com/\ngh: https://cli.github.com/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fisrt principles first",
    "section": "",
    "text": "Financial time series: a journey from code to maths, and back\n\n\n\n\n\n\ntime series\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nGit 101\n\n\n\n\n\n\nhacks\n\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nLearn transformer with makemore and torch\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nSQL 101\n\n\n\n\n\n\ndata engineering\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is required to achieve a top 20% ranking in Kaggle playground series\n\n\n\n\n\n\nkaggle\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nbits of fastai live-coding sessions\n\n\n\n\n\n\nhacks\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Recap 1 Concepts\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Risk Models Ep 2 machine learning methods for parameter estimation\n\n\n\n\n\n\ncredit risk\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Risk Models Ep 1 Fundamentals\n\n\n\n\n\n\ncredit risk\n\n\napplied probability\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Machine Learning: a series of 20 videos\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n\n\n\n\n\n\nSecret ‘SAWS’ of deep learning, feature backprop\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gist/makemore.html",
    "href": "posts/gist/makemore.html",
    "title": "Learn transformer with makemore and torch",
    "section": "",
    "text": "[last modified on 2024-03-22]"
  },
  {
    "objectID": "posts/gist/makemore.html#makemore",
    "href": "posts/gist/makemore.html#makemore",
    "title": "Learn transformer with makemore and torch",
    "section": "makemore",
    "text": "makemore\nKarpathy’s makemore is an end-to-end python application that takes in a text file, then generate new text similar to what’s given. The project makemore is part of his neural networks: zero to hero lecture series which I recommend to all.\nThe example dataset in the repo is a large collection of baby names (about 30k names) and the applicaiton trains a transformer model to learn the mechanism of naming things, then sample from the learned model.\nThe purpose of this post is three fold:\n- gain familiarity with transformer\n- summarise essential torch functionalities and workflow for building neural nets application\n- use minimal tools e.g argparse to produce a command line interface for the app\n\n\n\n\n\n\ndisclaimer\n\n\n\nall the code chunks in this post (including docstring) are taken from the makemore repository"
  },
  {
    "objectID": "posts/gist/makemore.html#torch-basics",
    "href": "posts/gist/makemore.html#torch-basics",
    "title": "Learn transformer with makemore and torch",
    "section": "torch basics",
    "text": "torch basics\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nSimplistically, a neural net application is built on two major components: a dataset and a model. The former shapes the behaviour of the latter by optimising some objective function. torch allows us to do both fairly easily and straightforward.\n\nDataset\nDataset class is a container like a sequence. To define a custom Dataset class, one must implement __init__, __len__, __getitem__, together with transformations relevant to the particular use case of the application. Exampe:\n\nclass CharDataset(Dataset):\n\n    def __init__(self, words:list[str], chars:str, max_word_length:int):\n        self.words = words\n        self.chars = chars\n        self.max_word_length = max_word_length\n        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n\n    def __len__(self):\n        return len(self.words)\n\n    def contains(self, word):\n        return word in self.words\n\n    def get_vocab_size(self):\n        return len(self.chars) + 1 # all the possible characters and special 0 token\n\n    def get_output_length(self):\n        return self.max_word_length + 1 # &lt;START&gt; token followed by words\n\n    def encode(self, word):\n        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n        return ix\n\n    def decode(self, ix):\n        word = ''.join(self.itos[i] for i in ix)\n        return word\n\n    def __getitem__(self, idx):\n        word = self.words[idx]\n        ix = self.encode(word)\n        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n        x[1:1+len(ix)] = ix\n        y[:len(ix)] = ix\n        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n        return x, y\n\nThe custom transformations in this example consists of mapping character to integer, aka tokenisation. There is one special token 0 representing both the start of name and end of name. We can loop through or get at index a CharDataset because we have implemented sufficient dunder methods for this.\n\nimport string\n\nexamples = CharDataset(['emma', 'richard'], string.ascii_letters, 10)\nfor e in examples:\n    print(e)\n\n(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n(tensor([ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0]), tensor([18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1]))\n\n\n\nprint(examples[0])\n\n(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n\n\nLet’s break down the output tuple x,y.\n\nx tensor\nIt starts with 0 token, then each input character gets mapped to an integer from 1 to 26, with trailing zeros if the length of name is less than max_word_length (set to be max length of names in the dataset)\n\n\ny tensor\nBy definition, it is the same as x shifted by 1 token to the left, modulo extra subtleties with the trailing -1. What is going on here? Well, in language modelling, the learning task is next token prediction, so at index idx such that x[idx].item()!=0, given x[:idx+1], the goal is to predict x[idx+1] which by definition is nothing but y[idx]. If x[idx].item()==0, then there is nothing to predict (name finished), we set by convention y[idx]=-1.\nOur ultimate goal is to build and train a neural net which can learn from the 30k x,y tuples a good way of sampling next token given some context. Concetely, throw a 0 token at the model and let the model sample the next token t1, concatenate it with 0, then sample next given [0,t1], until a 0 token is sampled which means we have arrived at the end of a name. Repeat this to produce as many names as we want. We’ll get back to inference later.\n\n\n\nDataLoader\nFor efficiency’s sake, it is beneficial to stack multiple examples together, aka mini-batch, and process them all at once. The DataLoader class is meant to help us with this. Example:\n\nclass InfiniteDataLoader:\n    \"\"\"\n    this is really hacky and I'm not proud of it, but there doesn't seem to be\n    a better way in PyTorch to just create an infinite dataloader?\n    \"\"\"\n\n    def __init__(self, dataset, **kwargs):\n        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n        self.data_iter = iter(self.train_loader)\n\n    def next(self):\n        try:\n            batch = next(self.data_iter)\n        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n            self.data_iter = iter(self.train_loader)\n            batch = next(self.data_iter)\n        return batch\n\n\ndataset = CharDataset(['emma', 'richard', 'ben', 'steve'],string.ascii_letters, 10)\nbatch_loader = InfiniteDataLoader(dataset, batch_size=2)\n\n\nfor _ in range(2):\n    X,Y = batch_loader.next()\n    print(f'{X=}') # B,T = batch_size, max_word_length+1\n    print('-'*60)\n    print(f'{Y=}') # (B,T)\n    print('*'*60)\n\nX=tensor([[ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0],\n        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n------------------------------------------------------------\nY=tensor([[18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1],\n        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n************************************************************\nX=tensor([[ 0,  2,  5, 14,  0,  0,  0,  0,  0,  0,  0],\n        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n------------------------------------------------------------\nY=tensor([[ 2,  5, 14,  0, -1, -1, -1, -1, -1, -1, -1],\n        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n************************************************************\n\n\n\n\nTensor ops and View\nTensor is the most fundamental data structure for neural nets.\nA tensor is a collection of numbers index by tuple of non-negative integers. In the above, we’ve seen that a batch X is 2d tensor wit shape (B,T), we can index X[b,t] for b in range(B) and t in range(T).\ntorch provides optimised tensor operations and auto differentiation engine. Rather than understanding low level optimisations (parallel programming as in e.g. cuda kernels), we just take these optimisations for granted in this post and we only concerned with what we can build with Tensor.\ntorch ops usually create new tensor as output. e.g. torch.cat, torch.stack.\nIn contrast, Tensor View avoids wasteful data copy.\nHere are a few common view ops.\n\nbasic slicing and indexing (following numpy)\n\nview\n\nsplit\n\nunsqueeze\n\nexpand\n\nFor a full list of view ops, refer to docs.\n\ntorch.arange(24).view(3,4,2)\n\ntensor([[[ 0,  1],\n         [ 2,  3],\n         [ 4,  5],\n         [ 6,  7]],\n\n        [[ 8,  9],\n         [10, 11],\n         [12, 13],\n         [14, 15]],\n\n        [[16, 17],\n         [18, 19],\n         [20, 21],\n         [22, 23]]])\n\n\n\nassert torch.equal(torch.arange(60).view(3,4,5),torch.arange(60).view(3,4,-1))\n\n\ntorch.tril(torch.ones(4,4))\n\ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\n\n\n\ntorch.randn(3,4).split(2,dim=1)  # a tuple of 4/2 tensors of shape (3,2) \n\n(tensor([[-0.1093,  0.0426],\n         [ 2.5843,  0.2994],\n         [-0.2158, -1.7210]]),\n tensor([[-1.2973,  0.2321],\n         [ 1.1551,  0.2394],\n         [ 0.4124,  0.1518]]))\n\n\n\natt = torch.arange(16).view(4,4)\natt.masked_fill(torch.tril(torch.ones(4,4))==0, -99)\n\ntensor([[  0, -99, -99, -99],\n        [  4,   5, -99, -99],\n        [  8,   9,  10, -99],\n        [ 12,  13,  14,  15]])\n\n\n\nassert torch.randn(24).view(2,3,4).transpose(1,2).shape == (2,4,3)\nassert torch.randn(5).unsqueeze(1).shape == (5,1)\nassert torch.cat([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,3+3)\nassert torch.stack([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,2,3)\n\n\n\nModule\nnn.Module is the base class for all neural nets, which, mathemtically, are just functions taking Tensor as input and compute the output as another Tensor. Just as we can compose functions, we can compose Module’s to build complicated achitechture.\ntorch offers built-in modules as LEGO pieces. Example:\n\nnn.Linear: random linear function that takes input dim and output dim as arguments\nnn.LayerNorm: standardise a tensor over shape (pass in as argument), then multiplied by weight, then add bias (default elementwise 1 and 0).\nnn.RELU: a parameter-less elementwise non-linear function\nnn.Embedding: just a random lookup table of shape B,T, n_embd if the input tensor is of shape B,T\n\nsome container classes:\n\nnn.ModuleDict: pass in a dictionary of name:instance pairs\nnn.ModuleList: pass in a list of instances\n\ncustom Module must implement forward method. Example:\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
  },
  {
    "objectID": "posts/gist/makemore.html#build-transformer",
    "href": "posts/gist/makemore.html#build-transformer",
    "title": "Learn transformer with makemore and torch",
    "section": "build Transformer",
    "text": "build Transformer\nTransformer is a custom module built on attention blocks, which themselves are built on multi-head attention layers. What we have here is a decoder/GPT.\nmore variants\nlet’s build from the layer level all the way to transformer. here is our model config. Notice that we should have a dataset first, infer from there the necessary config parameters for transformer. For example, the vocab_size is the size of all the possible tokens in the dataset, a value that may vary from dataset to dataset.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass ModelConfig:\n    block_size: int = None # length of the input sequences of integers\n    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n    # parameters below control the sizes of each model slightly differently\n    n_layer: int = 4\n    n_embd: int = 64\n    n_embd2: int = 64\n    n_head: int = 4\n\n\nself attention layer\nThe input of attn is 3d tensor of shape (B,T,C). The forward step of attention undergoes a few steps\n\nlinear C-&gt; 3C, split into qkv\n\nview C -&gt; (nh, C/nh) as heads\n\ncompute attention matrix with qk, which is used to average v\n\nre-assemble all heads to (B,T,C) then project\n\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                     .view(1, 1, config.block_size, config.block_size))\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n        return y\n\nThis is pretty self-explanatory. don’t forget super().__init__()\nA bit of caution here in line 35: transpose is a non-contiguous view op of the base tensor, which has performance penalty if not making it contingous. Also:\n\nx = torch.randn(2,3,2)\ntorch.ne(x.transpose(0,1).contiguous().view(3,4), x.view(3,4))\n\ntensor([[False, False,  True,  True],\n        [ True,  True,  True,  True],\n        [ True,  True, False, False]])\n\n\n\n\nblock\nStack attention layer and mlp, with layer normalization as well as residual connections to facilitate training.\n\nclass Block(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n            act     = NewGELU(),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\n\n\nthe entire thing\ntoken embedding + potitional embedding, then pass through layers of blocks, layer normalization, finally projection.\n\nclass Transformer(nn.Module):\n    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.parameters())\n        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n\n    def get_block_size(self):\n        return self.block_size\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t &lt;= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n\n        # if we are given some desired targets also calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\n        return logits, loss\n\nNote: F.cross_entropy caveats\n\nexpect ground truth of shape (D,)\n\nexpect predictions of shape (D,C) where C is the number of classes\n\nuse logtis for prediction (which is one F.softmax away from probability)\n\nignore_index=-1 matches -1 in the definition of yin CharDataset"
  },
  {
    "objectID": "posts/gist/makemore.html#evaluation",
    "href": "posts/gist/makemore.html#evaluation",
    "title": "Learn transformer with makemore and torch",
    "section": "evaluation",
    "text": "evaluation\nthe evaluation code is standard. Note:\n\nDataLoader can be looped through.\n\nbatch is a tuple of tensors each of shape (B, T)\n\nI am however not sure what are the benefits of entering the inference mode, given that code has already switched on model.eval(). TODO!\n\n@torch.inference_mode()\ndef evaluate(model, dataset, batch_size=50, max_batches=None):\n    model.eval()\n    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n    losses = []\n    for i, batch in enumerate(loader):\n        batch = [t.to(args.device) for t in batch]\n        X, Y = batch\n        logits, loss = model(X, Y)\n        losses.append(loss.item())\n        if max_batches is not None and i &gt;= max_batches:\n            break\n    mean_loss = torch.tensor(losses).mean().item()\n    model.train() # reset model back to training mode\n    return mean_loss"
  },
  {
    "objectID": "posts/gist/makemore.html#inference",
    "href": "posts/gist/makemore.html#inference",
    "title": "Learn transformer with makemore and torch",
    "section": "inference",
    "text": "inference\nGiven some context, generate things with trained model (no gradient update) one token at a time. This involves concat a generated token with the context, then pass the combined new context back into the model to get the next token, and so on …\n\n@torch.no_grad()\ndef generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n    \"\"\"\n    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n    \"\"\"\n    block_size = model.get_block_size()\n    for _ in range(max_new_tokens):\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = idx if idx.size(1) &lt;= block_size else idx[:, -block_size:]\n        # forward the model to get the logits for the index in the sequence\n        logits, _ = model(idx_cond)\n        # pluck the logits at the final step and scale by desired temperature\n        logits = logits[:, -1, :] / temperature\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits &lt; v[:, [-1]]] = -float('Inf')\n        # apply softmax to convert logits to (normalized) probabilities\n        probs = F.softmax(logits, dim=-1)\n        # either sample from the distribution or take the most likely element\n        if do_sample:\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            _, idx_next = torch.topk(probs, k=1, dim=-1)\n        # append sampled index to the running sequence and continue\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx"
  },
  {
    "objectID": "posts/gist/makemore.html#tie-it-all-up",
    "href": "posts/gist/makemore.html#tie-it-all-up",
    "title": "Learn transformer with makemore and torch",
    "section": "tie it all up",
    "text": "tie it all up\n\nimport os\nimport sys\nimport time\nimport math\nimport argparse\n\n\nargparse\nargparse module is in the python standard library centring around the ArgumentParser class, which has the add_argument() method and parse_args() method.\nWhen no specific value is given for a flag, the parser will use the default value. Alternatively, one can use action to store some specific value for instance a boolean. If neither exists, then the value of the flag is None.\nCalling parse_args() returns a Namespace object which stores arguments and values.\nRecall that the python built-in vars calls the __dict__ of a class.\n\n# parse command line args\nparser = argparse.ArgumentParser(description=\"Make More\")\n# system/input/output\nparser.add_argument('--input-file', '-i', type=str, default='names.txt', help=\"input file with things one per line\")\nparser.add_argument('--work-dir', '-o', type=str, default='out', help=\"output working directory\")\nparser.add_argument('--resume', action='store_true', help=\"when this flag is used, we will resume optimization from existing model in the workdir\")\nparser.add_argument('--sample-only', action='store_true', help=\"just sample from the model and quit, don't train\")\nparser.add_argument('--num-workers', '-n', type=int, default=4, help=\"number of data workers for both train/test\")\nparser.add_argument('--max-steps', type=int, default=-1, help=\"max number of optimization steps to run for, or -1 for infinite.\")\nparser.add_argument('--device', type=str, default='cpu', help=\"device to use for compute, examples: cpu|cuda|cuda:2|mps\")\nparser.add_argument('--seed', type=int, default=3407, help=\"seed\")\n# sampling\nparser.add_argument('--top-k', type=int, default=-1, help=\"top-k for sampling, -1 means no top-k\")\n# model\nparser.add_argument('--type', type=str, default='transformer', help=\"model class type to use, bigram|mlp|rnn|gru|bow|transformer\")\nparser.add_argument('--n-layer', type=int, default=4, help=\"number of layers\")\nparser.add_argument('--n-head', type=int, default=4, help=\"number of heads (in a transformer)\")\nparser.add_argument('--n-embd', type=int, default=64, help=\"number of feature channels in the model\")\nparser.add_argument('--n-embd2', type=int, default=64, help=\"number of feature channels elsewhere in the model\")\n# optimization\nparser.add_argument('--batch-size', '-b', type=int, default=32, help=\"batch size during optimization\")\nparser.add_argument('--learning-rate', '-l', type=float, default=5e-4, help=\"learning rate\")\nparser.add_argument('--weight-decay', '-w', type=float, default=0.01, help=\"weight decay\")\n\nargs = parser.parse_args()\nprint(vars(args))\n\n\n\ninits\nwe run things in order\n\ninit a dataset and let it determine part of model config\n\ninit model with config\n\ninit optimiser and dataloader\n\n\n# system inits\ntorch.manual_seed(args.seed)\ntorch.cuda.manual_seed_all(args.seed)\nos.makedirs(args.work_dir, exist_ok=True)\nwriter = SummaryWriter(log_dir=args.work_dir)\n\n# init datasets\ntrain_dataset, test_dataset = create_datasets(args.input_file)\nvocab_size = train_dataset.get_vocab_size()\nblock_size = train_dataset.get_output_length()\nprint(f\"dataset determined that: {vocab_size=}, {block_size=}\")\n\n# init model\nconfig = ModelConfig(vocab_size=vocab_size, block_size=block_size,\n                   n_layer=args.n_layer, n_head=args.n_head,\n                   n_embd=args.n_embd, n_embd2=args.n_embd2)\nif args.type == 'transformer':\n    model = Transformer(config)\nelif args.type == 'bigram':\n    model = Bigram(config)\nelif args.type == 'mlp':\n    model = MLP(config)\nelif args.type == 'rnn':\n    model = RNN(config, cell_type='rnn')\nelif args.type == 'gru':\n    model = RNN(config, cell_type='gru')\nelif args.type == 'bow':\n    model = BoW(config)\nelse:\n    raise ValueError(f'model type {args.type} is not recognized')\nmodel.to(args.device)\nprint(f\"model #params: {sum(p.numel() for p in model.parameters())}\")\nif args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n    print(\"resuming from existing model in the workdir\")\n    model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\nif args.sample_only:\n    print_samples(num=50)\n    sys.exit()\n\n# init optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n\n# init dataloader\nbatch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)\n\n\n\ntraining loop\nruns forever, save the model.state_dict() when a lower test loss is achieved\n\n# training loop\nbest_loss = None\nstep = 0\nwhile True:\n\n    t0 = time.time()\n\n    # get the next batch, ship to device, and unpack it to input and target\n    batch = batch_loader.next()\n    batch = [t.to(args.device) for t in batch]\n    X, Y = batch\n\n    # feed into the model\n    logits, loss = model(X, Y)\n\n    # calculate the gradient, update the weights\n    model.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n    if args.device.startswith('cuda'):\n        torch.cuda.synchronize()\n    t1 = time.time()\n\n    # logging\n    if step % 10 == 0:\n        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n\n    # evaluate the model\n    if step &gt; 0 and step % 500 == 0:\n        train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n        test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n        writer.add_scalar(\"Loss/train\", train_loss, step)\n        writer.add_scalar(\"Loss/test\", test_loss, step)\n        writer.flush()\n        print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n        # save the model to disk if it has improved\n        if best_loss is None or test_loss &lt; best_loss:\n            out_path = os.path.join(args.work_dir, \"model.pt\")\n            print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n            torch.save(model.state_dict(), out_path)\n            best_loss = test_loss\n\n    # sample from the model\n    if step &gt; 0 and step % 200 == 0:\n        print_samples(num=10)\n\n    step += 1\n    # termination conditions\n    if args.max_steps &gt;= 0 and step &gt;= args.max_steps:\n        break"
  },
  {
    "objectID": "posts/gist/walkthrough.html",
    "href": "posts/gist/walkthrough.html",
    "title": "bits of fastai live-coding sessions",
    "section": "",
    "text": "Jeremy Howard, the founder of fastai, organized a series of live coding sessions covering the basics of git, bash, vim, tmux, and more, as a companion to the free fastai course on deep learning. Here is the playlist and the forum post."
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-1",
    "href": "posts/gist/walkthrough.html#live-coding-1",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 1",
    "text": "Live coding 1\n\nintall WSL if on Windows (all commands below are typed in linux terminal in WSL)\nalt + enter for full screen\npwd print working directory\nwhich shows where a file is\nmkdir makes a directory\nls list stuffs -lah long format all files human readable\ndf -h disk free\ndu -sh * disk usage -s summary of all subdirectories in .\ndu -sh . disk usage of .\n\nconda-forge distribution as of september 2023, mambaforge/miniforge3 are the same. -c conda-forge is the default\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh to download the distribution\nbash Miniforge3-Linux-x86_64.sh -b to install, where -b for less intervention\ninstall pytorch here\nmamba install jupyter ipywidgets\nalias jl=\"jupyter lab\" add this command to the end of .bashrc to save alias for good\nmamba install -c fastai fastai\n\n\n\n\n\n\n\nTip\n\n\n\nJeremy automated setup of conda in this file. To run it, one needs to add executable permission to user chmod u+x setup-conda.sh"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-2",
    "href": "posts/gist/walkthrough.html#live-coding-2",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 2",
    "text": "Live coding 2\n\nbash\n\ncd - back to most recent directory\npushd ~ push current directory to a stack and change directory to ~\npopd pop what’s in the stack\nctrl+d exit for most program\nctrl+u ctrl+w ctrl+a ctrl+e move cursor faster\n\n\n\ntmux\n\ntmux\nctrl+b start of a tmux command, then\n\" split top-bottom\n% split left-right\nz zoom in/out\nd detach, back to bash\ntmux a attach stuffs running in tmux from bash (everything remains until restart computer)\n\n\n\ngit (with ssh)\n\nssh-keygen generates public/private rsa key pair (prompt where to save the file, by default it’s in ~/.ssh)\nlogin to github.com and upload public key cat ~/.ssh/id_rsa.pub\ngit clone git@github.com:fastai/fastbook.git\ngit status\ngit commit -am 'MESSAGE'\ngit push"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-3",
    "href": "posts/gist/walkthrough.html#live-coding-3",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 3",
    "text": "Live coding 3\n\nbash\n\nln -s ONE simlink ONE to here\n$PATH paths that bash knows to run program\n\n\n\npaperspace\n\npip install --user PACKAGE will install PACKAGE to ~/.local/lib/python3.*/site-packages which gets wiped after shutdown\nmv ~/.local /storage/.local then\nln -s /storage/.local ~/ to make it persistent (/storage is persistent across notebook instances)\n\n\n\njupyter lab\n\nctrl + shift + [ change tab\nctrl + b hide side column\n%%debug exit with q\nshift + tab or METHOD? shows signature\nMETHOD?? shows source code"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-4",
    "href": "posts/gist/walkthrough.html#live-coding-4",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 4",
    "text": "Live coding 4\n\n\n\n\n\n\nTip\n\n\n\nJeremy teaches how to write your first bash script. The job done via these scripts is to set up paperspace for persistent storage and configs across instances. The repo is here.\n\n\n\nfirst script pre-run.sh\n#!/usr/bin/env bash\n\npushd ~\n\nmkdir -p /storage/cfg\n\nif [ ! -e /storage/cfg/.conda ]; then\n        mamba create -yp /storage/cfg/.conda\nfi\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nchmod 700 /storage/cfg/.ssh\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\npopd\n\n\nsecond script setup.sh\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-5",
    "href": "posts/gist/walkthrough.html#live-coding-5",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 5",
    "text": "Live coding 5\n\nbash\n\ncat FILE display file\ncat f1 f2 &gt; combined concat\ncat f1 &gt;&gt; f2 append\n\n\n\nvim\n\ni insert mode\nesc back to command mode\nin command mode try :q to quit :wq to write and quit\ntutorial https://vim-adventures.com/\n\nalternatively, type code . then edit/create file with VS code"
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-6",
    "href": "posts/gist/walkthrough.html#live-coding-6",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 6",
    "text": "Live coding 6\n\ndu -sh * | grep 'G' search ouput of du -sh * that contains G to identify directories larger than GB\nconda install universal-ctags\ncopy config files to Paperspace (they’ll be persistent if we’ve run the bash script before in live coding 4.)\n\ncopy ssh keys to ~/.ssh and change permissions chmod 644 ~/.ssh/id_rsa.pub chmod 600 ~/.ssh/id_rsa\nfirst time git commit needs ~/.gitconfig to have name and email of the user, just follow the prompt."
  },
  {
    "objectID": "posts/gist/walkthrough.html#live-coding-7",
    "href": "posts/gist/walkthrough.html#live-coding-7",
    "title": "bits of fastai live-coding sessions",
    "section": "Live coding 7",
    "text": "Live coding 7\n\npip isntall --user kaggle\n\npre-append ~/.bashrc with export PATH=~/.local/bin:$PATH\nsource .bashrc\ncreate kaggle.json file from kaggle website and copy it into ~/.kaggle\nnavigate into .kaggle and chmod 600 kaggle.json\nkaggle competitions donwnload -c NAME\n\nTry time unzip -q BLA to see how long it takes to unzip.\n\nnvidia-smi dmon if sm is low, this means i/o slow. Try\n\nresize image\nmove files to local (see get_data.sh below)\nreduce augmentation\nchange to CPU instance?\n\n\nOn paperspace, create get_data.sh in /notebooks (persistent)\n#!/user/bin/env bash\ncd\nmkdir BLA\ncd BLA\nkaggle competitions donwnload -c NAME\nunzip -q NAME"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html",
    "href": "posts/gist/top-fifth-kaggle.html",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "",
    "text": "The playground series is devoted to tabular datasets and are the most accessible competitions for beginners to learn and develop skills. This blog/notebook is to showcase some streamlined approach to achieve a relatively good performance. The notebook is written for Kaggle playground series season 3 episode 26 (last one in 2023). At the time of writing this blog, the submission achieves 149/871 (top 18%) ranking. final ranking private leaderboard: 332/1663 (top 20%)\nOutline:\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html#getting-set-up",
    "href": "posts/gist/top-fifth-kaggle.html#getting-set-up",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "Getting set up",
    "text": "Getting set up\n\ncomp = 'playground-series-s3e26'\npath = setup_comp(comp, install='')\n\nDownloading playground-series-s3e26.zip to /home/xy/git/1principle/posts/gist\n\n\n100%|██████████████████████████████████████████████████████| 350k/350k [00:00&lt;00:00, 1.56MB/s]\n\n\n\n\n\n\n\n\n\npath\n\nPath('playground-series-s3e26')\n\n\n\nimport pandas as pd\n\ntrn_path = path/'train.csv'\ntrn = pd.read_csv(trn_path)\ntrn.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7905 entries, 0 to 7904\nData columns (total 20 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   id             7905 non-null   int64  \n 1   N_Days         7905 non-null   int64  \n 2   Drug           7905 non-null   object \n 3   Age            7905 non-null   int64  \n 4   Sex            7905 non-null   object \n 5   Ascites        7905 non-null   object \n 6   Hepatomegaly   7905 non-null   object \n 7   Spiders        7905 non-null   object \n 8   Edema          7905 non-null   object \n 9   Bilirubin      7905 non-null   float64\n 10  Cholesterol    7905 non-null   float64\n 11  Albumin        7905 non-null   float64\n 12  Copper         7905 non-null   float64\n 13  Alk_Phos       7905 non-null   float64\n 14  SGOT           7905 non-null   float64\n 15  Tryglicerides  7905 non-null   float64\n 16  Platelets      7905 non-null   float64\n 17  Prothrombin    7905 non-null   float64\n 18  Stage          7905 non-null   float64\n 19  Status         7905 non-null   object \ndtypes: float64(10), int64(3), object(7)\nmemory usage: 1.2+ MB"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html#preprocessing-data",
    "href": "posts/gist/top-fifth-kaggle.html#preprocessing-data",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nget_dataset(path, 'joebeachcapital/cirrhosis-patient-survival-prediction', force=True) # filename= cirrhosis.csv\n\n\ndef preprocess(df, train=True, dropna=False):\n    df_ = df.copy()\n    df_['is_gen']='Y'\n    if train:\n        df1 = pd.read_csv(path/'cirrhosis.csv') # original data based on which the dataset is synthesized\n        df1 = pd.concat([df1.drop('Status', axis=1), df1['Status']], axis=1) # move status to last col, same as df_\n        df1['is_gen']='N'\n        df1.columns = df_.columns\n        df_ = pd.concat([df_,df1], axis=0).reset_index(drop=True)\n        if dropna: df_=df_.dropna()\n        df_['Status']= df_.Status.map({'C':0, 'CL':1,'D':2})\n    return df_"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html#modelling",
    "href": "posts/gist/top-fifth-kaggle.html#modelling",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "Modelling",
    "text": "Modelling\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,PowerTransformer,LabelEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score, cross_validate, KFold\nfrom sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import make_scorer, mean_absolute_error, classification_report, log_loss\nfrom sklearn.linear_model import LogisticRegression\n\nfrom scipy.stats import loguniform\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nmodels not supporting nan\n\ndf = preprocess(pd.read_csv(trn_path),train=True, dropna=True)\nX, y = df.drop('Status', axis=1).iloc[:,1:], df['Status']\n\n\nct = make_column_transformer(\n                (PowerTransformer(), make_column_selector(dtype_include = np.number)),\n                (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), make_column_selector(dtype_include=object)), \n                remainder = 'passthrough')\n\n\n%%time\nlogit_cv =cross_val_score(\n    make_pipeline(ct, LogisticRegression(max_iter=1000)),\n    X,y, scoring = 'neg_log_loss', cv=10, n_jobs=-1)\nprint(f'logitstic regression {-logit_cv.mean()=}')\n\n/home/xy/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n\n\nlogitstic regression -logit_cv.mean()=0.5110572273752632\nCPU times: user 92.2 ms, sys: 20.3 ms, total: 113 ms\nWall time: 1.05 s\n\n\n\n%%time\nRF_cv = cross_val_score(make_pipeline(ct, RandomForestClassifier(**{'n_estimators': 1000,\n                                                  'criterion': 'log_loss',\n                                                  'max_depth': 14,\n                                                  'min_samples_split': 3,\n                                                  'min_samples_leaf': 1,\n                                                  'max_features': 4,\n                                                  'random_state': 1,\n                                                  'n_jobs': -1})),\n                        X, y, scoring = 'neg_log_loss', cv = 10, n_jobs=-1)\nprint(f\"random forrest {-RF_cv.mean()=}\")\n\nrandom forrest -RF_cv.mean()=0.44559989137605854\nCPU times: user 2min 23s, sys: 33.3 s, total: 2min 56s\nWall time: 48.5 s\n\n\n\n\nmodels supporting nan\n\ndf = preprocess(pd.read_csv(trn_path),train=True, dropna=False)\nX, y = df.drop('Status', axis=1).iloc[:,1:], df['Status']\n\n\n%%time\nHB_cv = cross_val_score(make_pipeline(ct, HistGradientBoostingClassifier(**{'l2_regularization': 8.876168706639714,\n                                                          'early_stopping': False,\n                                                          'learning_rate': 0.009956485590638034,\n                                                          'max_iter': 500,\n                                                          'max_depth': 16,\n                                                          'max_bins': 255,\n                                                          'min_samples_leaf': 16,\n                                                          'max_leaf_nodes': 18,\n                                                          'random_state': 3})),\n                        X, y, scoring = 'neg_log_loss', cv = 10, n_jobs = -1)\n\nprint(f\"histGB {-HB_cv.mean()=}\")\n\n/home/xy/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 6] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n\n\nhistGB -HB_cv.mean()=0.43771002014457927\nCPU times: user 162 ms, sys: 118 ms, total: 281 ms\nWall time: 15.5 s\n\n\n\n%%time\nLGBM_cv = cross_val_score(make_pipeline(ct,LGBMClassifier(**{'n_estimators': 1000,\n                                            'learning_rate': 0.013657589160895923,\n                                            'max_depth': 17,\n                                            'reg_alpha': 1.9791969860931342,\n                                            'reg_lambda': 1.2857088172765347,\n                                            'num_leaves': 37,\n                                            'subsample': 0.6351453342675659,\n                                            'colsample_bytree': 0.2644509924064132})),\n                          X, y, scoring = 'neg_log_loss', cv = 10, n_jobs = -1)\n\n\nprint(f\"Light GBM  {-LGBM_cv.mean()=}\") \n\nLight GBM  -LGBM_cv.mean()=0.42275781396747264\n\n\n\n%%time\nXGB_cv = cross_val_score(make_pipeline(ct, XGBClassifier(**{'max_depth': 7,\n                                          'learning_rate': 0.03570188608151033,\n                                          'n_estimators': 1000,\n                                          'gamma': 0.6440001307764849,\n                                          'min_child_weight': 2,\n                                          'colsample_bytree': 0.27034458854562116,\n                                          'subsample': 0.8435412915999765})), \n                          X, y, scoring = 'neg_log_loss', cv = 10, n_jobs = -1)\n\n\nprint(f\"XGBoost {-XGB_cv.mean()=}\")\n\nXGBoost -XGB_cv.mean()=0.42872410511564896\n\n\n\ndef cv(X,y,cv=10):\n    clf = LGBMClassifier(**{'n_estimators': 1000,\n                            'learning_rate': 0.013657589160895923,\n                            'max_depth': 17,\n                            'reg_alpha': 1.9791969860931342,\n                            'reg_lambda': 1.2857088172765347,\n                            'num_leaves': 37,\n                            'subsample': 0.6351453342675659,\n                            'colsample_bytree': 0.2644509924064132})\n    ct = make_column_transformer(\n                (PowerTransformer(), make_column_selector(dtype_include = np.number)),\n                (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), make_column_selector(dtype_include=object)), \n                remainder = 'passthrough')\n    model = make_pipeline(ct, clf)\n    return cross_validate(model, X, y, cv=cv, scoring='neg_log_loss', return_estimator=True, n_jobs=-1)\n\n\n%%time\ncv_output = cv(X,y,cv=10)\n\n\nprint(f\"{-cv_output['test_score'].mean()=}, {cv_output['test_score'].std()=}\")\n\n-cv_output['test_score'].mean()=0.4206909352553819, cv_output['test_score'].std()=0.04075158479629894"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html#submitting-to-kaggle",
    "href": "posts/gist/top-fifth-kaggle.html#submitting-to-kaggle",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\n\nss = pd.read_csv(path/'sample_submission.csv')\nss.head()\n\n\n\n\n\n\n\n\nid\nStatus_C\nStatus_CL\nStatus_D\n\n\n\n\n0\n7905\n0.628084\n0.034788\n0.337128\n\n\n1\n7906\n0.628084\n0.034788\n0.337128\n\n\n2\n7907\n0.628084\n0.034788\n0.337128\n\n\n3\n7908\n0.628084\n0.034788\n0.337128\n\n\n4\n7909\n0.628084\n0.034788\n0.337128\n\n\n\n\n\n\n\n\ntst = preprocess(pd.read_csv(path/'test.csv'), train=False)\ntst.head()\n\n\n\n\n\n\n\n\nid\nN_Days\nDrug\nAge\nSex\nAscites\nHepatomegaly\nSpiders\nEdema\nBilirubin\nCholesterol\nAlbumin\nCopper\nAlk_Phos\nSGOT\nTryglicerides\nPlatelets\nProthrombin\nStage\nis_gen\n\n\n\n\n0\n7905\n3839\nD-penicillamine\n19724\nF\nN\nY\nN\nN\n1.2\n546.0\n3.37\n65.0\n1636.0\n151.90\n90.0\n430.0\n10.6\n2.0\nY\n\n\n1\n7906\n2468\nD-penicillamine\n14975\nF\nN\nN\nN\nN\n1.1\n660.0\n4.22\n94.0\n1257.0\n151.90\n155.0\n227.0\n10.0\n2.0\nY\n\n\n2\n7907\n51\nPlacebo\n13149\nF\nN\nY\nN\nY\n2.0\n151.0\n2.96\n46.0\n961.0\n69.75\n101.0\n213.0\n13.0\n4.0\nY\n\n\n3\n7908\n2330\nD-penicillamine\n20510\nF\nN\nN\nN\nN\n0.6\n293.0\n3.85\n40.0\n554.0\n125.55\n56.0\n270.0\n10.6\n2.0\nY\n\n\n4\n7909\n1615\nD-penicillamine\n21904\nF\nN\nY\nN\nN\n1.4\n277.0\n2.97\n121.0\n1110.0\n125.00\n126.0\n221.0\n9.8\n1.0\nY\n\n\n\n\n\n\n\n\ntst_pred = np.stack([est.predict_proba(tst.iloc[:,1:]) for est in cv_output['estimator']]).mean(0)\n\n\nss.iloc[:,1:] = tst_pred\n\n\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nid,Status_C,Status_CL,Status_D\n7905,0.3034480206600728,0.02175049687406757,0.6748014824658597\n7906,0.464722990035046,0.17105995489987008,0.3642170550650838\n7907,0.034054616093133115,0.011479074721858778,0.954466309185008\n7908,0.9778662946803056,0.002733559845527006,0.01940014547416722\n7909,0.8730251010963693,0.042703149687327954,0.08427174921630272\n7910,0.9909153131787145,0.0011266786376778267,0.007958008183607803\n7911,0.9843376622366685,0.0014776242979965683,0.014184713465334847\n7912,0.0945863204842192,0.026772389955302976,0.8786412895604778\n7913,0.009370330198415863,0.0019239529424342871,0.98870571685915\n\n\n\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'lgbm 10fold avg', comp)"
  },
  {
    "objectID": "posts/gist/top-fifth-kaggle.html#conclusion",
    "href": "posts/gist/top-fifth-kaggle.html#conclusion",
    "title": "What is required to achieve a top 20% ranking in Kaggle playground series",
    "section": "Conclusion",
    "text": "Conclusion\nThe hypterparameters for each model should be found before hand with e.g. optuna. Here we copy those from this excellent notebook. Note however that the ensemble method adopted here is less sophisticated than the said notebook, the purpose of which is to reach a reasonable place faster.\nApart from careful ensemble (such as weighted average), it would be useful to exploit/create more predictive features. Some domain knowledge might come in handy.\nA different direction is to replace tree-based models by neural nets. This is worth another post and hopefully I will come back to it soon."
  },
  {
    "objectID": "posts/deep-dive/uml.html",
    "href": "posts/deep-dive/uml.html",
    "title": "Understanding Machine Learning: a series of 20 videos",
    "section": "",
    "text": "From Oct 2022 to Feb 2023, I have created a series of 20 videos (in Chinese) on machine learning theory and algorithms on bilibili.com, closely following the excellent textbook written by Shalev-Shwartz and Ben-David, which is freely available on the authors’ webpage.\n\nPlaylist in my YouTube channel\nTopics covered include\n\nProbably Approximately Correct (PAC) learning model\nVC dimension\nno free lunch theorem\nlinear predictors: logistic regression, linear regression, perceptron\nconvex learning problems\nregularisation\nstochastic gradient descent\nsupport vector machine\nkernel methods\ndecision trees\nboosting: AdaBoost\nclustering: k-means, spectral clustering, linkage-based clustering\ndimenionality reduction: PCA, compressed sensing\ngenerative models: naive Bayes, LDA, EM algorithms"
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html",
    "href": "posts/deep-dive/crm2-ml.html",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter estimation",
    "section": "",
    "text": "In a previous post, we’ve modelled the loss of a portfolio of \\(d\\) instruments as follows \\[\nL = \\sum_{i=1}^d \\mu_i S_i I_i,\n\\] where \\(L\\) represents the total loss, \\(\\mu_i\\) is the exposure at default, \\(S_i\\) is the loss given default, and \\(I_i\\) is the event of default for the \\(i\\)-th instrument. To compute expected loss, VaR, and other relevant metrics in risk management, it is crucial to estimate these underlying parameters accurately. In this post, our focus is on estimating \\(p_i=\\mathbb{E}[I_i]\\), which is formulated as a machine learning problem."
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html#estimating-probaiblity-of-default-pd",
    "href": "posts/deep-dive/crm2-ml.html#estimating-probaiblity-of-default-pd",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter estimation",
    "section": "Estimating probaiblity of default (PD)",
    "text": "Estimating probaiblity of default (PD)\nConsider the real-world example of a bank approving loans for applicants based on their profiles. In this scenario, every applicant must fill out a comprehensive application form, including details such as their profession, age, amount of debts, monthly salary, and so on. The bank maintains records and, in retrospect, knows who has defaulted on their loans.\nTo formalize this process, each applicant corresponds to a vector in \\(\\mathbb{R}^k\\), known as the feature vector, which incorporates all the information from the form (possibly encoded for categorical values, e.g., converting ‘profession’ into dummy variables). The output we aim to predict is whether the applicant is in default (1) or not (0).\nThis constitutes a binary classification problem. With a substantial number of input-output pairs (features and default status) available, supervised learning algorithms can be employed to learn a relationship that can subsequently be used for predictions.\nMany supervised learning algorithms for binary prediction actually output probabilities (specifically, the probability of the label being 1). This is suitable for our goal, as we precisely seek to estimate probabilities.\nFor illustration, let’s consider a credit default risk dataset from this Kaggle competition. We set up the competition with fastkaggle module.\n\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\ncomp = 'home-credit-default-risk'\npath = setup_comp(comp, install='')\npath.ls()\n\n(#10) [Path('home-credit-default-risk/application_train.csv'),Path('home-credit-default-risk/bureau_balance.csv'),Path('home-credit-default-risk/application_test.csv'),Path('home-credit-default-risk/sample_submission.csv'),Path('home-credit-default-risk/previous_application.csv'),Path('home-credit-default-risk/POS_CASH_balance.csv'),Path('home-credit-default-risk/HomeCredit_columns_description.csv'),Path('home-credit-default-risk/credit_card_balance.csv'),Path('home-credit-default-risk/installments_payments.csv'),Path('home-credit-default-risk/bureau.csv')]\n\n\nWe primarily look into application_train.csv, which contains 308k rows and 121 input features. While there are numerous aspects to discuss regarding this dataset, for the sake of brevity, I will address two significant points here:\n\nMissing values\nThere are many missing values in this dataset. To be more precise, 41 columns actually have half of their values missing.\nThis is a critical issue that needs to be addressed because many off-the-shelf machine learning models in scikit-learn, such as logistic regression, random forest, and support vector machines, cannot handle missing values represented as np.nan. There are two possible options to handle this:\n\nImpute the missing values before feeding the data into these models. Imputation can be done using “rule-based” methods such as scikit-learn’s SimpleImputer or “learning-based” methods such as scikit-learn’s IterativeImputer.\nUse a different model that supports missing values natively. For instance, many gradient boosting implementations like HistGradientBoosting, XGBoost, LightGBM, and CatBoost handle missing values natively.\n\nFor the sake of providing a quick benchmark, we have opted for the second option and are using scikit-learn’s gradient boosting implementation, HistGradientBoostingClassifier. Explaining the detailed workings of gradient boosting is a vast topic that we might delve into in a future post.\n\n\nUnbalanced data\nRoughly 8% of the obligors goes into default in this dataset, making it unbalanced. From the perspective of risk management, it is important to predict defaults (label 1) accurately. Therefore, when it comes to evaluate model performance, accuracy is not an appropriate metric. Simply predicting non-default for all obligors would result in a correct prediction 92% of the time, but it’s never correct for the defaults.\nApplying a machine learning model directly to an unbalanced dataset can lead to sub-optimal results. We’ll demonstrate this point in the next section. A simple mitigation strategy is to sub-sample the majority class to match the size of the minority class, creating a balanced dataset. In this example, the minority class comprises roughly 25k rows, so we can train a model using this strategy as the balanced data is not too small to be useful. xx Obviously, the downside of this strategy is that we have thrown away lots of valuable data, but let’s not worry about it at this stage of obtaining a quick benchmark."
  },
  {
    "objectID": "posts/deep-dive/crm2-ml.html#implementation",
    "href": "posts/deep-dive/crm2-ml.html#implementation",
    "title": "Credit Risk Models Ep 2 machine learning methods for parameter estimation",
    "section": "Implementation",
    "text": "Implementation\nFirst we feed the whole unbalanced dataset into HistGradientBoostingClassifier.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\ndf = pd.read_csv(path/'application_train.csv')\nX = df.drop('TARGET', axis=1)\ny = df.TARGET\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=1123)\n\nohe = OneHotEncoder(drop='if_binary')\nmodel = HistGradientBoostingClassifier()\n\nct = make_column_transformer(\n    (ohe, make_column_selector(dtype_exclude=np.number)),\n    remainder='passthrough',\n)\n\npipe = make_pipeline(\n    ct,model\n)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.92      0.96     61330\n           1       0.02      0.56      0.04       173\n\n    accuracy                           0.92     61503\n   macro avg       0.51      0.74      0.50     61503\nweighted avg       1.00      0.92      0.96     61503\n\n\n\nAs we can see, the precision for class 1 is only 0.02, indicating that amongst all that are identified as defaults, only 2% of them are true defaults. Does this mean that the model we chose is rubbish? Not necessarily. Let’s use the exact same model but now with a balanced dataset created using the sub-sampling strategy.\n\nmask = (df['TARGET']==1)\nm = mask.sum()\n\ndf_balance = pd.concat([df[~mask].sample(m,random_state=1),df[mask]], axis=0)\n\ny = df_balance.TARGET\nX = df_balance.drop('TARGET', axis=1)\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=112)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n\n              precision    recall  f1-score   support\n\n           0       0.70      0.68      0.69      5050\n           1       0.68      0.69      0.69      4880\n\n    accuracy                           0.69      9930\n   macro avg       0.69      0.69      0.69      9930\nweighted avg       0.69      0.69      0.69      9930\n\n\n\nMuch better! All the metrics look roughly the same, hovering around 69%. While this is far from being deployable in the real world, as a baseline, it’s far more reasonable than our previous attempt.\nWe’ll conclude the post here. To further enhance the overall performance, it’s necessary to meticulously explore the features, engage in more feature engineering, employ clever methods of data imputation, and conduct thorough hyperparameter tuning. Give it a go!"
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html",
    "href": "posts/deep-dive/crm1-fun.html",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "",
    "text": "This is the first post in a series dedicated to credit risk models. The model discussed in this post is intentionally naive, as it relies on several unrealistic assumptions for the sake of simplicity and tractability. Despite its simplicity, this model serves as an excellent starting point for understanding the fundamental concepts of credit risk. It lays the foundation for more complex models that will be explored later in this series. The mathematical concepts introduced in this post are commonly found in any first-year probability textbook"
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#pd-lgd-ead-approach",
    "href": "posts/deep-dive/crm1-fun.html#pd-lgd-ead-approach",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "PD-LGD-EAD approach",
    "text": "PD-LGD-EAD approach\nCredit risk models are mathematical representations of the potential losses within a portfolio of financial instruments. Each instrument carries a probability of default, leading to either a complete loss or a partial loss. Modeling credit risk is crucial as it offers valuable insights to stakeholders, enabling them to take proactive measures to mitigate these losses. Consequently, it plays a pivotal role in the financial sector.\nLet’s introduce some notation. Consider a portfolio containing \\(d\\) instruments. The event of default for the \\(i\\)-th instrument is denoted as \\(D_i\\). Here, \\(\\mu_i\\) represents the exposure at default (EAD) of the \\(i\\)-th instrument, and \\(S_i \\in [0,1]\\) the loss given default (LGD) where \\(S\\) signifies severity, indicating the actual loss relative to the exposure. We define \\(I_i := I_{D_i}\\) as the indicator of the event \\(D_i\\), which follows a Bernoulli distribution with parameter \\(p_i := \\mathbb{P}[D_i]\\). The total loss of the portfolio is expressed as the sum: \\[\nL = \\sum_{i=1}^d \\mu_i S_i I_i\n\\tag{1}\\] From the perspective of risk managers, the values of \\(\\mu_i\\) are known. However, the uncertainly lies in whether an instrument will default and the corresponding severity, making the total loss \\(L\\) a random variable. Our interest lies in understanding the probability distribution of \\(L\\).\nIn this post we make the following naive assumptions.\n\n\n\n\n\n\nImportant\n\n\n\nAssume that\n\n\\(\\{(I_i,S_i)\\}_{i=1}^d\\) is a family of independent random vectors.\n\\(I_i\\) is independent of \\(S_i\\) for all \\(i\\in [d]\\).\n\n\n\nUnder this assumption, it is clear that \\[\n\\mathbb{E}[L] = \\sum_{i=1}^d \\mu_i \\mathbb{E}[S_i] p_i\n\\] \\[\n\\mathrm{Var}[L] = \\sum_{i=1}^d \\mu_i^2 (\\mathbb{E}[S_i^2] p_i  - \\mathbb{E}[S]^2 p_i^2)\n\\] The expected loss (EL) is an important quantity in the Basel III guidelines."
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#var-and-expected-shortfalls",
    "href": "posts/deep-dive/crm1-fun.html#var-and-expected-shortfalls",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "VaR and expected shortfalls",
    "text": "VaR and expected shortfalls\nIn a stable economy, default events are infrequent, making credit risk modeling primarily focused on rare occurrences. Obtaining a precise measure of the credit risk of a portfolio is not achieved by merely calculating the average; instead, it’s crucial to comprehend the quantiles. In the realm of credit risk, these quantiles are referred to as the Value at Risk (VaR).\n\\[\n\\mathrm{VaR}_\\alpha(L) = \\inf\\{t\\in\\mathbb{R}: \\mathbb{P}[L\\ge t]\\le \\alpha \\}.\n\\] Another commonly utilized metric is the Expected Shortfall, which represents the conditional expectation of the loss \\(L\\) given that \\(L\\) exceeds the VaR \\[\nE_\\alpha(L) = \\mathbb{E}[L|L\\ge\\mathrm{VaR}_\\alpha].\n\\] Clearly \\(E_\\alpha(L)\\ge \\mathrm{VaR}_\\alpha(L)\\).\nIt’s crucial to acknowledge that the joint distribution of \\((I_i, S_i)\\) is unknown. To compute VaR and expected shortfalls, risk analysts must undertake at least three tasks:\n\nModel Calibration: This involves determining the model’s parameters using available data.\nDistribution Computation: Compute the distribution of \\(L\\) either analytically or through Monte Carlo simulation, based on the calibrated parameters.\nMetrics Computation: Utilize the obtained distribution to compute VaR, shortfall, or other relevant metrics either analytically or numerically.\n\nThe calibration process demands substantial effort and is specific to the chosen model. We will delve into this topic in a future post. For the remainder of this discussion, let’s assume that we have already established a set of parameters and focus on the last two steps.\nIt’s important to emphasize that deriving the exact distribution of \\(L\\) can be tedious, if not infeasible. In practice, risk analysts resort to simulations and numerical computations to determine VaR and shortfalls. This pragmatic approach is the one we adopt here."
  },
  {
    "objectID": "posts/deep-dive/crm1-fun.html#monte-carlo-simulation-for-estimating-var-and-shortfalls",
    "href": "posts/deep-dive/crm1-fun.html#monte-carlo-simulation-for-estimating-var-and-shortfalls",
    "title": "Credit Risk Models Ep 1 Fundamentals",
    "section": "Monte Carlo simulation for estimating VaR and shortfalls",
    "text": "Monte Carlo simulation for estimating VaR and shortfalls\nStep 1\nWe first specify the parameters \\(p_i, \\mu_i\\) and parameters for the distribution of \\(S_i\\). These parameters are generated at random with arbitrary choice of distributions, following\nBolder (2018)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# set random seed for reproducibility\ng = np.random.default_rng(1123)\n\nd = 100 # number of instruments\n\n# set the probability of default for d instruments, low 0.1 high 7 per cent\nx = g.chisquare(1,(d,))\nx = np.where(x&lt;0.1, 0.1, x)\nx = np.where(x&gt;7, 7, x)\np = x/100 \n\n# set exposures parameters normalised to have total exposure 1000\ny = g.weibull(3,(d,))\nmu = y*1000/y.sum()\n\n# set serverity parameters\na,b = 1.5, 2.5\n\nStep 2\nNext we simulate \\(L\\) for a large number of times accoding to Equation 1. We sort the samples in the end, useful for computing the empirical distributions later on.\n\nm = 50_000 # number of repetitions\nS = g.beta(a, b, (m,d))\nI = g.uniform(size=(m,d)) &lt; p\nL = (S*I)@ mu\nL = np.sort(L)\n\nStep 3\nWe plot the histogram and empirical distribution, then estimate VaR and expected shortfalls.\n\n# Estimate VaR \npers = np.array([95,99,99.9])\nalphas = 1 - pers/100\nvars = np.percentile(L, pers)\n\nfig, axes = plt.subplots(2,1)\naxes[0].hist(L, bins=30, label='Histogram of Loss')\nfor i, var in enumerate(vars): \n  axes[0].axvline(var,linestyle='dashed', color=g.uniform(size=(3,)),label=f'VaR{alphas[i]:.3f}')\naxes[0].legend()\n\naxes[1].plot(L,np.linspace(0,1,len(L),endpoint=False), color= 'red', label='empirical distribution')\naxes[1].legend()\nfor var in vars: \n  axes[1].axvline(var,linestyle='dashed')\nplt.show()\n\n# Estimate shortfalls\nshortfalls = np.zeros(len(vars))\nfor i,var in enumerate(vars):\n  shortfalls[i] = L[L&gt;var].mean()\n\npd.set_option('display.precision', 4)\npd.DataFrame({'alphas': alphas, 'VaR': vars, 'Shortfalls': shortfalls}).set_index('alphas')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVaR\nShortfalls\n\n\nalphas\n\n\n\n\n\n\n0.050\n12.6668\n16.2741\n\n\n0.010\n18.6163\n21.4599\n\n\n0.001\n25.4548\n28.1533\n\n\n\n\n\n\n\nWe’ve completed tasks 2 and 3! Next time, we’ll delve into methods of parameter estimation, still within the naive setting. This will bring us full circle and complete the lifecycle of a single model."
  }
]