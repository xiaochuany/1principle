[
  {
    "objectID": "posts/deep-dive/uml.html",
    "href": "posts/deep-dive/uml.html",
    "title": "Understanding Machine Learning: a series of 20 videos",
    "section": "",
    "text": "Note\n\n\n\nAs preparation for teaching a machine learning course to a partner university in China, I created a series of 20 videos (in Chinese) on machine learning theory and algorithms on bilibili.com from October 2022 to February 2023. I closely followed the excellent textbook written by Shalev-Shwartz and Ben-David, which is freely available on the authors’ webpage.\n\n\nTopics covered include\n\nProbably Approximately Correct (PAC) learning model\nVC dimension\nno free lunch theorem\nlinear predictors: logistic regression, linear regression, perceptron\nconvex learning problems\nregularisation\nstochastic gradient descent\nsupport vector machine\nkernel methods\ndecision trees\nboosting: AdaBoost\nclustering: k-means, spectral clustering, linkage-based clustering\ndimenionality reduction: PCA, compressed sensing\ngenerative models: naive Bayes, LDA, EM algorithms\n\nPlaylist in my YouTube channel"
  },
  {
    "objectID": "posts/deep-dive/CRM1-naive.html",
    "href": "posts/deep-dive/CRM1-naive.html",
    "title": "Fast intro to credit risk models 1 - naive model",
    "section": "",
    "text": "Note\n\n\n\nThis is the first post in a series dedicated to credit risk models. The model discussed in this post is intentionally naive, as it relies on several unrealistic assumptions for the sake of simplicity and tractability. Despite its simplicity, this model serves as an excellent starting point for understanding the fundamental concepts of credit risk. It lays the foundation for more complex models that will be explored later in this series. The mathematical concepts introduced in this post are commonly found in any first-year probability textbook\n\n\nCredit risk models are mathematical representations of the potential losses within a portfolio of financial instruments. Each instrument carries a probability of default, leading to either a complete loss or a partial loss. Modeling credit risk is crucial as it offers valuable insights to stakeholders, enabling them to take proactive measures to mitigate these losses. Consequently, it plays a pivotal role in the financial sector.\nLet’s introduce some notation. Consider a portfolio containing \\(d\\) instruments. The event of default for the \\(i\\)-th instrument is denoted as \\(D_i\\). Here, \\(\\mu_i\\) represents the potential exposure of the \\(i\\)-th instrument, and \\(S_i \\in [0,1]\\) signifies severity, indicating the actual exposure relative to the potential exposure. We define \\(I_i := I_{D_i}\\) as the indicator of the event \\(D_i\\), which follows a Bernoulli distribution with parameter \\(p_i := \\mathbb{P}[D_i]\\). The total loss of the portfolio is expressed as the sum: \\[\nL = \\sum_{i=1}^d \\mu_i S_i I_i\n\\tag{1}\\] From the perspective of risk managers, the values of \\(\\mu_i\\) are known. However, the uncertainly lies in whether an instrument will default and the corresponding severity, making the total loss \\(L\\) a random variable. Our interest lies in understanding the probability distribution of \\(L\\).\nIn this post we make the following naive assumptions.\n\n\n\n\n\n\nImportant\n\n\n\nAssume that\n\n\\(\\{(I_i,S_i)\\}_{i=1}^d\\) is a family of independent random vectors.\n\\(I_i\\) is independent of \\(S_i\\) for all \\(i\\in [d]\\).\n\n\n\nIn a stable economy, default events are infrequent, making credit risk modeling primarily focused on rare occurrences. Obtaining a precise measure of the credit risk of a portfolio is not achieved by merely calculating the average; instead, it’s crucial to comprehend the quantiles. In the realm of credit risk, these quantiles are referred to as the Value at Risk (VaR).\n\\[\n\\mathrm{VaR}_\\alpha(L) = \\inf\\{t\\in\\mathbb{R}: \\mathbb{P}[L\\ge t]\\le \\alpha \\}.\n\\] Another commonly utilized metric is the Expected Shortfall, which represents the conditional expectation of the loss \\(L\\) given that \\(L\\) exceeds the VaR \\[\nE_\\alpha(L) = \\mathbb{E}[L|L\\ge\\mathrm{VaR}_\\alpha].\n\\] Clearly \\(E_\\alpha(L)\\ge \\mathrm{VaR}_\\alpha(L)\\).\nIt’s crucial to acknowledge that the joint distribution of \\((I_i, S_i)\\) is unknown. To compute VaR and expected shortfalls, risk analysts must undertake at least three tasks:\n\nModel Calibration: This involves determining the model’s parameters using available data.\nDistribution Computation: Compute the distribution of \\(L\\) either analytically or through Monte Carlo simulation, based on the calibrated parameters.\nQuantile Computation: Utilize the obtained distribution to compute VaR, shortfall, or other relevant metrics either analytically or numerically.\n\nThe calibration process demands substantial effort and is specific to the chosen model. We will delve into this topic in a future post. For the remainder of this discussion, let’s assume that we have already established a set of parameters and focus on the last two steps.\nIt’s important to emphasize that deriving the exact distribution of \\(L\\) can be tedious, if not infeasible. In practice, risk analysts resort to simulations and numerical computations to determine VaR and shortfalls. This pragmatic approach is the one we adopt here.\nStep 1\nWe first specify the parameters \\(p_i, \\mu_i\\) and parameters for the distribution of \\(S_i\\). These parameters are generated at random with arbitrary choice of distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# set random seed for reproducibility\ng = np.random.default_rng(1123)\n\nd = 100 # number of instruments\n\n# set the probability of default for d instruments, low 0.1 high 7 per cent\nx = g.chisquare(1,(d,))\nx = np.where(x&lt;0.1, 0.1, x)\nx = np.where(x&gt;7, 7, x)\np = x/100 \n\n# set exposures parameters normalised to have total exposure 1000\ny = g.weibull(3,(d,))\nmu = y*1000/y.sum()\n\n# set serverity parameters\na,b = 1.5, 2.5\n\nStep 2\nNext we simulate \\(L\\) for a large number of times accoding to Equation 1. We sort the samples in the end, useful for computing the empirical distributions later on.\n\nm = 50_000 # number of repetitions\nS = g.beta(a, b, (m,d))\nI = g.uniform(size=(m,d)) &lt; p\nL = (S*I)@ mu\nL = np.sort(L)\n\nStep 3\nWe plot the histogram and empirical distribution, then estimate VaR and expected shortfalls.\n\n# Estimate VaR \npers = np.array([95,99,99.9])\nalphas = 1 - pers/100\nvars = np.percentile(L, pers)\n\nfig, axes = plt.subplots(2,1)\naxes[0].hist(L, bins=30, label='Histogram of Loss')\nfor i, var in enumerate(vars): \n  axes[0].axvline(var,linestyle='dashed', color=g.uniform(size=(3,)),label=f'VaR{alphas[i]:.3f}')\naxes[0].legend()\n\naxes[1].plot(L,np.linspace(0,1,len(L),endpoint=False), color= 'red', label='empirical distribution')\naxes[1].legend()\nfor var in vars: \n  axes[1].axvline(var,linestyle='dashed')\nplt.show()\n\n# Estimate shortfalls\nshortfalls = np.zeros(len(vars))\nfor i,var in enumerate(vars):\n  shortfalls[i] = L[L&gt;var].mean()\n\npd.set_option('display.precision', 4)\npd.DataFrame({'alphas': alphas, 'VaR': vars, 'Shortfalls': shortfalls}).set_index('alphas')\n\n\n\n\n\n\n\n\n\n\n\nVaR\nShortfalls\n\n\nalphas\n\n\n\n\n\n\n0.050\n12.6668\n16.2741\n\n\n0.010\n18.6163\n21.4599\n\n\n0.001\n25.4548\n28.1533\n\n\n\n\n\n\n\nWe’ve completed tasks 2 and 3! Next time, we’ll delve into methods of parameter estimation, still within the naive setting. This will bring us full circle and complete the lifecycle of a single model.\nReference: David Jamieson Bolder - Credit-Risk Modelling (2018)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Passionate about probabilistic machine learning and quantitative finance, Xiaochuan explores the intricate intersection of mathematics and real-world applications.\nCurious about Xiaochuan’s research? Dive into his world through his webpage."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematics in the real world",
    "section": "",
    "text": "Fast intro to credit risk models 1 - naive model\n\n\n\n\n\n\n\ncredit risk modelling\n\n\napplied probability\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Machine Learning: a series of 20 videos\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\n  \n\n\n\n\nSecret ‘SAWS’ of deep learning, feature backprop\n\n\n\n\n\n\n\ndeep learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nXiaochuan Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deep-dive/saws.html",
    "href": "posts/deep-dive/saws.html",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "",
    "text": "Note\n\n\n\nMy colleague Simon Shaw came up with the memorable notation SAWS for the key recursion step in backprop in a second year undergraduate course in deep learning. I found this a perfect example of “good notation helps memorisation” which hopefully reinforces understanding. Here’s how we get to this recursion and a python implementation.\nEveryone knows that a neural network is a universal function approximator that can be used to learn complex relationships between inputs and outputs. It is a learning machine that mimics biological neural networks in the human brain. Mathematically, given an input \\(x\\in\\mathbb{R}^{n_x}\\), a neural nerwork computes an ouput \\(\\hat y\\in\\mathbb{R}^{n_y}\\) as follows, \\[\\begin{align*}\na^{[1]} &= \\sigma(W^{[1]\\top} x + b^{[1]}) \\\\\na^{[2]} &= \\sigma(W^{[2]\\top} a^{[1]} + b^{[2]}) \\\\\n&\\vdots \\\\\na^{[L]} &= \\sigma(W^{[L]\\top} a^{[L-1]} + b^{[L]}) \\\\\n\\hat y &= \\sigma(W^{[L+1]\\top} a^{[L]} + b^{[L+1]})\n\\end{align*}\\]\nHere \\(\\sigma:\\mathbb{R}\\to\\mathbb{R}\\) is any nonlinear differentiable function (or sufficiently close to be such), \\(L\\) is the number of hidden layers, \\(W\\) and \\(b\\) are weight matrices and bias vectors. We use \\(n_l\\) to denote the number of hidden nodes in the \\(l\\)-th layer. The pre-activation vector at layer \\(l\\) is denoted by \\(z^{[l]}\\).\nFrom the definition we see that \\(\\hat y\\) is not only a function of \\(x\\), but also a function of \\(z^{[1]}, z^{[2]}\\) and so forth using sub-networks. This may seem like stating the obvious, but it’s a useful fact to keep in mind when we derive the algorithm for training these networks."
  },
  {
    "objectID": "posts/deep-dive/saws.html#sec-training",
    "href": "posts/deep-dive/saws.html#sec-training",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Training",
    "text": "Training\nHow to train a neural network to make good predictions? Well, we first need to specify a computable objective. To achieve this we introduce a loss function that measures how good or bad a predictor is compared to the ground truth. This is called supervised learning. A commonly used loss is the squared loss \\[\\begin{align*}\n    \\ell(u,v) =  \\frac{1}{2} \\|u-v\\|^2\n\\end{align*}\\] where \\(\\|\\cdot\\|\\) is the Euclidean norm. Then our goal is to minimize \\(J=\\ell(y,\\hat y)\\).\nA general-purpose optimization method is gradient descent. In every iteration step, this method updates the parameters (i.e. weights and biases) by moving along the opposite of the gradient of the loss with respect to the parameters. Due to the characterization of the gradient of a function as being the direction along which the function increases the most (infinitesimally speaking), this method is heuristically justified for minimizing an objective function when the step size (aka learning rate) is not too large.\nA crucial point is that we need to compute all the partial derivatives for every gradient descent step! Mathematically this is tedious but not hard at all. Everyone knows that to differentiate a composition of functions, the chain rule is our best friend. Sure, we have multiple compositions, but it does not produce any conceptual complications because we can just apply the chain rule multiple times.\nTake weight matrix \\(W^{[1]}\\) as an example. Any small nudge on its value would result in changes in \\(z^{[1]}\\), and once we have the value of \\(z^{[1]}\\), we feed it into the sub-network made of layers \\(1\\) to \\(L+1\\) and get the output. Hence \\(J=g(z^{[1]})\\) for some \\(g\\). By the chain rule,\n\\[\\begin{align*}\n    \\frac{\\partial J}{\\partial W^{[1]}} =  \n    \\sum_i \\frac{\\partial J}{\\partial z^{[1]}_i} \\frac{\\partial z^{[1]}_i}{\\partial W^{[1]}}.\n\\end{align*}\\]\nThe second gradient in the summand is the rather simple because \\(z^{[1]}\\) is a linear function of \\(W^{[1]}\\). However, the first gradient is not explicit because the function \\(g\\) is cumbersome as a composition of compositions of compositions … What we can do is to apply chain rule again, then \\[\\begin{align*}\n    \\frac{\\partial J}{\\partial z^{[1]}}\n    = \\sum_i \\frac{\\partial J}{\\partial z^{[2]}_i} \\frac{\\partial z^{[2]}_i}{\\partial z^{[1]}}\n\\end{align*}\\]\nRecalling \\(z^{[2]} = W^{[2]\\top} \\sigma(z^{[1]}) + b^{[2]}\\), the second gradient is easy to calculate. Hence, the gradient of \\(J\\) with respect to the first layer pre-activation is a linear combination of the gradient with respect to the second layer pre-activation. Applying the chain rule recursively in the forward direction all the way to the output layer, we can express \\(\\frac{\\partial J}{\\partial z^{[1]}}\\) as a multiple sum over \\(\\frac{\\partial J}{\\partial z^{[L+1]}}\\) times multiple products of gradients of consecutive pre-activations.\nFollowing the same recipe, we can compute the gradients with respect to weights and biases of all the layers. In summary, we would need for all \\(l=1,\\ldots,L+1\\):\nand chain them together using multiple sums and products. This seems like a lot of work, even for a computer!\nHere comes an important observation. There are lots of redundant computations if we use the recipe just described to compute an explicit form for all the gradients at every layer.\nThe big idea is to take advantage of the recursive relations between gradients with respect to pre-activations of consecutive layers. To be more precise, let’s rewrite both equations at a general layer (we also include an equation for the biases).\n\\[\n\\frac{\\partial J}{\\partial W^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial W^{[l]}}.\n\\tag{1}\\]\n\\[\\frac{\\partial J}{\\partial b^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l]}_i} \\frac{\\partial z^{[l]}_i}{\\partial b^{[l]}}\n\\tag{2}\\]\n\\[\\frac{\\partial J}{\\partial z^{[l]}} = \\sum_i \\frac{\\partial J}{\\partial z^{[l+1]}_i} \\frac{\\partial z^{[l+1]}_i}{\\partial z^{[l]}}\n\\tag{3}\\]\nLet \\(S^{[l]} = \\frac{\\partial J}{\\partial z^{[l]}}\\). We use equations Equation 1 and Equation 2, along with \\(S^{[L+1]}\\) (easy to compute), to find the required gradients for updating \\(W^{[L+1]}\\) and \\(b^{[L+1]}\\). Then we use equation Equation 3 to find \\(S^{[L]}\\), which can be plugged back into equations Equation 1 and Equation 2 to get the required gradient for updating \\(W^{[L]}\\) and \\(b^{[L]}\\), and so on.\nWhat we just described is the famous backpropagation. The advantage of this approach is that we compute each basic computation (itemized above) only once for each gradient descent iteration.\nFrom layer to layer, the computation is done sequentially, because output of \\(l+1\\)-th layer is requiredd as the input for computing gradients of \\(l\\)-th layer. For each fixed \\(l\\), however, it is better to parallelise the computation and write Equation 1 -Equation 3 in matrix forms. Here is how we do it:\n\nSquared loss\nSet \\(A^{[l]}=\\mathrm{diag}(\\sigma'(z^{[l]}))\\) and \\(e = y - \\hat y\\). It is easy to see that \\[\\begin{align*}\n    S^{[L+1]} = - A^{[L+1]} e\n\\end{align*}\\] Equation Equation 3 can be written in matrix form as well \\[\nS^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}\n\\tag{4}\\] Now the gradients \\[\\begin{align*}\n    dW^{[l]} &= a^{[l-1]} S^{[l]\\top} \\\\\n    db^{[l]} &= S^{[l]}\n\\end{align*}\\]\nEquation 4 is what I meant by secret “SAWS” of deep learning!\n\n\nCross entropy loss\nThe XE loss is defined for two probability mass functions as follows \\[\\begin{align*}\n    \\ell(u,v) = -\\sum_k u_k\\log v_k.\n\\end{align*}\\] It is particularly well suited for multiclass classification problem. To ensure that \\(\\hat y\\) is a probability mass function. We use the softmax activation function at the output layer \\[\\begin{align*}\n    \\mathrm{softmax}(x) =  \\frac{e^{x}}{\\sum_i e^{x_i}}\n\\end{align*}\\] All we have to do is to change the computation of \\(S^{[L+1]}\\), namely the gradient of the XE loss with respect to the output layer pre-activation, then back propagate using the last three equations in the squared loss case. A routine application of chain rule yields that \\[\\begin{align*}\n    S^{[L+1]} = - e\n\\end{align*}\\] where \\(e\\) was defined earlier in the squared loss case."
  },
  {
    "objectID": "posts/deep-dive/saws.html#implementation",
    "href": "posts/deep-dive/saws.html#implementation",
    "title": "Secret ‘SAWS’ of deep learning, feature backprop",
    "section": "Implementation",
    "text": "Implementation\nHere is a python implementation of the training with min-batch SGD, 1 hidden layer, sigmoid activation in the hidden and output layers, and squared loss.\n\nimport numpy as np\n\ndef sig(x):\n    return (1+np.exp(-x))**-1\n\ng = np.random.default_rng(1123)\n\nd,m = 20, 1000\nd_out = 10\nd_hid = 20\nbatch_size = 8\nn_epoch = 2\nlr = 0.01\n\nX_train = g.normal(0,1,(d,m))\nY_train = g.uniform(size=(d_out,m))\n\nW1 = g.normal(0,1,(d,d_hid))\nW2 = g.normal(0,1,(d_hid,d_out))\nb1 = g.normal(0,1,(d_hid,1))*0.01\nb2 = g.normal(0,1,(d_out,1))*0.01\n\nerrors=[]\n\nfor epoch in range(n_epoch):\n    error = 0\n    # permute the data\n    perm_idx = g.permutation(m)\n    X_train = X_train[:,perm_idx]\n    Y_train = Y_train[:,perm_idx]\n    for bat in range(m//batch_size):\n        x_batch = X_train[:,bat*batch_size:(1+bat)*batch_size]\n        y_batch = Y_train[:,bat*batch_size:(1+bat)*batch_size]\n        # forward pass\n        z1 = np.matmul(W1.T,x_batch) + b1\n        a1 = sig(z1)\n        z2 = np.matmul(W2.T,a1) + b2\n        a2 = sig(z2)\n        # backward pass\n        e = y_batch - a2\n        A2 = sig(z2)*(1-sig(z2))\n        S2 = - A2*e\n        A1 = sig(z1)*(1-sig(z1))\n        S1 = A1*np.matmul(W2,S2)\n        # gradient descent\n        dW2 = np.matmul(a1,S2.T)\n        db2 = np.sum(S2, axis = 1, keepdims=True)\n        dW1 = np.matmul(x_batch,S1.T)\n        db1 = np.sum(S1, axis = 1, keepdims=True)\n        W2 -= lr * dW2\n        b2 -= lr * db2\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        # compute the error \n        error += 0.5*np.sum(e*e)\n    # report error of the current epoch\n    print(\"Epoch:\", epoch, \"SE:\", error)\n    errors.append(error)\n\nEpoch: 0 SE: 1084.095701311093\nEpoch: 1 SE: 931.4561396806077\n\n\nExercise: implement the training with XE loss, more hidden layers, and the tricks we will mention in the next section."
  }
]