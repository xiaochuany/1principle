<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.466">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Xiaochuan Yang">
<meta name="dcterms.date" content="2023-10-10">

<title>first-ish principle - Secret ‘SAWS’ of deep learning, feature backprop</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">first-ish principle</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/xiaochuany"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Secret ‘SAWS’ of deep learning, feature backprop</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
                <div class="quarto-category">code</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Xiaochuan Yang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 10, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#multi-layer-perceptron" id="toc-multi-layer-perceptron" class="nav-link active" data-scroll-target="#multi-layer-perceptron">Multi-Layer Perceptron</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a>
  <ul class="collapse">
  <li><a href="#squared-loss" id="toc-squared-loss" class="nav-link" data-scroll-target="#squared-loss">Squared loss</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">Cross entropy loss</a></li>
  </ul></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation">Python Implementation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!-- \usepackage{minted} -->
<blockquote class="blockquote">
<p>My colleague Simon Shaw came up with the memorable notation SAWS for the key recursion step in backprop in a Year 2 module on deep learning. I found this a perfect example of “good notation helps memorisation” which hopefully reinforces understanding. In this post, I will explain how we get to this recursion and end the post with a python implementation.</p>
</blockquote>
<section id="multi-layer-perceptron" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<p>Everyone knows that a neural network is a universal function approximator that can be used to learn complex relationships between inputs and outputs. It is a learning machine that mimics biological neural networks in the human brain. Mathematically, given an input <span class="math inline">\(x\in\mathbb{R}^{n_x}\)</span>, a neural nerwork computes an ouput <span class="math inline">\(\hat y\in\mathbb{R}^{n_y}\)</span> as follows, <span class="math display">\[\begin{align*}
a^{[1]} &amp;= \sigma(W^{[1]\top} x + b^{[1]}) \\
a^{[2]} &amp;= \sigma(W^{[2]\top} a^{[1]} + b^{[2]}) \\
&amp;\vdots \\
a^{[L]} &amp;= \sigma(W^{[L]\top} a^{[L-1]} + b^{[L]}) \\
\hat y &amp;= \sigma(W^{[L+1]\top} a^{[L]} + b^{[L+1]})
\end{align*}\]</span></p>
<p>Here <span class="math inline">\(\sigma:\mathbb{R}\to\mathbb{R}\)</span> is any nonlinear differentiable function (or sufficiently close to be such), <span class="math inline">\(L\)</span> is the number of hidden layers, <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> are weight matrices and bias vectors. We use <span class="math inline">\(n_l\)</span> to denote the number of hidden nodes in the <span class="math inline">\(l\)</span>-th layer. The pre-activation vector at layer <span class="math inline">\(l\)</span> is denoted by <span class="math inline">\(z^{[l]}\)</span>.</p>
<p>From the definition we see that <span class="math inline">\(\hat y\)</span> is not only a function of <span class="math inline">\(x\)</span>, but also a function of <span class="math inline">\(z^{[1]}, z^{[2]}\)</span> and so forth using sub-networks. This may seem like stating the obvious, but it’s a useful fact to keep in mind when we derive the algorithm for training these networks.</p>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>How to train a neural network to make good predictions? Well, we first need to specify a computable objective. To achieve this we introduce a loss function that measures how good or bad a predictor is compared to the ground truth. This is called supervised learning. A commonly used loss is the squared loss <span class="math display">\[\begin{align*}
    \ell(u,v) =  \frac{1}{2} \|u-v\|^2
\end{align*}\]</span> where <span class="math inline">\(\|\cdot\|\)</span> is the Euclidean norm. Then our goal is to minimize <span class="math inline">\(J=\ell(y,\hat y)\)</span>.</p>
<p>A general-purpose optimization method is gradient descent. In every iteration step, this method updates the parameters (i.e.&nbsp;weights and biases) by moving along the opposite of the gradient of the loss with respect to the parameters. Due to the characterization of the gradient of a function as being the direction along which the function increases the most (infinitesimally speaking), this method is heuristically justified for minimizing an objective function when the step size (aka learning rate) is not too large.</p>
<p>A crucial point is that we need to compute all the partial derivatives for every gradient descent step! Mathematically this is tedious but not hard at all. Everyone knows that to differentiate a composition of functions, the chain rule is our best friend. Sure, we have multiple compositions, but it does not produce any conceptual complications because we can just apply the chain rule multiple times.</p>
<p>Take weight matrix <span class="math inline">\(W^{[1]}\)</span> as an example. Any small nudge on its value would result in changes in <span class="math inline">\(z^{[1]}\)</span>, and once we have the value of <span class="math inline">\(z^{[1]}\)</span>, we feed it into the sub-network made of layers <span class="math inline">\(1\)</span> to <span class="math inline">\(L+1\)</span> and get the output. Hence <span class="math inline">\(J=g(z^{[1]})\)</span> for some <span class="math inline">\(g\)</span>. By the chain rule,<br>
<span class="math display">\[\begin{align*}
    \frac{\partial J}{\partial W^{[1]}} =  
    \sum_i \frac{\partial J}{\partial z^{[1]}_i} \frac{\partial z^{[1]}_i}{\partial W^{[1]}}.
\end{align*}\]</span></p>
<p>The second gradient in the summand is the rather simple because <span class="math inline">\(z^{[1]}\)</span> is a linear function of <span class="math inline">\(W^{[1]}\)</span>. However, the first gradient is not explicit because the function <span class="math inline">\(g\)</span> is cumbersome as a composition of compositions of compositions … What we can do is to apply chain rule again, then <span class="math display">\[\begin{align*}
    \frac{\partial J}{\partial z^{[1]}}
    = \sum_i \frac{\partial J}{\partial z^{[2]}_i} \frac{\partial z^{[2]}_i}{\partial z^{[1]}}
\end{align*}\]</span></p>
<p>Recalling <span class="math inline">\(z^{[2]} = W^{[2]\top} \sigma(z^{[1]}) + b^{[2]}\)</span>, the second gradient is easy to calculate. Hence, the gradient of <span class="math inline">\(J\)</span> with respect to the first layer pre-activation is a linear combination of the gradient with respect to the second layer pre-activation. Applying the chain rule recursively in the forward direction all the way to the output layer, we can express <span class="math inline">\(\frac{\partial J}{\partial z^{[1]}}\)</span> as a multiple sum over <span class="math inline">\(\frac{\partial J}{\partial z^{[L+1]}}\)</span> times multiple products of gradients of consecutive pre-activations.</p>
Following the same recipe, we can compute the gradients with respect to weights and biases of all the layers. In summary, we would need for all <span class="math inline">\(l=1,\ldots,L+1\)</span>:
<p>and chain them together using multiple sums and products. This seems like a lot of work, even for a computer!</p>
<p>Here comes an important observation. There are lots of redundant computations if we use the recipe just described to compute an explicit form for all the gradients at every layer.</p>
<p>The big idea is to take advantage of the recursive relations between gradients with respect to pre-activations of consecutive layers. To be more precise, let’s rewrite both equations at a general layer (we also include an equation for the biases).</p>
<p><span id="eq-gradW"><span class="math display">\[
\frac{\partial J}{\partial W^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l]}_i} \frac{\partial z^{[l]}_i}{\partial W^{[l]}}.
\tag{1}\]</span></span></p>
<p><span id="eq-gradb"><span class="math display">\[\frac{\partial J}{\partial b^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l]}_i} \frac{\partial z^{[l]}_i}{\partial b^{[l]}}
\tag{2}\]</span></span></p>
<p><span id="eq-recursion"><span class="math display">\[\frac{\partial J}{\partial z^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l+1]}_i} \frac{\partial z^{[l+1]}_i}{\partial z^{[l]}}
\tag{3}\]</span></span></p>
<p>Let <span class="math inline">\(S^{[l]} = \frac{\partial J}{\partial z^{[l]}}\)</span>. We use equations <a href="#eq-gradW" class="quarto-xref">Equation&nbsp;1</a> and <a href="#eq-gradb" class="quarto-xref">Equation&nbsp;2</a>, along with <span class="math inline">\(S^{[L+1]}\)</span> (easy to compute), to find the required gradients for updating <span class="math inline">\(W^{[L+1]}\)</span> and <span class="math inline">\(b^{[L+1]}\)</span>. Then we use equation <a href="#eq-recursion" class="quarto-xref">Equation&nbsp;3</a> to find <span class="math inline">\(S^{[L]}\)</span>, which can be plugged back into equations <a href="#eq-gradW" class="quarto-xref">Equation&nbsp;1</a> and <a href="#eq-gradb" class="quarto-xref">Equation&nbsp;2</a> to get the required gradient for updating <span class="math inline">\(W^{[L]}\)</span> and <span class="math inline">\(b^{[L]}\)</span>, and so on.</p>
<p>What we just described is the famous backpropagation. The advantage of this approach is that we compute each basic computation (itemized above) only once for each gradient descent iteration.</p>
<p>From layer to layer, the computation is done sequentially, because output of <span class="math inline">\(l+1\)</span>-th layer is requiredd as the input for computing gradients of <span class="math inline">\(l\)</span>-th layer. For each fixed <span class="math inline">\(l\)</span>, however, it is better to parallelise the computation and write <a href="#eq-gradW" class="quarto-xref">Equation&nbsp;1</a> -<a href="#eq-recursion" class="quarto-xref">Equation&nbsp;3</a> in matrix forms. Here is how we do it:</p>
<section id="squared-loss" class="level3">
<h3 class="anchored" data-anchor-id="squared-loss">Squared loss</h3>
<p>Set <span class="math inline">\(A^{[l]}=\mathrm{diag}(\sigma'(z^{[l]}))\)</span> and <span class="math inline">\(e = y - \hat y\)</span>. It is easy to see that <span class="math display">\[\begin{align*}
    S^{[L+1]} = - A^{[L+1]} e
\end{align*}\]</span> Equation <a href="#eq-recursion" class="quarto-xref">Equation&nbsp;3</a> can be written in matrix form as well <span id="eq-saws"><span class="math display">\[
S^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}
\tag{4}\]</span></span> Now the gradients <span class="math display">\[\begin{align*}
    dW^{[l]} &amp;= a^{[l-1]} S^{[l]\top} \\
    db^{[l]} &amp;= S^{[l]}
\end{align*}\]</span></p>
<p><a href="#eq-saws" class="quarto-xref">Equation&nbsp;4</a> is what I meant by secret “SAWS” of deep learning!</p>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">Cross entropy loss</h3>
<p>The XE loss is defined for two probability mass functions as follows <span class="math display">\[\begin{align*}
    \ell(u,v) = -\sum_k u_k\log v_k.
\end{align*}\]</span> It is particularly well suited for multiclass classification problem. To ensure that <span class="math inline">\(\hat y\)</span> is a probability mass function. We use the softmax activation function at the output layer <span class="math display">\[\begin{align*}
    \mathrm{softmax}(x) =  \frac{e^{x}}{\sum_i e^{x_i}}
\end{align*}\]</span> All we have to do is to change the computation of <span class="math inline">\(S^{[L+1]}\)</span>, namely the gradient of the XE loss with respect to the output layer pre-activation, then back propagate using the last three equations in the squared loss case. A routine application of chain rule yields that <span class="math display">\[\begin{align*}
    S^{[L+1]} = - e
\end{align*}\]</span> where <span class="math inline">\(e\)</span> was defined earlier in the squared loss case.</p>
</section>
</section>
<section id="python-implementation" class="level2">
<h2 class="anchored" data-anchor-id="python-implementation">Python Implementation</h2>
<p>Here is a python implementation of the training with min-batch SGD, 1 hidden layer, sigmoid activation in the hidden and output layers, and squared loss.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sig(x):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))<span class="op">**-</span><span class="dv">1</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> np.random.default_rng(<span class="dv">1123</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>d,m <span class="op">=</span> <span class="dv">20</span>, <span class="dv">1000</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>d_out <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>d_hid <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>n_epoch <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> g.normal(<span class="dv">0</span>,<span class="dv">1</span>,(d,m))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> g.uniform(size<span class="op">=</span>(d_out,m))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> g.normal(<span class="dv">0</span>,<span class="dv">1</span>,(d,d_hid))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> g.normal(<span class="dv">0</span>,<span class="dv">1</span>,(d_hid,d_out))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> g.normal(<span class="dv">0</span>,<span class="dv">1</span>,(d_hid,<span class="dv">1</span>))<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> g.normal(<span class="dv">0</span>,<span class="dv">1</span>,(d_out,<span class="dv">1</span>))<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>errors<span class="op">=</span>[]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epoch):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># permute the data</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    perm_idx <span class="op">=</span> g.permutation(m)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> X_train[:,perm_idx]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    Y_train <span class="op">=</span> Y_train[:,perm_idx]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> bat <span class="kw">in</span> <span class="bu">range</span>(m<span class="op">//</span>batch_size):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        x_batch <span class="op">=</span> X_train[:,bat<span class="op">*</span>batch_size:(<span class="dv">1</span><span class="op">+</span>bat)<span class="op">*</span>batch_size]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        y_batch <span class="op">=</span> Y_train[:,bat<span class="op">*</span>batch_size:(<span class="dv">1</span><span class="op">+</span>bat)<span class="op">*</span>batch_size]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        z1 <span class="op">=</span> np.matmul(W1.T,x_batch) <span class="op">+</span> b1</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        a1 <span class="op">=</span> sig(z1)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        z2 <span class="op">=</span> np.matmul(W2.T,a1) <span class="op">+</span> b2</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        a2 <span class="op">=</span> sig(z2)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward pass</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        e <span class="op">=</span> y_batch <span class="op">-</span> a2</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        A2 <span class="op">=</span> sig(z2)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>sig(z2))</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        S2 <span class="op">=</span> <span class="op">-</span> A2<span class="op">*</span>e</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        A1 <span class="op">=</span> sig(z1)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>sig(z1))</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        S1 <span class="op">=</span> A1<span class="op">*</span>np.matmul(W2,S2)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># gradient descent</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        dW2 <span class="op">=</span> np.matmul(a1,S2.T)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        db2 <span class="op">=</span> np.<span class="bu">sum</span>(S2, axis <span class="op">=</span> <span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        dW1 <span class="op">=</span> np.matmul(x_batch,S1.T)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        db1 <span class="op">=</span> np.<span class="bu">sum</span>(S1, axis <span class="op">=</span> <span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        W2 <span class="op">-=</span> lr <span class="op">*</span> dW2</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        b2 <span class="op">-=</span> lr <span class="op">*</span> db2</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        W1 <span class="op">-=</span> lr <span class="op">*</span> dW1</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        b1 <span class="op">-=</span> lr <span class="op">*</span> db1</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the error </span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        error <span class="op">+=</span> <span class="fl">0.5</span><span class="op">*</span>np.<span class="bu">sum</span>(e<span class="op">*</span>e)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># report error of the current epoch</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Epoch:"</span>, epoch, <span class="st">"SE:"</span>, error)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    errors.append(error)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 SE: 1084.095701311093
Epoch: 1 SE: 931.4561396806077</code></pre>
</div>
</div>
<p><strong>Exercise</strong>: implement the training with XE loss and more hidden layers.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>