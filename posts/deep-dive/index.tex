% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Secret `SAWS' of deep learning, feature backprop},
  pdfauthor={Xiaochuan Yang},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Secret `SAWS' of deep learning, feature backprop}
\author{Xiaochuan Yang}
\date{2023-10-08}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, enhanced, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, sharp corners, breakable, frame hidden]}{\end{tcolorbox}}\fi

\usepackage{minted}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathrsfs,mathtools,bbm}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\NN{\mathbb{N}}
\def\QQ{\mathbb{Q}}
\def\PP{\mathbb{P}}
\def\XX{\mathbb{X}}
\def\EE{\mathbb{E}}
\def\Var{\mathbb{V}\mathrm{ar}}
\def\Cov{\mathbb{C}\mathrm{ov}}
\def\Corr{\mathbb{C}\mathrm{orr}}
\def\Ent{\mathbb{E}\mathrm{nt}}
\def\1{\mathbbm{1}}

Everyone knows that a neural network is a universal function
approximator that can be used to learn complex relationships between
inputs and outputs. It is a learning machine that mimics biological
neural networks in the human brain. Mathematically, given an input
\(x\in\mathbb{R}^{n_x}\), a neural nerwork computes an ouput
\(\hat y\in\mathbb{R}^{n_y}\) as follows, \begin{align*}
a^{[1]} &= \sigma(W^{[1]\top} x + b^{[1]}) \\
a^{[2]} &= \sigma(W^{[2]\top} a^{[1]} + b^{[2]}) \\
&\vdots \\
a^{[L]} &= \sigma(W^{[L]\top} a^{[L-1]} + b^{[L]}) \\
\hat y &= \sigma(W^{[L+1]\top} a^{[L]} + b^{[L+1]})
\end{align*}

Here \(\sigma:\mathbb{R}\to\mathbb{R}\) is any nonlinear differentiable
function (or sufficiently close to be such), \(L\) is the number of
hidden layers, \(W\) and \(b\) are weight matrices and bias vectors. We
use \(n_l\) to denote the number of hidden nodes in the \(l\)-th layer.
The pre-activation vector at layer \(l\) is denoted by \(z^{[l]}\).

From the definition we see that \(\hat y\) is not only a function of
\(x\), but also a function of \(z^{[1]}, z^{[2]}\) and so forth using
sub-networks. This may seem like stating the obvious, but it's a useful
fact to keep in mind when we derive the algorithm for training these
networks.

\hypertarget{sec-training}{%
\subsection{Training}\label{sec-training}}

How to train a neural network to make good predictions? Well, we first
need to specify a computable objective. To achieve this we introduce a
loss function that measures how good or bad a predictor is compared to
the ground truth. This is called supervised learning. A commonly used
loss is the squared loss \begin{align*}
    \ell(u,v) =  \frac{1}{2} \|u-v\|^2
\end{align*} where \(\|\cdot\|\) is the Euclidean norm. Then our goal is
to minimize \(J=\ell(y,\hat y)\).

A general-purpose optimization method is gradient descent. In every
iteration step, this method updates the parameters (i.e.~weights and
biases) by moving along the opposite of the gradient of the loss with
respect to the parameters. Due to the characterization of the gradient
of a function as being the direction along which the function increases
the most (infinitesimally speaking), this method is heuristically
justified for minimizing an objective function when the step size (aka
learning rate) is not too large.

A crucial point is that we need to compute all the partial derivatives
for every gradient descent step! Mathematically this is tedious but not
hard at all. Everyone knows that to differentiate a composition of
functions, the chain rule is our best friend. Sure, we have multiple
compositions, but it does not produce any conceptual complications
because we can just apply the chain rule multiple times.

Take weight matrix \(W^{[1]}\) as an example. Any small nudge on its
value would result in changes in \(z^{[1]}\), and once we have the value
of \(z^{[1]}\), we feed it into the sub-network made of layers \(1\) to
\(L+1\) and get the output. Hence \(J=g(z^{[1]})\) for some \(g\). By
the chain rule,\\
\begin{align*}
    \frac{\partial J}{\partial W^{[1]}} =  
    \sum_i \frac{\partial J}{\partial z^{[1]}_i} \frac{\partial z^{[1]}_i}{\partial W^{[1]}}. 
\end{align*}

The second gradient in the summand is the rather simple because
\(z^{[1]}\) is a linear function of \(W^{[1]}\). However, the first
gradient is not explicit because the function \(g\) is cumbersome as a
composition of compositions of compositions \ldots{} What we can do is
to apply chain rule again, then \begin{align*}
    \frac{\partial J}{\partial z^{[1]}}
    = \sum_i \frac{\partial J}{\partial z^{[2]}_i} \frac{\partial z^{[2]}_i}{\partial z^{[1]}} 
\end{align*}

Recalling \(z^{[2]} = W^{[2]\top} \sigma(z^{[1]}) + b^{[2]}\), the
second gradient is easy to calculate. Hence, the gradient of \(J\) with
respect to the first layer pre-activation is a linear combination of the
gradient with respect to the second layer pre-activation. Applying the
chain rule recursively in the forward direction all the way to the
output layer, we can express \(\frac{\partial J}{\partial z^{[1]}}\) as
a multiple sum over \(\frac{\partial J}{\partial z^{[L+1]}}\) times
multiple products of gradients of consecutive pre-activations.

Following the same recipe, we can compute the gradients with respect to
weights and biases of all the layers. In summary, we would need for all
\(l=1,\ldots,L+1\):

\begin{itemize}
    \item $\frac{\partial z^{[l]}}{\partial z^{[l-1]}}$
    \item $\frac{\partial z^{[l]}}{\partial W^{[l]}}$
\end{itemize}

and chain them together using multiple sums and products. This seems
like a lot of work, even for a computer!

Here comes an important observation. There are lots of redundant
computations if we use the recipe just described to compute an explicit
form for all the gradients at every layer.

The big idea is to take advantage of the recursive relations between
gradients with respect to pre-activations of consecutive layers. To be
more precise, let's rewrite both equations at a general layer (we also
include an equation for the biases).

\begin{equation}\protect\hypertarget{eq-gradW}{}{
\frac{\partial J}{\partial W^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l]}_i} \frac{\partial z^{[l]}_i}{\partial W^{[l]}}.
}\label{eq-gradW}\end{equation}

\begin{equation}\protect\hypertarget{eq-gradb}{}{\frac{\partial J}{\partial b^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l]}_i} \frac{\partial z^{[l]}_i}{\partial b^{[l]}}
}\label{eq-gradb}\end{equation}

\begin{equation}\protect\hypertarget{eq-recursion}{}{\frac{\partial J}{\partial z^{[l]}} = \sum_i \frac{\partial J}{\partial z^{[l+1]}_i} \frac{\partial z^{[l+1]}_i}{\partial z^{[l]}}
}\label{eq-recursion}\end{equation}

Let \(S^{[l]} = \frac{\partial J}{\partial z^{[l]}}\). We use equations
Equation~\ref{eq-gradW} and Equation~\ref{eq-gradb}, along with
\(S^{[L+1]}\) (easy to compute), to find the required gradients for
updating \(W^{[L+1]}\) and \(b^{[L+1]}\). Then we use equation
Equation~\ref{eq-recursion} to find \(S^{[L]}\), which can be plugged
back into equations Equation~\ref{eq-gradW} and Equation~\ref{eq-gradb}
to get the required gradient for updating \(W^{[L]}\) and \(b^{[L]}\),
and so on.

What we just described is the famous backpropagation. The advantage of
this approach is that we compute each basic computation (itemized above)
only once for each gradient descent iteration.

From layer to layer, the computation is done sequentially, because
output of \(l+1\)-th layer is requiredd as the input for computing
gradients of \(l\)-th layer. For each fixed \(l\), however, it is better
to parallelise the computation and write Equation~\ref{eq-gradW}
-Equation~\ref{eq-recursion} in matrix forms. Here is how we do it:

\hypertarget{squared-loss}{%
\subsubsection{Squared loss}\label{squared-loss}}

Set \(A^{[l]}=\mathrm{diag}(\sigma'(z^{[l]}))\) and \(e = y - \hat y\).
It is easy to see that \begin{align*}
    S^{[L+1]} = - A^{[L+1]} e 
\end{align*} Equation Equation~\ref{eq-recursion} can be written in
matrix form as well \begin{equation}\protect\hypertarget{eq-saws}{}{
S^{[l]} = A^{[l]} W^{[l+1]} S^{[l+1]}
}\label{eq-saws}\end{equation} Now the gradients \begin{align*}
    dW^{[l]} &= a^{[l-1]} S^{[l]\top} \\
    db^{[l]} &= S^{[l]}
\end{align*}

Equation~\ref{eq-saws} is what I meant by secret ``SAWS'' of deep
learning!

\hypertarget{cross-entropy-loss}{%
\subsubsection{Cross entropy loss}\label{cross-entropy-loss}}

The XE loss is defined for two probability mass functions as follows
\begin{align*}
    \ell(u,v) = -\sum_k u_k\log v_k.
\end{align*} It is particularly well suited for multiclass
classification problem. To ensure that \(\hat y\) is a probability mass
function. We use the softmax activation function at the output layer
\begin{align*}
    \mathrm{softmax}(x) =  \frac{e^{x}}{\sum_i e^{x_i}}
\end{align*} All we have to do is to change the computation of
\(S^{[L+1]}\), namely the gradient of the XE loss with respect to the
output layer pre-activation, then back propagate using the last three
equations in the squared loss case. A routine application of chain rule
yields that \begin{align*}
    S^{[L+1]} = - e
\end{align*} where \(e\) was defined earlier in the squared loss case.

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

Here is a python implementation of the training with min-batch SGD, 1
hidden layer, sigmoid activation in the hidden and output layers, and
squared loss.

\textbf{Exercise}: implement the training with XE loss, more hidden
layers, and the tricks we will mention in the next section.



\end{document}
