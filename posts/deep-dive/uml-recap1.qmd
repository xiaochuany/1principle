---
title: "Machine Learning Recap 1 Concepts"
author: "Xiaochuan Yang"
date: "2023-10-27"
categories: [python, machine learning]
draft: false
toc: true
number-sections: false
---

\def\PP{\mathbb{P}}
\def\VAR{\mathrm{VaR}}
\def\RR{\mathbb{R}}
\def\EE{\mathbb{E}}


## Setting up the scene  

In a supervised learning, we specify 

- an example space $\mathcal X$  
- a label space $\mathcal Y$
-  a collection of hypotheses $h: \mathcal X \to \mathcal Y$, making up the hypothesis class  (inductive bias), from which we want to pick a predictor
-  a loss function $\ell: \mathcal Y \times \mathcal Y \to \RR_+$, quantifying how good or bad a prediction $\hat y$ is compared to the ground truth label $y$


:::{.callout-important}
We are given an independent and identically distributed input-output pairs 
$S = \{(x_i,y_i), i=[m]\}\subset \mathcal X\times\mathcal Y$ with distribution $D$. 
:::

Our goal is to pick the "best" predictor in the sense of minimising the true loss  
$$
L_D(h) = \EE_{(x,y)\sim D}[\ell(h(x),y))]  
$$
where $D$ is the *true* distribution of $(x,y)$.

Obviously *a priori* the distribution $D$ of the samples is unkonwn. However by law of large numbers we know that 
$$
L_S(h):= \frac{1}{m}\sum_{i=1}^m \ell(h(x_i),y_i)
$$
is a consistent estimator of $L_D(h)$ (when $m\to\infty$ under mild condition on the distribution of $\ell(h(x),y)$). This motivates the empirical risk minimisation (ERM) approach i.e. we look for 
$$
h_S \in \mathrm{argmin}_{h\in\mathcal H} L_S(h)
$$
Hence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when $m$ is large, for a fixed $h$. Whether such approximation is valid uniformly for all the hypothesis in $\mathcal H$, in other words, whether $h_S\approx h^*\in \mathrm{argmin}_{h\in\mathcal H} L_D(h)$, is at the centre of the so-called learning theory. 

We often decompose the generalisation error $L_D(h_S)$ in two parts:
$$
L_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)] 
$$
the first is called the approximation error, and second estimation error. 


## Let's be concrete  

From a practical point of view, we may not want to get into the learning theory bounds despite its elegance, what we do is to  split the sample $S^0$ into two parts $S, V$, where $S$ is used to find an ERM which we denote by $h_S$, another for testing whether the found ERM achieves small $L_D(h_S)$. The rationale is simple, since we make iid assumption, $V$ is independent of $S$, hence $L_{V}(h_S) \approx L_D(h_S)$ when $|V|$ is not too small. Therefore, the smallness of $L_{V}(h_S)$ indicates good quality (small true risk) of our predictor $h_1$. 

The split method for arrays is implemented in `sklearn.model_selection`

```{python}
from sklearn.model_selection import train_test_split
import numpy as np
g = np.random.default_rng(12)
x = g.normal(0,1,(20,))
x_tr, x_te = train_test_split(x,test_size=0.2,random_state=12)
```

Typically, examples are vectors in $\RR^d, d\ge 1$. In $k$-way ($k\ge 2$) classification problems, labels are one-hot encodings $e_1,..., e_k$ where $e_i$ is the unit vector in $\RR^k$ with one on the $i$ th coordinate and zero elsewhere. If $k=2$, we can drop the second coordinate and simply denote the two classes by $\{1, -1\}$ or $\{0,1\}$. In regression problems, the labels are in the continuum $\RR^k, k\ge 1$. 

Now consider $\mathcal H$. For classification problems, instead of predicting discrete class labels directly, it is sometimes beneficial to predict a probability mass function over the $k$ classes. From the pmf, a label can be obtained by taking argmax. In other words, the range of $h\in\mathcal H$ is assumed to be $\{y\in\RR^k: y_i\ge 0, y_1+...+y_k=1\}$. For regression problems there are no such constraints and the range can be the whole $\RR^k$. 

The choie of the loss function may vary, depending on what goal we are trying to achiecve. Researchers can design new losses suitable for their use case. Here we mention a few popular ones. For classification problems, if we use hypothesis predicting pmf, the cross entropy loss is often a good choice
$$
XE(p,\hat p) =  - \sum_{i=1}^k p_i \log(\hat p_i)
$$
For regression problems, the squared loss is often a good choice
$$SE(y,\hat y)= \|y-\hat y\|^2_2$$
where $\|.\|$ is Euclidean norm. 

One of the advantages of these loss functions is that they are convex in the $\hat p$ or $\hat y$ variables, making it possible to leverage the machinery of convex optimistion when it comes to the actual training process. 

Many loss functions are already implemented in `sklearn.metrics`. The XE is named `log_loss` and the squared loss is named `mean_sqquared_error`. Let's list all of them.

```{python}
import sklearn.metrics as metrics
for m in dir(metrics):
    if not m.startswith('_'): print(m)
```

It comes in handy that `sklearn` has them already defined. Implementing each one of them is often not hard, e.g.
```{python}
def log_loss(yt,yp):
    yt[yt==0]= 1-yp[yt==0]
    return -np.log(y).mean()
```


<!-- ## Model selection / hyperparameter tuning 

Models may have many hyperparameters (learning rate, tree depth, neural network architectures etc). One can use k-fold cross validation to pick the one that optimises a certain metric prescribed by the user of these models. This is 

```{python}
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
``` -->

## Analysis of errors

In practice, $D$ is unknown and we only observe the training error $L_S(h_S)$ and the validation error $L_V(h_S)$. Hence we decompose the generalisation error differently
$$
L_D(h_S) = [L_D(h_S) - L_V(h_S)] + [L_V(h_S) - L_S(h_S)] + L_S(h_S)
$$
The first term is small when $|V|$ is moderately large by independence of $V$ and $S$. 
The second and third term are observalbe and several cases may arise. 

- the gap is small and the training error is small. This is a happy scenario
- the training error is large. To address this, we may consider
  - engarge the hypothesis class, 
  - completely changing it, 
  - find a better feature representation,
  - find a better optimiser  
- training error is small but the gap is large. To address this, we may consider
  - add regularisation
  - get more training data
  - reduce the hypothesis class

It is benefiical to plot the learning curve during training. This amounts to visualise the training error and validation error on the same plot as time evolves (every X batches, every X epoch etc). 

