{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5211294d-c48f-4f8a-a52f-647bea6feb2a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learn transformer with makemore and torch\n",
    "author: Xiaochuan Yang\n",
    "date: '2024-02-16'\n",
    "categories: [deep learning, python]\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9f11c",
   "metadata": {},
   "source": [
    "## makemore\n",
    "\n",
    "Karpathy's [makemore](https://github.com/karpathy/makemore) is an end-to-end python application that takes in a pure text file, then generate new text similar to what's given. The project `makemore` is part of his [neural networks: zero to hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) series which I recommend to all. \n",
    "\n",
    "The example data in the repo  is a large collection of baby names (about 30k names) and the applicaiton trains a state of the art **transformer** model to learn the  mechanism of naming things, then sample from the learned model.  \n",
    "\n",
    "The purpose of this post is three fold:  \n",
    "- gain familiarity with **transformer**  \n",
    "- summarise essential `torch` functionalities and workflow for building neural nets application  \n",
    "- use minimal tools e.g `argparse` to produce a command line interface for the app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d4387-44b3-4cd2-98b3-886ccd679974",
   "metadata": {},
   "source": [
    "## torch stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421b774-ac6c-48cf-b592-1d2d1a13431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc0dfe-b4d7-41bd-99cc-7af94952b891",
   "metadata": {},
   "source": [
    "Simplistically, a neural net application is built on two major components: a dataset and a model. `torch` allows us to do both fairly easily and straightforward.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "`Dataset` class is a container like a sequence. To define a custom Dataset class, one must implement `__init__`, `__len__`,  `__getitem__`, together with transformations relevant to the particular use case of the application. Exampe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8b793-8053-45f1-80fe-0b631de31e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words:list[str], chars:str, max_word_length:int):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759590a6-b4e0-4659-b822-eb28f6cdcb65",
   "metadata": {},
   "source": [
    "The custom transformations in this example consists of mapping character to integer, aka tokenisation. There is one special token `0` representing both the start of name and end of name. We can loop through or get at index a CharDataset because we have implemented sufficient dunder methods for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba078a-b7bf-4750-bc01-779b0f03474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n",
      "(tensor([ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0]), tensor([18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1]))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "examples = CharDataset(['emma', 'richard'], string.ascii_letters, 10)\n",
    "for e in examples:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7bfcc-8842-4277-bc65-1ce40a6a3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n"
     ]
    }
   ],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba9465-3526-44e5-8af7-868a7e8d0925",
   "metadata": {},
   "source": [
    "Let's break down the output tuple `x,y`. \n",
    "\n",
    "### `x` tensor\n",
    "It starts with `0` token, then each input character gets mapped to an integer from 1 to 26, with trailing zeros if the length of name is less than `max_word_length` (set to be max length of names in the dataset)\n",
    "\n",
    "### `y` tensor\n",
    "By definition, it is the same as `x` shifted by 1 token to the left, modulo extra subtleties with the trailing -1. What is going on here? Well, in language modelling, the learning task is next token prediction, so at index `idx` such that `x[idx].item()!=0`, given `x[:idx+1]`, the goal is to predict `x[idx+1]` which by definition is nothing but `y[idx]`.  If `x[idx].item()==0`, then there is nothing to predict (name finished), we set by convention `y[idx]=-1`.\n",
    "\n",
    "Our ultimate goal is to build and train a neural net which can learn from the 30k  `x,y` tuples a good way of sampling next token given some context. Concetely, throw a `0` token at the model and let the model sample the next token `t1`, concatenate it with `0`, then sample next given `[0,t1]`, until a `0` token is sampled which means we have arrived at the end of a name. Repeat this to produce as many names as we want. We'll get back to inference later.\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "For efficiency's sake, it is beneficial to stack multiple examples together, aka mini-batch, and process them all at once. The `DataLoader` class is meant to help us with this.  Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a3f8-bcca-47b0-9cb8-bb632f9e85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    \"\"\"\n",
    "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
    "    a better way in PyTorch to just create an infinite dataloader?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80355603-6656-43e6-89f1-c43228bfcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharDataset(['emma', 'richard', 'ben', 'steve'],string.ascii_letters, 10)\n",
    "batch_loader = InfiniteDataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc76ef-9a1f-4ed4-871a-2ebd1df19764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0],\n",
      "        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n",
      "------------------------------------------------------------\n",
      "Y=tensor([[18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1],\n",
      "        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n",
      "************************************************************\n",
      "X=tensor([[ 0,  2,  5, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n",
      "------------------------------------------------------------\n",
      "Y=tensor([[ 2,  5, 14,  0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    X,Y = batch_loader.next()\n",
    "    print(f'{X=}') # B,T = batch_size, max_word_length+1\n",
    "    print('-'*60)\n",
    "    print(f'{Y=}') # (B,T)\n",
    "    print('*'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cae3a3-c748-4249-badb-d96ed2e9632c",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "Most fundamental data structure for neural nets.\n",
    "\n",
    "A tensor is a collection of numbers index by tuple of non-negative integers. In the above, we've seen that a batch `X` is (B,T) tensor, we can index `X[b,t]` for b in range(B) and t in range(T). \n",
    "\n",
    "`torch` provides optimised tensor operations and auto differentiation engine. Rather than understanding low level optimisations (parallel programming as in e.g. cuda kernels), we just take these optimisations for granted and see what we can build with `Tensor`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1e291-6d38-413c-907b-1e87bb84aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11],\n",
       "         [12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).view(3,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ce1da-b121-4053-803e-64736e33771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.arange(60).view(3,4,5),torch.arange(60).view(3,4,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460478e-e2af-4976-9e53-7113b393df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e7052-e605-4459-96b9-2bd91b911cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1093,  0.0426],\n",
       "         [ 2.5843,  0.2994],\n",
       "         [-0.2158, -1.7210]]),\n",
       " tensor([[-1.2973,  0.2321],\n",
       "         [ 1.1551,  0.2394],\n",
       "         [ 0.4124,  0.1518]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4).split(2,dim=1)  # a tuple of 4/2 tensors of shape (3,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3aaa5-f92e-4580-beb4-3d7e960d1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, -99, -99, -99],\n",
       "        [  4,   5, -99, -99],\n",
       "        [  8,   9,  10, -99],\n",
       "        [ 12,  13,  14,  15]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = torch.arange(16).view(4,4)\n",
    "att.masked_fill(torch.tril(torch.ones(4,4))==0, -99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a960984-d5bf-4cf8-818e-235cdd14057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.randn(24).view(2,3,4).transpose(1,2).shape == (2,4,3)\n",
    "assert torch.randn(5).unsqueeze(1).shape == (5,1)\n",
    "assert torch.cat([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,3+3)\n",
    "assert torch.stack([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002a33a-e96e-42d9-9c96-e5868fc1da8e",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "`nn.Module` is the base class for all neural nets, which, mathemtically, are just functions taking  Tensor as input and compute the output as another Tensor. Just as we can compose functions, we can compose Module's to build complicated achitechture. \n",
    "\n",
    "custom Module must implement `forward` method. Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314ebbd-4a19-4bed-b5e5-1d75ff05e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6c07-7bdd-41e0-8acd-1131d85c73c4",
   "metadata": {},
   "source": [
    "## build Transformer \n",
    "\n",
    "the transformer is a custom module built on attention blocks, which themselves are built on multi-head masked self-attention layers. \n",
    "\n",
    "let's build from the layer level all the way to transformer. here is our model config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160e45a-b3df-4605-8c1e-2de212c917d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca4fa2-93e5-4b55-894a-9bb1087d0145",
   "metadata": {},
   "source": [
    "### attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be828be-16cd-40ac-9090-4c9210c665bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7fc80-f5c5-47ab-a147-05a02415bbf6",
   "metadata": {},
   "source": [
    "### block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7dd9e-2e11-4c90-bf0a-8b18fba0e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249554e4-55c5-4bee-a0ef-d24d29c700f7",
   "metadata": {},
   "source": [
    "### the entire thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9b92d-50d6-4b72-9221-3d134b7dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe632e4-79c6-4865-af05-b99b7a1e146e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
