{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5211294d-c48f-4f8a-a52f-647bea6feb2a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learn transformer with makemore and torch\n",
    "author: Xiaochuan Yang\n",
    "date: '2024-02-16'\n",
    "categories: [deep learning, python]\n",
    "format:\n",
    "  html:\n",
    "    code-line-numbers: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9f11c",
   "metadata": {},
   "source": [
    "## makemore\n",
    "\n",
    "Karpathy's [makemore](https://github.com/karpathy/makemore) is an end-to-end python application that takes in a pure text file, then generate new text similar to what's given. The project `makemore` is part of his [neural networks: zero to hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) series which I recommend to all. \n",
    "\n",
    "The example data in the repo  is a large collection of baby names (about 30k names) and the applicaiton trains a state of the art **transformer** model to learn the  mechanism of naming things, then sample from the learned model.  \n",
    "\n",
    "The purpose of this post is three fold:  \n",
    "- gain familiarity with **transformer**  \n",
    "- summarise essential `torch` functionalities and workflow for building neural nets application  \n",
    "- use minimal tools e.g `argparse` to produce a command line interface for the app\n",
    "\n",
    ":::{.callout-tip}\n",
    "### disclaimer\n",
    "all the code chunks in this post (including docstring) are taken from the [makemore](https://github.com/karpathy/makemore) repository\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d4387-44b3-4cd2-98b3-886ccd679974",
   "metadata": {},
   "source": [
    "## torch stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421b774-ac6c-48cf-b592-1d2d1a13431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc0dfe-b4d7-41bd-99cc-7af94952b891",
   "metadata": {},
   "source": [
    "Simplistically, a neural net application is built on two major components: a dataset and a model. `torch` allows us to do both fairly easily and straightforward.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "`Dataset` class is a container like a sequence. To define a custom Dataset class, one must implement `__init__`, `__len__`,  `__getitem__`, together with transformations relevant to the particular use case of the application. Exampe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8b793-8053-45f1-80fe-0b631de31e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words:list[str], chars:str, max_word_length:int):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759590a6-b4e0-4659-b822-eb28f6cdcb65",
   "metadata": {},
   "source": [
    "The custom transformations in this example consists of mapping character to integer, aka tokenisation. There is one special token `0` representing both the start of name and end of name. We can loop through or get at index a CharDataset because we have implemented sufficient dunder methods for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba078a-b7bf-4750-bc01-779b0f03474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n",
      "(tensor([ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0]), tensor([18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1]))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "examples = CharDataset(['emma', 'richard'], string.ascii_letters, 10)\n",
    "for e in examples:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7bfcc-8842-4277-bc65-1ce40a6a3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0,  5, 13, 13,  1,  0,  0,  0,  0,  0,  0]), tensor([ 5, 13, 13,  1,  0, -1, -1, -1, -1, -1, -1]))\n"
     ]
    }
   ],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba9465-3526-44e5-8af7-868a7e8d0925",
   "metadata": {},
   "source": [
    "Let's break down the output tuple `x,y`. \n",
    "\n",
    "#### `x` tensor\n",
    "It starts with `0` token, then each input character gets mapped to an integer from 1 to 26, with trailing zeros if the length of name is less than `max_word_length` (set to be max length of names in the dataset)\n",
    "\n",
    "#### `y` tensor\n",
    "By definition, it is the same as `x` shifted by 1 token to the left, modulo extra subtleties with the trailing -1. What is going on here? Well, in language modelling, the learning task is next token prediction, so at index `idx` such that `x[idx].item()!=0`, given `x[:idx+1]`, the goal is to predict `x[idx+1]` which by definition is nothing but `y[idx]`.  If `x[idx].item()==0`, then there is nothing to predict (name finished), we set by convention `y[idx]=-1`.\n",
    "\n",
    "Our ultimate goal is to build and train a neural net which can learn from the 30k  `x,y` tuples a good way of sampling next token given some context. Concetely, throw a `0` token at the model and let the model sample the next token `t1`, concatenate it with `0`, then sample next given `[0,t1]`, until a `0` token is sampled which means we have arrived at the end of a name. Repeat this to produce as many names as we want. We'll get back to inference later.\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "For efficiency's sake, it is beneficial to stack multiple examples together, aka mini-batch, and process them all at once. The `DataLoader` class is meant to help us with this.  Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a3f8-bcca-47b0-9cb8-bb632f9e85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    \"\"\"\n",
    "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
    "    a better way in PyTorch to just create an infinite dataloader?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80355603-6656-43e6-89f1-c43228bfcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharDataset(['emma', 'richard', 'ben', 'steve'],string.ascii_letters, 10)\n",
    "batch_loader = InfiniteDataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc76ef-9a1f-4ed4-871a-2ebd1df19764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[ 0, 18,  9,  3,  8,  1, 18,  4,  0,  0,  0],\n",
      "        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n",
      "------------------------------------------------------------\n",
      "Y=tensor([[18,  9,  3,  8,  1, 18,  4,  0, -1, -1, -1],\n",
      "        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n",
      "************************************************************\n",
      "X=tensor([[ 0,  2,  5, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 19, 20,  5, 22,  5,  0,  0,  0,  0,  0]])\n",
      "------------------------------------------------------------\n",
      "Y=tensor([[ 2,  5, 14,  0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [19, 20,  5, 22,  5,  0, -1, -1, -1, -1, -1]])\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    X,Y = batch_loader.next()\n",
    "    print(f'{X=}') # B,T = batch_size, max_word_length+1\n",
    "    print('-'*60)\n",
    "    print(f'{Y=}') # (B,T)\n",
    "    print('*'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cae3a3-c748-4249-badb-d96ed2e9632c",
   "metadata": {},
   "source": [
    "### Tensor shape ops\n",
    "\n",
    "Most fundamental data structure for neural nets.\n",
    "\n",
    "A tensor is a collection of numbers index by tuple of non-negative integers. In the above, we've seen that a batch `X` is (B,T) tensor, we can index `X[b,t]` for b in range(B) and t in range(T). \n",
    "\n",
    "`torch` provides optimised tensor operations and auto differentiation engine. Rather than understanding low level optimisations (parallel programming as in e.g. cuda kernels), we just take these optimisations for granted and see what we can build with `Tensor`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1e291-6d38-413c-907b-1e87bb84aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11],\n",
       "         [12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).view(3,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ce1da-b121-4053-803e-64736e33771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.arange(60).view(3,4,5),torch.arange(60).view(3,4,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460478e-e2af-4976-9e53-7113b393df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e7052-e605-4459-96b9-2bd91b911cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1093,  0.0426],\n",
       "         [ 2.5843,  0.2994],\n",
       "         [-0.2158, -1.7210]]),\n",
       " tensor([[-1.2973,  0.2321],\n",
       "         [ 1.1551,  0.2394],\n",
       "         [ 0.4124,  0.1518]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4).split(2,dim=1)  # a tuple of 4/2 tensors of shape (3,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3aaa5-f92e-4580-beb4-3d7e960d1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, -99, -99, -99],\n",
       "        [  4,   5, -99, -99],\n",
       "        [  8,   9,  10, -99],\n",
       "        [ 12,  13,  14,  15]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = torch.arange(16).view(4,4)\n",
    "att.masked_fill(torch.tril(torch.ones(4,4))==0, -99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a960984-d5bf-4cf8-818e-235cdd14057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.randn(24).view(2,3,4).transpose(1,2).shape == (2,4,3)\n",
    "assert torch.randn(5).unsqueeze(1).shape == (5,1)\n",
    "assert torch.cat([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,3+3)\n",
    "assert torch.stack([torch.ones(3,3), torch.arange(9).view(3,3)], dim=1).shape == (3,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002a33a-e96e-42d9-9c96-e5868fc1da8e",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "`nn.Module` is the base class for all neural nets, which, mathemtically, are just functions taking  Tensor as input and compute the output as another Tensor. Just as we can compose functions, we can compose Module's to build complicated achitechture. \n",
    "\n",
    "`torch` offers built-in modules as LEGO pieces. Example:\n",
    "\n",
    " - `nn.Linear`: random linear function that takes input dim and output dim as arguments\n",
    " - `nn.LayerNorm`: standardise a tensor over shape (pass in as argument), then multiplied by weight, then add bias (default elementwise 1 and 0).\n",
    " - `nn.RELU`: a parameter-less elementwise non-linear function\n",
    " - `nn.Embedding`: just a random lookup table of shape B,T, n_embd if the input tensor is of shape B,T \n",
    "\n",
    "some container classes: \n",
    "\n",
    "- `nn.ModuleDict`: pass in a dictionary of name:instance pairs \n",
    "- `nn.ModuleList`: pass in a list of instances \n",
    "\n",
    "custom Module must implement `forward` method. Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314ebbd-4a19-4bed-b5e5-1d75ff05e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6c07-7bdd-41e0-8acd-1131d85c73c4",
   "metadata": {},
   "source": [
    "## build Transformer \n",
    "\n",
    "Transformer is a custom module built on attention blocks, which themselves are built on multi-head masked self-attention layers. \n",
    "\n",
    "let's build from the layer level all the way to transformer. here is our model config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160e45a-b3df-4605-8c1e-2de212c917d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca4fa2-93e5-4b55-894a-9bb1087d0145",
   "metadata": {},
   "source": [
    "### attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be828be-16cd-40ac-9090-4c9210c665bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-line-numbers=\"true\"\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1dd17-8b1f-45ea-9b32-d7d35db9da0a",
   "metadata": {},
   "source": [
    "This is pretty self-explanatory. just don't forget `super().__init__`\n",
    "\n",
    "A bit of caution here on line 35: `y = y.view(B,T,C)` is not OK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee02b0-4698-45db-ac1e-c8603fcedea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True, False, False]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3,2)\n",
    "torch.ne(x.transpose(0,1).contiguous().view(3,4), x.view(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7fc80-f5c5-47ab-a147-05a02415bbf6",
   "metadata": {},
   "source": [
    "### block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7dd9e-2e11-4c90-bf0a-8b18fba0e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249554e4-55c5-4bee-a0ef-d24d29c700f7",
   "metadata": {},
   "source": [
    "### the entire thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9b92d-50d6-4b72-9221-3d134b7dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b34fed-cc27-44f7-ad78-3c0ae2386333",
   "metadata": {},
   "source": [
    "The only thing that deserves some care is `F.cross_entropy`. Here\n",
    "\n",
    "- ground truth is a tensor of shape (D,)\n",
    "- predictions is a tensor of shape (D,C) where C is the number of classes\n",
    "- use `logtis` for prediction (which is one `F.softmax` away from probability)\n",
    "- `ignore_index=-1` matches `-1` in the definition of `y`in `CharDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7ec6c-07c4-4418-baa8-a7a864b49591",
   "metadata": {},
   "source": [
    "## eval helpers\n",
    "\n",
    "the evaluation code is standard   \n",
    "\n",
    "- DataLoader can be looped through.  \n",
    "- batch is a tuple (X,Y) of tensors of shape batch_size, T\n",
    "\n",
    "I am however not sure what are the benefits of entering the inference mode, given that code has already switched on model.eval(). TODO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826bc25-2273-4561-8fd7-99ef14567437",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = [t.to(args.device) for t in batch]\n",
    "        X, Y = batch\n",
    "        logits, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c25f7-64d1-4a78-a055-064efd4862b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700c4b4-f7d2-4d1a-be29-fd11eb0d7433",
   "metadata": {},
   "source": [
    "## tie it all up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c1f5f-90de-4d00-95ca-727d421d3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c75515-bf63-422c-aaf0-f1b9dc66ffe5",
   "metadata": {},
   "source": [
    "### argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27438b93-71c9-40fa-b5f0-9f5792123cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse command line args\n",
    "parser = argparse.ArgumentParser(description=\"Make More\")\n",
    "# system/input/output\n",
    "parser.add_argument('--input-file', '-i', type=str, default='names.txt', help=\"input file with things one per line\")\n",
    "parser.add_argument('--work-dir', '-o', type=str, default='out', help=\"output working directory\")\n",
    "parser.add_argument('--resume', action='store_true', help=\"when this flag is used, we will resume optimization from existing model in the workdir\")\n",
    "parser.add_argument('--sample-only', action='store_true', help=\"just sample from the model and quit, don't train\")\n",
    "parser.add_argument('--num-workers', '-n', type=int, default=4, help=\"number of data workers for both train/test\")\n",
    "parser.add_argument('--max-steps', type=int, default=-1, help=\"max number of optimization steps to run for, or -1 for infinite.\")\n",
    "parser.add_argument('--device', type=str, default='cpu', help=\"device to use for compute, examples: cpu|cuda|cuda:2|mps\")\n",
    "parser.add_argument('--seed', type=int, default=3407, help=\"seed\")\n",
    "# sampling\n",
    "parser.add_argument('--top-k', type=int, default=-1, help=\"top-k for sampling, -1 means no top-k\")\n",
    "# model\n",
    "parser.add_argument('--type', type=str, default='transformer', help=\"model class type to use, bigram|mlp|rnn|gru|bow|transformer\")\n",
    "parser.add_argument('--n-layer', type=int, default=4, help=\"number of layers\")\n",
    "parser.add_argument('--n-head', type=int, default=4, help=\"number of heads (in a transformer)\")\n",
    "parser.add_argument('--n-embd', type=int, default=64, help=\"number of feature channels in the model\")\n",
    "parser.add_argument('--n-embd2', type=int, default=64, help=\"number of feature channels elsewhere in the model\")\n",
    "# optimization\n",
    "parser.add_argument('--batch-size', '-b', type=int, default=32, help=\"batch size during optimization\")\n",
    "parser.add_argument('--learning-rate', '-l', type=float, default=5e-4, help=\"learning rate\")\n",
    "parser.add_argument('--weight-decay', '-w', type=float, default=0.01, help=\"weight decay\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0d3ce-1dbb-487e-b835-58d2d52a0cea",
   "metadata": {},
   "source": [
    "### inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4b13a-58f6-4d1b-9e53-983bd8346af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system inits\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "os.makedirs(args.work_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=args.work_dir)\n",
    "\n",
    "# init datasets\n",
    "train_dataset, test_dataset = create_datasets(args.input_file)\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_output_length()\n",
    "print(f\"dataset determined that: {vocab_size=}, {block_size=}\")\n",
    "\n",
    "# init model\n",
    "config = ModelConfig(vocab_size=vocab_size, block_size=block_size,\n",
    "                   n_layer=args.n_layer, n_head=args.n_head,\n",
    "                   n_embd=args.n_embd, n_embd2=args.n_embd2)\n",
    "if args.type == 'transformer':\n",
    "    model = Transformer(config)\n",
    "elif args.type == 'bigram':\n",
    "    model = Bigram(config)\n",
    "elif args.type == 'mlp':\n",
    "    model = MLP(config)\n",
    "elif args.type == 'rnn':\n",
    "    model = RNN(config, cell_type='rnn')\n",
    "elif args.type == 'gru':\n",
    "    model = RNN(config, cell_type='gru')\n",
    "elif args.type == 'bow':\n",
    "    model = BoW(config)\n",
    "else:\n",
    "    raise ValueError(f'model type {args.type} is not recognized')\n",
    "model.to(args.device)\n",
    "print(f\"model #params: {sum(p.numel() for p in model.parameters())}\")\n",
    "if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n",
    "    print(\"resuming from existing model in the workdir\")\n",
    "    model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n",
    "if args.sample_only:\n",
    "    print_samples(num=50)\n",
    "    sys.exit()\n",
    "\n",
    "# init optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "# init dataloader\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad0e18-7509-4718-a2d7-fe0313342cbc",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6dc75-1ab2-4048-a79d-2b2e43dde577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "best_loss = None\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # get the next batch, ship to device, and unpack it to input and target\n",
    "    batch = batch_loader.next()\n",
    "    batch = [t.to(args.device) for t in batch]\n",
    "    X, Y = batch\n",
    "\n",
    "    # feed into the model\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # calculate the gradient, update the weights\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n",
    "    if args.device.startswith('cuda'):\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # logging\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "\n",
    "    # evaluate the model\n",
    "    if step > 0 and step % 500 == 0:\n",
    "        train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
    "        test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, step)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, step)\n",
    "        writer.flush()\n",
    "        print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "        # save the model to disk if it has improved\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            out_path = os.path.join(args.work_dir, \"model.pt\")\n",
    "            print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n",
    "            torch.save(model.state_dict(), out_path)\n",
    "            best_loss = test_loss\n",
    "\n",
    "    # sample from the model\n",
    "    if step > 0 and step % 200 == 0:\n",
    "        print_samples(num=10)\n",
    "\n",
    "    step += 1\n",
    "    # termination conditions\n",
    "    if args.max_steps >= 0 and step >= args.max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a6fb9-99e2-41dc-b3c0-763598808246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
