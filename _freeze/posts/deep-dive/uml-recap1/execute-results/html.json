{
  "hash": "49124bcd844f7e8edf0ad86bfd5711be",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Recap 1 Concepts\"\nauthor: \"Xiaochuan Yang\"\ndate: \"2023-10-27\"\ncategories: [python, machine learning]\ntoc: true\n---\n\n\\def\\PP{\\mathbb{P}}\n\\def\\VAR{\\mathrm{VaR}}\n\\def\\RR{\\mathbb{R}}\n\\def\\EE{\\mathbb{E}}\n\n\n## Setting up the scene  \n\nIn a supervised learning, we specify \n\n- an example space $\\mathcal X$  \n- a label space $\\mathcal Y$\n-  a collection of hypotheses $h: \\mathcal X \\to \\mathcal Y$, making up the hypothesis class  (inductive bias), from which we want to pick a predictor\n-  a loss function $\\ell: \\mathcal Y \\times \\mathcal Y \\to \\RR_+$, quantifying how good or bad a prediction $\\hat y$ is compared to the ground truth label $y$\n\n\n:::{.callout-important}\nWe are given an independent and identically distributed input-output pairs \n$S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y$ with distribution $D$. \n:::\n\nOur goal is to pick the \"best\" predictor in the sense of minimising the true loss  \n$$\nL_D(h) = \\EE_{(x,y)\\sim D}[\\ell(h(x),y))]  \n$$\nwhere $D$ is the *true* distribution of $(x,y)$.\n\nObviously *a priori* the distribution $D$ of the samples is unkonwn. However by law of large numbers we know that \n$$\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n$$\nis a consistent estimator of $L_D(h)$ (when $m\\to\\infty$ under mild condition on the distribution of $\\ell(h(x),y)$). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \n$$\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n$$\nHence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when $m$ is large, for a fixed $h$. Whether such approximation is valid uniformly for all the hypothesis in $\\mathcal H$, in other words, whether $h_S\\approx h^*\\in \\mathrm{argmin}_{h\\in\\mathcal H} L_D(h)$, is at the centre of the so-called learning theory. \n\nWe often decompose the generalisation error $L_D(h_S)$ in two parts:\n$$\nL_D(h_S) =  L_D(h^*) + [L_D(h_S) - L_D(h^*)] \n$$\nthe first is called the approximation error, and second estimation error. \n\n\n## Let's be concrete  \n\nFrom a practical point of view, we may not want to get into the learning theory bounds despite its elegance, what we do is to  split the sample $S^0$ into two parts $S, V$, where $S$ is used to find an ERM which we denote by $h_S$, another for testing whether the found ERM achieves small $L_D(h_S)$. The rationale is simple, since we make iid assumption, $V$ is independent of $S$, hence $L_{V}(h_S) \\approx L_D(h_S)$ when $|V|$ is not too small. Therefore, the smallness of $L_{V}(h_S)$ indicates good quality (small true risk) of our predictor $h_1$. \n\nThe split method for arrays is implemented in `sklearn.model_selection`\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\ng = np.random.default_rng(12)\nx = g.normal(0,1,(20,))\nx_tr, x_te = train_test_split(x,test_size=0.2,random_state=12)\n```\n:::\n\n\nTypically, examples are vectors in $\\RR^d, d\\ge 1$. In $k$-way ($k\\ge 2$) classification problems, labels are one-hot encodings $e_1,..., e_k$ where $e_i$ is the unit vector in $\\RR^k$ with one on the $i$ th coordinate and zero elsewhere. If $k=2$, we can drop the second coordinate and simply denote the two classes by $\\{1, -1\\}$ or $\\{0,1\\}$. In regression problems, the labels are in the continuum $\\RR^k, k\\ge 1$. \n\nNow consider $\\mathcal H$. For classification problems, instead of predicting discrete class labels directly, it is sometimes beneficial to predict a probability mass function over the $k$ classes. From the pmf, a label can be obtained by taking argmax. In other words, the range of $h\\in\\mathcal H$ is assumed to be $\\{y\\in\\RR^k: y_i\\ge 0, y_1+...+y_k=1\\}$. For regression problems there are no such constraints and the range can be the whole $\\RR^k$. \n\nThe choie of the loss function may vary, depending on what goal we are trying to achiecve. Researchers can design new losses suitable for their use case. Here we mention a few popular ones. For classification problems, if we use hypothesis predicting pmf, the cross entropy loss is often a good choice\n$$\nXE(p,\\hat p) =  - \\sum_{i=1}^k p_i \\log(\\hat p_i)\n$$\nFor regression problems, the squared loss is often a good choice\n$$SE(y,\\hat y)= \\|y-\\hat y\\|^2_2$$\nwhere $\\|.\\|$ is Euclidean norm. \n\nOne of the advantages of these loss functions is that they are convex in the $\\hat p$ or $\\hat y$ variables, making it possible to leverage the machinery of convex optimistion when it comes to the actual training process. \n\nMany loss functions are already implemented in `sklearn.metrics`. The XE is named `log_loss` and the squared loss is named `mean_sqquared_error`. Let's list all of them.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\nfor m in dir(metrics):\n    if not m.startswith('_'): print(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusionMatrixDisplay\nDetCurveDisplay\nDistanceMetric\nPrecisionRecallDisplay\nPredictionErrorDisplay\nRocCurveDisplay\naccuracy_score\nadjusted_mutual_info_score\nadjusted_rand_score\nauc\naverage_precision_score\nbalanced_accuracy_score\nbrier_score_loss\ncalinski_harabasz_score\ncheck_scoring\nclass_likelihood_ratios\nclassification_report\ncluster\ncohen_kappa_score\ncompleteness_score\nconfusion_matrix\nconsensus_score\ncoverage_error\nd2_absolute_error_score\nd2_pinball_score\nd2_tweedie_score\ndavies_bouldin_score\ndcg_score\ndet_curve\neuclidean_distances\nexplained_variance_score\nf1_score\nfbeta_score\nfowlkes_mallows_score\nget_scorer\nget_scorer_names\nhamming_loss\nhinge_loss\nhomogeneity_completeness_v_measure\nhomogeneity_score\njaccard_score\nlabel_ranking_average_precision_score\nlabel_ranking_loss\nlog_loss\nmake_scorer\nmatthews_corrcoef\nmax_error\nmean_absolute_error\nmean_absolute_percentage_error\nmean_gamma_deviance\nmean_pinball_loss\nmean_poisson_deviance\nmean_squared_error\nmean_squared_log_error\nmean_tweedie_deviance\nmedian_absolute_error\nmultilabel_confusion_matrix\nmutual_info_score\nnan_euclidean_distances\nndcg_score\nnormalized_mutual_info_score\npair_confusion_matrix\npairwise\npairwise_distances\npairwise_distances_argmin\npairwise_distances_argmin_min\npairwise_distances_chunked\npairwise_kernels\nprecision_recall_curve\nprecision_recall_fscore_support\nprecision_score\nr2_score\nrand_score\nrecall_score\nroc_auc_score\nroc_curve\nsilhouette_samples\nsilhouette_score\ntop_k_accuracy_score\nv_measure_score\nzero_one_loss\n```\n:::\n:::\n\n\nIt comes in handy that `sklearn` has them already defined. Implementing each one of them is often not hard, e.g.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef log_loss(yt,yp):\n    yt[yt==0]= 1-yp[yt==0]\n    return -np.log(y).mean()\n```\n:::\n\n\n<!-- ## Model selection / hyperparameter tuning \n\nModels may have many hyperparameters (learning rate, tree depth, neural network architectures etc). One can use k-fold cross validation to pick the one that optimises a certain metric prescribed by the user of these models. This is \n\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n``` -->\n\n## Analysis of errors\n\nIn practice, $D$ is unknown and we only observe the training error $L_S(h_S)$ and the validation error $L_V(h_S)$. Hence we decompose the generalisation error differently\n$$\nL_D(h_S) = [L_D(h_S) - L_V(h_S)] + [L_V(h_S) - L_S(h_S)] + L_S(h_S)\n$$\nThe first term is small when $|V|$ is moderately large by independence of $V$ and $S$. \nThe second and third term are observalbe and several cases may arise. \n\n- the gap is small and the training error is small. This is a happy scenario\n- the training error is large. To address this, we may consider\n  - engarge the hypothesis class, \n  - completely changing it, \n  - find a better feature representation,\n  - find a better optimiser  \n- training error is small but the gap is large. To address this, we may consider\n  - add regularisation\n  - get more training data\n  - reduce the hypothesis class\n\nIt is benefiical to plot the learning curve during training. This amounts to visualise the training error and validation error on the same plot as time evolves (every X batches, every X epoch etc). \n\n",
    "supporting": [
      "uml-recap1_files"
    ],
    "filters": [],
    "includes": {}
  }
}