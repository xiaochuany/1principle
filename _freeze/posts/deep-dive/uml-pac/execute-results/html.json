{
  "hash": "70214dcdbd1624f6d956530e974ad8a4",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Recaps 1\"\nauthor: \"Xiaochuan Yang\"\ndate: \"2023-10-27\"\n# format:\n#   html: \n#     include-in-header: ../assets/macro.qmd\ncategories: [code, machine learning]\ndraft: false\ntoc: true\nnumber-sections: false\n---\n\n\\def\\PP{\\mathbb{P}}\n\\def\\VAR{\\mathrm{VaR}}\n\\def\\RR{\\mathbb{R}}\n\\def\\EE{\\mathbb{E}}\n\n\n## Setting up the scene  \n\nIn a supervised learning, we specify \n\n- an example space $\\mathcal X$  \n- a label space $\\mathcal Y$\n-  a collection of hypotheses $h: \\mathcal X \\to \\mathcal Y$, making up the hypothesis class  (inductive bias), from which we want to pick a predictor\n-  a loss function $\\ell: \\mathcal Y \\times \\mathcal Y \\to \\RR_+$, quantifying how good or bad a prediction $\\hat y$ is compared to the ground truth label $y$\n\n\n:::{.callout-important}\nWe are given an independent and identically distributed input-output pairs \n$S = \\{(x_i,y_i), i=[m]\\}\\subset \\mathcal X\\times\\mathcal Y$ with distribution $D$. \n:::\n\nOur goal is to pick the \"best\" predictor in the sense of minimising the true loss  \n$$\nL_D(h) = \\EE_{(x,y)\\sim D}[\\ell(h(x),y))]  \n$$\nwhere $D$ is the *true* distribution of $(x,y)$.\n\nObviously *a priori* the distribution $D$ of the samples is unkonwn. However by law of large numbers we know that \n$$\nL_S(h):= \\frac{1}{m}\\sum_{i=1}^m \\ell(h(x_i),y_i)\n$$\nis a consistent estimator of $L_D(h)$ (when $m\\to\\infty$ under mild condition on the distribution of $\\ell(h(x),y)$). This motivates the empirical risk minimisation (ERM) approach i.e. we look for \n$$\nh_S \\in \\mathrm{argmin}_{h\\in\\mathcal H} L_S(h)\n$$\nHence, instead of minimising the true risk, we minimise the empirical risk, which is close to the true risk when $m$ is large, for a fixed $h$. Whether such approximation is valid uniformly for all the hypothesis in $\\mathcal H$ is at the centre of the so-called learning theory . \n\n\n## Let's be concrete  \n\nFrom a practical point of view, we may not want to get into the learning theory bounds despite its elegance, what we do is to  split the sample $S$ into two parts $S_1, S_2$, where $S_1$ is used to find an ERM which we denote by $h_1$, another for testing whether the found ERM achieves small $L_D(h_1)$. The rationale is simple, since we make iid assumption, $S_2$ is independent of $S_1$, hence $L_{S_2}(h_1) \\approx L_D(h_1)$ when $|S_2|$ is not too small. Therefore, the smallness of $L_{S_2}(h_1)$ indicates good quality (small true risk) of our predictor $h_1$. \n\nThe split method for arrays is implemented in `sklearn.model_selection`\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\ng = np.random.default_rng(12)\nx = g.normal(0,1,(20,))\nx_tr, x_te = train_test_split(x,test_size=0.2,random_state=12)\n```\n:::\n\n\nTypically, examples are vectors in $\\RR^d, d\\ge 1$. In $k$-way ($k\\ge 2$) classification problems, labels are one-hot encodings $e_1,..., e_k$ where $e_i$ is the unit vector in $\\RR^k$ with one on the $i$ th coordinate and zero elsewhere. If $k=2$, we can drop the second coordinate and simply denote the two classes by $\\{1, -1\\}$ or $\\{0,1\\}$. In regression problems, the labels are in the continuum $\\RR^k, k\\ge 1$. \n\nNow consider $\\mathcal H$. For classification problems, instead of predicting discrete class labels directly, it is often beneficial to predict a probability mass function over the $k$ classes. From this a label can be obtained by taking argmax. In other words, the range of $h\\in\\mathcal H$ is assumed to be $\\{y\\in\\RR^k: y_i\\ge 0, y_1+...+y_k=1\\}$. For regression problems there are no such constraints and the range can be the whole $\\RR^k$. \n\nThe choie of the loss function may vary, depending on what goal we are trying to achiecve. Researchers can design new losses suitable for their use case. Here we mention a few popular ones. For classification problems, if we use hypothesis predicting pmf, the cross entropy loss is often a good choice\n$$\nXE(p,\\hat p) =  - \\sum_{i=1}^k p_i \\log(\\hat p_i)\n$$\nFor regression problems, the squared loss is often a good choice\n$$SE(y,\\hat y)= \\|y-\\hat y\\|^2_2$$\nwhere $\\|.\\|$ is Euclidean norm. \n\nThere are many losses already implemented in `sklearn.metrics`. The XE is named `log_loss` and the squared loss is named `mean_sqquared_error`\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\nfor m in dir(metrics):\n    if not m.startswith('_'): print(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusionMatrixDisplay\nDetCurveDisplay\nDistanceMetric\nPrecisionRecallDisplay\nPredictionErrorDisplay\nRocCurveDisplay\naccuracy_score\nadjusted_mutual_info_score\nadjusted_rand_score\nauc\naverage_precision_score\nbalanced_accuracy_score\nbrier_score_loss\ncalinski_harabasz_score\ncheck_scoring\nclass_likelihood_ratios\nclassification_report\ncluster\ncohen_kappa_score\ncompleteness_score\nconfusion_matrix\nconsensus_score\ncoverage_error\nd2_absolute_error_score\nd2_pinball_score\nd2_tweedie_score\ndavies_bouldin_score\ndcg_score\ndet_curve\neuclidean_distances\nexplained_variance_score\nf1_score\nfbeta_score\nfowlkes_mallows_score\nget_scorer\nget_scorer_names\nhamming_loss\nhinge_loss\nhomogeneity_completeness_v_measure\nhomogeneity_score\njaccard_score\nlabel_ranking_average_precision_score\nlabel_ranking_loss\nlog_loss\nmake_scorer\nmatthews_corrcoef\nmax_error\nmean_absolute_error\nmean_absolute_percentage_error\nmean_gamma_deviance\nmean_pinball_loss\nmean_poisson_deviance\nmean_squared_error\nmean_squared_log_error\nmean_tweedie_deviance\nmedian_absolute_error\nmultilabel_confusion_matrix\nmutual_info_score\nnan_euclidean_distances\nndcg_score\nnormalized_mutual_info_score\npair_confusion_matrix\npairwise\npairwise_distances\npairwise_distances_argmin\npairwise_distances_argmin_min\npairwise_distances_chunked\npairwise_kernels\nprecision_recall_curve\nprecision_recall_fscore_support\nprecision_score\nr2_score\nrand_score\nrecall_score\nroc_auc_score\nroc_curve\nsilhouette_samples\nsilhouette_score\ntop_k_accuracy_score\nv_measure_score\nzero_one_loss\n```\n:::\n:::\n\n\nThese are not difficult to implement, e.g. see below for log loss (in the case of binary classification), but it comes in handy that `sklearn` has them already defined. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef log_loss(yt,yp):\n    yt[yt==0]= 1-yp[yt==0]\n    return -np.log(y).mean()\n```\n:::\n\n\n",
    "supporting": [
      "uml-pac_files"
    ],
    "filters": [],
    "includes": {}
  }
}