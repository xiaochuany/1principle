{
  "hash": "0c2d3f43993636bee59cdc785174842d",
  "result": {
    "markdown": "---\ntitle: \"Credit Risk Models Ep 2 machine learning methods for parameter tuning\"\nauthor: \"Xiaochuan Yang\"\ndate: \"2023-10-16\"\n# format:\n#   html: \n#     include-in-header: ../assets/macro.qmd\ncategories: [credit risk modelling, code, machine learning]\ndraft: false\ntoc: true\nnumber-sections: false\n---\n\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\n>In a [previous post](crm1-fun.qmd), we've modelled the loss of a portfolio of $d$ instruments as follows\n$$L = \\sum_{i=1}^d \\mu_i S_i I_i,\n$$ \nwhere $L$ represents the total loss, $\\mu_i$ is the exposure at default, $S_i$ is the loss given default, and $I_i$ is the event of default for the $i$-th instrument.\nTo compute expected loss, VaR, and other relevant metrics in risk management, it is crucial to estimate these underlying parameters accurately.\nIn this post, our focus is on estimating $p_i=\\mathbb{E}[I_i]$, which is formulated as a machine learning problem\n\n## Estimating probaiblity of default (PD)\n\n\nConsider the real-world example of a bank approving loans for applicants based on their profiles. In this scenario, every applicant must fill out a comprehensive application form, including details such as their profession, age, amount of debts, monthly salary, and so on. The bank maintains records and, in retrospect, knows who has defaulted on their loans.\n\nTo formalize this process, each applicant corresponds to a vector in $\\RR^k$, known as the feature vector, which incorporates all the information from the form (possibly encoded for categorical values, e.g., converting 'profession' into dummy variables). The output we aim to predict is whether the applicant is in default (1) or not (0).\n\nThis constitutes a binary classification problem. With a substantial number of input-output pairs (features and default status) available, supervised learning algorithms can be employed to learn a relationship that can subsequently be used for predictions.\n\nMany supervised learning algorithms for binary prediction actually output probabilities (specifically, the probability of the label being 1). This is suitable for our goal, as we precisely seek to estimate probabilities.\n\nFor illustration, let's consider a credit default risk dataset from this [Kaggle competition](https://www.kaggle.com/competitions/home-credit-default-risk). The dataset we are working with contains 308k rows and 121 input features. \n\nWhile there are numerous aspects to discuss regarding this dataset, for the sake of brevity, I will address two significant points here:\n\n\n### Missing values\n\n\nThere are many missing values in this dataset. To be more precise, 41 columns actually have half of their values missing.\n\nThis is a critical issue that needs to be addressed because many off-the-shelf machine learning models in scikit-learn, such as logistic regression, random forest, and support vector machines, cannot handle missing values represented as `np.nan`. There are two possible options to handle this:\n\n1. **Impute the missing values** before feeding the data into these models. Imputation can be done using \"rule-based\" methods such as scikit-learn's SimpleImputer or \"learning-based\" methods such as scikit-learn's IterativeImputer.\n\n2. **Use a different model that supports missing values natively.** For instance, many gradient boosting implementations like HistGradientBoosting, XGBoost, LightGBM, and CatBoost handle missing values natively.\n\nFor the sake of providing a quick benchmark, we have opted for the second option and are using scikit-learn's gradient boosting implementation, HistGradientBoostingClassifier. Explaining the detailed workings of gradient boosting is a vast topic that we might delve into in a future post.\n\n\n### Unbalanced data\n\nRoughly 8% of the obligors goes into default in this dataset, making it unbalanced. From the perspective of risk management, it is important to predict defaults (label 1) accurately. Therefore, when it comes to evaluate model performance,  accuracy is not an appropriate metric. Simply predicting non-default for all obligors would result in a correct prediction 92% of the time, but it's never correct for the defaults. \n\nApplying a machine learning model directly to an unbalanced dataset can lead to sub-optimal results. We'll demonstrate this point in the next section. A simple mitigation strategy is to sub-sample the majority class to match the size of the minority class,  creating a balanced dataset. In this example, the minority class comprises roughly 25k rows, so we can train a model using this strategy as the balanced data is not too small to be useful. \n\nObviously, the downside of this strategy is that we have thrown away lots of valuable data, but let's not worry about it at this stage of obtaining a quick benchmark. \n\n## Implementation\n\nFirst we feed the whole unbalanced dataset into HistGradientBoostingClassifier. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\nPATH = '../../../LearnFromTabularData/data/raw'\ndf = pd.read_csv(os.path.join(PATH,'application_train.csv'))\n\ny = df.pop('TARGET')\nX = df\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=1123)\n\nohe = OneHotEncoder(drop='if_binary')\nscaler = StandardScaler()\nmodel = HistGradientBoostingClassifier()\n\nct = make_column_transformer(\n    (ohe, make_column_selector(dtype_exclude=np.number)),\n    (scaler, make_column_selector(dtype_include=np.number)),\n    remainder='drop',\n)\n\npipe = make_pipeline(\n    ct,model\n)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       1.00      0.92      0.96     61354\n           1       0.02      0.59      0.04       149\n\n    accuracy                           0.92     61503\n   macro avg       0.51      0.76      0.50     61503\nweighted avg       1.00      0.92      0.96     61503\n\n```\n:::\n:::\n\n\nAs we can see, the precision for class 1 is only 0.02, indicating that only 2% of the predicted defaults are true defaults.\nDoes this mean that the model we chose is rubbish?\nNot necessarily. Let's use the exact same model but now with a balanced dataset created using the sub-sampling strategy\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(os.path.join(PATH,'application_train.csv'))\nmask = (df['TARGET']==1)\nm = mask.sum()\ndf_balance = pd.concat([df[~mask].sample(m,random_state=119),df[mask]], axis=0)\n\ny = df_balance.pop(\"TARGET\")\nX = df_balance\n\nX_tr, X_dev, y_tr, y_dev = train_test_split(X,y,test_size=0.2, random_state=112)\n\nohe = OneHotEncoder(drop='if_binary')\nscaler = StandardScaler()\nmodel = HistGradientBoostingClassifier()\n\nct = make_column_transformer(\n    (ohe, make_column_selector(dtype_exclude=np.number)),\n    (scaler, make_column_selector(dtype_include=np.number)),\n    remainder='drop',\n)\n\npipe = make_pipeline(\n    ct,model\n)\n\npipe.fit(X_tr,y_tr)\ny_pred = pipe.predict(X_dev)\nprint(classification_report(y_pred,y_dev))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.69      0.68      0.68      5034\n           1       0.67      0.69      0.68      4896\n\n    accuracy                           0.68      9930\n   macro avg       0.68      0.68      0.68      9930\nweighted avg       0.68      0.68      0.68      9930\n\n```\n:::\n:::\n\n\nMuch better! All the metrics look roughly the same, hovering around 69%. While this is far from being deployable in the real world, as a baseline, it's far more reasonable than our previous attempt.\n\nWe'll conclude the post here. To further enhance the overall performance, it's necessary to meticulously explore the features, engage in more feature engineering, employ clever methods of data imputation, and conduct thorough hyperparameter tuning. Give it a go!\n\n",
    "supporting": [
      "crm2-ml_files"
    ],
    "filters": [],
    "includes": {}
  }
}