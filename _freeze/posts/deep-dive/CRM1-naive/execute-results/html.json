{
  "hash": "2a18ee497f756533a935be22df261bfc",
  "result": {
    "markdown": "---\ntitle: \"Credit Risk Models Ep 1 Fundamentals\"\nauthor: \"Xiaochuan Yang\"\ndate: \"2023-10-15\"\n# format:\n#   html: \n#     code-fold: true\ncategories: [credit risk modelling, applied probability, code]\n---\n\n<!-- latex stuff -->\n\\def\\PP{\\mathbb{P}}\n\\def\\VAR{\\mathrm{VaR}}\n\\def\\RR{\\mathbb{R}}\n\\def\\EE{\\mathbb{E}}\n\n::: {.callout-note}\nThis is the first post in a series dedicated to credit risk models. The model discussed in this post is intentionally naive, as it relies on several unrealistic assumptions for the sake of simplicity and tractability. Despite its simplicity, this model serves as an excellent starting point for understanding the fundamental concepts of credit risk. It lays the foundation for more complex models that will be explored later in this series. The mathematical concepts introduced in this post are commonly found in any first-year probability textbook\n:::\n\n\n## PD-LGD-EAD approach\n\n\n\nCredit risk models are mathematical representations of the potential losses within a portfolio of financial instruments. Each instrument carries a probability of default, leading to either a complete loss or a partial loss. Modeling credit risk is crucial as it offers valuable insights to stakeholders, enabling them to take proactive measures to mitigate these losses. Consequently, it plays a pivotal role in the financial sector.\n\n\n\n\nLet's introduce some notation. Consider a portfolio containing $d$ instruments. The event of default for the $i$-th instrument is denoted as $D_i$. Here, $\\mu_i$ represents the exposure at default (EAD) of the $i$-th instrument, and $S_i \\in [0,1]$ the loss given default (LGD) where $S$ signifies severity, indicating the actual loss relative to the exposure. We define $I_i := I_{D_i}$ as the indicator of the event $D_i$, which follows a Bernoulli distribution with parameter $p_i := \\mathbb{P}[D_i]$. The total loss of the portfolio is expressed as the sum: \n$$\nL = \\sum_{i=1}^d \\mu_i S_i I_i\n$${#eq-loss}\nFrom the perspective of risk managers, the values of $\\mu_i$ are known. However, the uncertainly lies in whether an instrument will default and the corresponding severity, making the total loss $L$ a random variable. Our interest lies in understanding the probability distribution of $L$. \n\nIn this post we make the following naive assumptions.\n\n::: {.callout-important}\n Assume that \n\n - $\\{(I_i,S_i)\\}_{i=1}^d$ is a family of independent random vectors. \n - $I_i$ is independent of $S_i$ for all $i\\in [d]$.\n:::\n\nUnder this assumption, it is clear that\n$$\n\\EE[L] = \\sum_{i=1}^d \\mu_i \\EE[S_i] p_i\n$$\n$$\n\\mathrm{Var}[L] = \\sum_{i=1}^d \\mu_i^2 (\\EE[S_i^2] p_i  - \\EE[S]^2 p_i^2)\n$$\nThe expected loss (EL) is an important quantity in the [Basel III guidelines](https://www.eba.europa.eu/regulation-and-policy/single-rulebook/interactive-single-rulebook/1586).  \n\n## VaR and expected shortfalls\n\n\nIn a stable economy, default events are infrequent, making credit risk modeling primarily focused on rare occurrences. Obtaining a precise measure of the credit risk of a portfolio is not achieved by merely calculating the average; instead, it's crucial to comprehend the quantiles. In the realm of credit risk, these quantiles are referred to as the Value at Risk (VaR).  \n$$\n\\VAR_\\alpha(L) = \\inf\\{t\\in\\RR: \\PP[L\\ge t]\\le \\alpha \\}.\n$$\nAnother commonly utilized metric is the Expected Shortfall, which represents the conditional expectation of the loss $L$ given that $L$ exceeds the VaR\n$$\nE_\\alpha(L) = \\EE[L|L\\ge\\VAR_\\alpha]. \n$$\nClearly $E_\\alpha(L)\\ge \\VAR_\\alpha(L)$. \n\nIt's crucial to acknowledge that the joint distribution of $(I_i, S_i)$ is **unknown**. To compute VaR and expected shortfalls, risk analysts must undertake at least three tasks:\n\n1. **Model Calibration:** This involves determining the model's parameters using available data.\n2. **Distribution Computation:** Compute the distribution of $L$ either analytically or through Monte Carlo simulation, based on the calibrated parameters.\n3. **Metrics Computation:** Utilize the obtained distribution to compute VaR, shortfall, or other relevant metrics either analytically or numerically.\n\nThe calibration process demands substantial effort and is specific to the chosen model. We will delve into this topic in a future post. For the remainder of this discussion, let's assume that we have already established a set of parameters and focus on the last two steps.\n\nIt's important to emphasize that deriving the exact distribution of $L$ can be tedious, if not infeasible. In practice, risk analysts resort to simulations and numerical computations to determine VaR and shortfalls. This pragmatic approach is the one we adopt here.\n\n## Monte Carlo simulation for estimating VaR and shortfalls\n\n**Step 1**\n\nWe first specify the parameters $p_i, \\mu_i$ and parameters for the distribution of $S_i$. These parameters are generated at random with arbitrary choice of distributions, following the book by [David Jamieson Bolder - Credit-Risk Modelling (2018)](https://link.springer.com/book/10.1007/978-3-319-94688-7)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# set random seed for reproducibility\ng = np.random.default_rng(1123)\n\nd = 100 # number of instruments\n\n# set the probability of default for d instruments, low 0.1 high 7 per cent\nx = g.chisquare(1,(d,))\nx = np.where(x<0.1, 0.1, x)\nx = np.where(x>7, 7, x)\np = x/100 \n\n# set exposures parameters normalised to have total exposure 1000\ny = g.weibull(3,(d,))\nmu = y*1000/y.sum()\n\n# set serverity parameters\na,b = 1.5, 2.5\n```\n:::\n\n\n**Step 2** \n\nNext we simulate $L$ for a large number of times accoding to @eq-loss. We sort the samples in the end, useful for computing the empirical distributions later on. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nm = 50_000 # number of repetitions\nS = g.beta(a, b, (m,d))\nI = g.uniform(size=(m,d)) < p\nL = (S*I)@ mu\nL = np.sort(L)\n```\n:::\n\n\n**Step 3**\n\nWe plot the histogram and empirical distribution, then estimate VaR and expected shortfalls. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Estimate VaR \npers = np.array([95,99,99.9])\nalphas = 1 - pers/100\nvars = np.percentile(L, pers)\n\nfig, axes = plt.subplots(2,1)\naxes[0].hist(L, bins=30, label='Histogram of Loss')\nfor i, var in enumerate(vars): \n  axes[0].axvline(var,linestyle='dashed', color=g.uniform(size=(3,)),label=f'VaR{alphas[i]:.3f}')\naxes[0].legend()\n\naxes[1].plot(L,np.linspace(0,1,len(L),endpoint=False), color= 'red', label='empirical distribution')\naxes[1].legend()\nfor var in vars: \n  axes[1].axvline(var,linestyle='dashed')\nplt.show()\n\n# Estimate shortfalls\nshortfalls = np.zeros(len(vars))\nfor i,var in enumerate(vars):\n  shortfalls[i] = L[L>var].mean()\n\npd.set_option('display.precision', 4)\npd.DataFrame({'alphas': alphas, 'VaR': vars, 'Shortfalls': shortfalls}).set_index('alphas')\n```\n\n::: {.cell-output .cell-output-display}\n![](CRM1-naive_files/figure-html/cell-4-output-1.png){width=592 height=411}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VaR</th>\n      <th>Shortfalls</th>\n    </tr>\n    <tr>\n      <th>alphas</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.050</th>\n      <td>12.6668</td>\n      <td>16.2741</td>\n    </tr>\n    <tr>\n      <th>0.010</th>\n      <td>18.6163</td>\n      <td>21.4599</td>\n    </tr>\n    <tr>\n      <th>0.001</th>\n      <td>25.4548</td>\n      <td>28.1533</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe've completed tasks 2 and 3! Next time, we'll delve into methods of parameter estimation, still within the naive setting. This will bring us full circle and complete the lifecycle of a single model.\n\n",
    "supporting": [
      "CRM1-naive_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}